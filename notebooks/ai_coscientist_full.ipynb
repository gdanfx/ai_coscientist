{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GDBt3erMBG4",
        "outputId": "0885bb9f-f47d-418a-fe7b-e870712b52c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Setup & Configuration Complete!\n",
            "📊 Configured Sources: PubMed, ArXiv, CrossRef, Web Academic\n",
            "🔑 Gemini API configured successfully\n",
            "⚙️ Ready to run main literature search agent\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# SETUP & CONFIGURATION CELL\n",
        "# Enhanced Multi-Source Literature Search - Dependencies and Config\n",
        "# ============================================================================\n",
        "\n",
        "#@title Setup & Configuration - Multi-Source Literature Search\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q langchain-community duckduckgo-search langgraph langchain-google-genai\n",
        "\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "import logging\n",
        "import json\n",
        "from typing import List, Dict, Any, Optional\n",
        "from datetime import datetime\n",
        "from urllib.parse import quote\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "\n",
        "# LangGraph imports\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# LangChain and Gemini imports\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION VARIABLES\n",
        "# ============================================================================\n",
        "\n",
        "# API Configuration - UPDATE WITH YOUR CREDENTIALS\n",
        "PUBMED_EMAIL = \"dgonchar@stanford.edu\"  # REQUIRED for PubMed\n",
        "CROSSREF_EMAIL = \"dgonchar@stanford.edu\"  # Optional: For better rate limits\n",
        "\n",
        "# Search Configuration\n",
        "RATE_LIMIT_DELAY = 0.5\n",
        "MAX_RESULTS_PER_SOURCE = 3\n",
        "TOTAL_MAX_RESULTS = 10\n",
        "\n",
        "# ============================================================================\n",
        "# STATE DEFINITION\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class GenerationState:\n",
        "    \"\"\"State shared across all nodes in the generation workflow\"\"\"\n",
        "    research_goal: str = \"\"\n",
        "    constraints: List[str] = None\n",
        "    search_queries: List[str] = None\n",
        "    literature_findings: List[Dict[str, Any]] = None\n",
        "    synthesized_knowledge: str = \"\"\n",
        "    generated_proposals: List[Dict[str, Any]] = None\n",
        "    iteration: int = 0\n",
        "    max_iterations: int = 3\n",
        "    status: str = \"initialized\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.constraints is None:\n",
        "            self.constraints = []\n",
        "        if self.search_queries is None:\n",
        "            self.search_queries = []\n",
        "        if self.literature_findings is None:\n",
        "            self.literature_findings = []\n",
        "        if self.generated_proposals is None:\n",
        "            self.generated_proposals = []\n",
        "\n",
        "# ============================================================================\n",
        "# GEMINI MODEL SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Set up your Gemini API key using Colab's secure userdata\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "# Initialize Gemini model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=4000\n",
        ")\n",
        "\n",
        "print(\"✅ Setup & Configuration Complete!\")\n",
        "print(\"📊 Configured Sources: PubMed, ArXiv, CrossRef, Web Academic\")\n",
        "print(\"🔑 Gemini API configured successfully\")\n",
        "print(\"⚙️ Ready to run main literature search agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L56ykRxEMCWZ",
        "outputId": "9e85fa78-0042-421e-a852-b75981ce6f06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Testing Enhanced Multi-Source Agent...\n",
            "🚀 Enhanced AI Co-Scientist with Multi-Source Literature Search\n",
            "============================================================\n",
            "📊 Configured Sources: PubMed, ArXiv, CrossRef, Web Academic\n",
            "============================================================\n",
            "🔍 Generating multi-source search queries...\n",
            "✅ Generated 4 optimized queries\n",
            "   1. AI personalized medicine neurodegenerative diseases\n",
            "   2. machine learning precision medicine Alzheimer\n",
            "   3. computational biomarkers neurodegeneration\n",
            "   4. artificial intelligence clinical decision support\n",
            "📚 Searching multiple literature sources...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:No meaningful web results for: artificial intelligence clinical decision support\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Found 12 papers from 1 sources:\n",
            "   • PubMed: 12 papers\n",
            "🧠 Synthesizing knowledge from multiple sources...\n",
            "✅ Multi-source knowledge synthesis complete\n",
            "💡 Generating hypotheses from multi-source research...\n",
            "✅ Generated hypotheses based on 12 papers from 1 sources\n",
            "\n",
            "============================================================\n",
            "🎉 ENHANCED SEARCH SUCCESS!\n",
            "============================================================\n",
            "\n",
            "📋 Research Goal: Develop AI-driven personalized medicine approaches for neurodegenerative diseases\n",
            "\n",
            "📚 Literature Found (12 papers):\n",
            "   • PubMed: 12 papers\n",
            "\n",
            "🔝 Top Papers by Relevance:\n",
            "   1. Targeted Gut Microbiota Modulation Enhances Levodopa Bioavailability and Motor Recovery in MPTP Parkinson's Disease Models.\n",
            "      Source: PubMed | Citations: 0\n",
            "   2. Macrophages: sentinels, warriors, and healers.\n",
            "      Source: PubMed | Citations: 0\n",
            "   3. Machine learning-enhanced SERS diagnostics: Accelerating the AI-powered transition from laboratory discoveries to clinical practice.\n",
            "      Source: PubMed | Citations: 0\n",
            "\n",
            "💡 Generated Hypotheses:\n",
            "   **Novel Hypotheses for AI-Driven Personalized Medicine in Neurodegeneration:**\n",
            "\n",
            "**Hypothesis 1: Multi-Modal Temporal Fusion**\n",
            "A federated learning approach combining longitudinal biomarker data, neuroimaging, and wearable sensor data can predict disease progression 2-3 years earlier than current met...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# LITERATURE SEARCH AGENT - INTERFACE CONTRACT\n",
        "# ============================================================================\n",
        "\"\"\"\n",
        "📋 AGENT: Enhanced Multi-Source Literature Search Agent\n",
        "\n",
        "🔌 INPUTS:\n",
        "- research_goal: str (e.g., \"Develop AI-driven personalized medicine for neurodegeneration\")\n",
        "- constraints: List[str] (e.g., [\"Must integrate multiple data modalities\", \"Focus on translational applications\"])\n",
        "\n",
        "📤 OUTPUTS:\n",
        "- GenerationState object containing:\n",
        "  - search_queries: List[str] (optimized search terms)\n",
        "  - literature_findings: List[Dict] (papers from multiple sources)\n",
        "  - synthesized_knowledge: str (LLM-generated synthesis)\n",
        "  - generated_proposals: List[Dict] (novel hypotheses)\n",
        "  - status: str (workflow progress indicator)\n",
        "\n",
        "📊 DATA STRUCTURES:\n",
        "- Literature Finding:\n",
        "  {\n",
        "    'title': str,\n",
        "    'abstract': str,\n",
        "    'authors': List[str],\n",
        "    'journal': str,\n",
        "    'year': str,\n",
        "    'source': str,  # 'PubMed' | 'ArXiv' | 'CrossRef' | 'Web Search'\n",
        "    'url': str,\n",
        "    'key_findings': List[str],\n",
        "    'relevance_score': float,\n",
        "    'citation_count': int\n",
        "  }\n",
        "\n",
        "- Generated Proposal:\n",
        "  {\n",
        "    'id': str,\n",
        "    'content': str,\n",
        "    'timestamp': str,\n",
        "    'based_on_sources': int,\n",
        "    'source_diversity': int\n",
        "  }\n",
        "\n",
        "🔗 INTER-AGENT COMMUNICATION:\n",
        "- Accepts: research_goal, constraints from Supervisor\n",
        "- Provides: GenerationState to Proximity → Reflection → Ranking → Evolution agents\n",
        "- Status updates for Supervisor monitoring\n",
        "\n",
        "⚠️ ERROR HANDLING:\n",
        "- API timeouts with exponential backoff\n",
        "- Rate limiting with delays\n",
        "- Fallback to available sources when others fail\n",
        "- Empty result handling with meaningful messages\n",
        "\n",
        "🧪 TESTING INTERFACE:\n",
        "- Input: test_research_goal, test_constraints\n",
        "- Expected: GenerationState with ≥1 proposal, successful literature search\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCED MULTI-SOURCE LITERATURE SEARCH AGENT\n",
        "# ============================================================================\n",
        "#@title MULTI-SOURCE LITERATURE SEARCH AGENT\n",
        "# Required imports\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass, field\n",
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Configuration variables\n",
        "PUBMED_EMAIL = \"dgonchar@stanford.edu\"  # Replace with your email\n",
        "CROSSREF_EMAIL = \"dgonchar@stanford.edu\"  # Replace with your email\n",
        "RATE_LIMIT_DELAY = 1.0  # Increased from 0.5 to 1.0 seconds between API calls\n",
        "MAX_RESULTS_PER_SOURCE = 3\n",
        "TOTAL_MAX_RESULTS = 12  # Slightly increased to accommodate more sources\n",
        "WEB_SEARCH_DELAY = 3.0  # Separate delay for web search to avoid rate limits\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ============================================================================\n",
        "# STATE MANAGEMENT\n",
        "# ============================================================================\n",
        "\n",
        "@dataclass\n",
        "class GenerationState:\n",
        "    \"\"\"State for the generation workflow\"\"\"\n",
        "    research_goal: str = \"\"\n",
        "    constraints: List[str] = field(default_factory=list)\n",
        "    search_queries: List[str] = field(default_factory=list)\n",
        "    literature_findings: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    synthesized_knowledge: str = \"\"\n",
        "    generated_proposals: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    status: str = \"initialized\"\n",
        "    iteration: int = 0\n",
        "\n",
        "# ============================================================================\n",
        "# LITERATURE SEARCH FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def search_pubmed(query: str, max_results: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Search PubMed for biomedical literature\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Searching PubMed for: {query}\")\n",
        "\n",
        "        # Step 1: Search for PMIDs\n",
        "        search_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
        "        search_params = {\n",
        "            'db': 'pubmed',\n",
        "            'term': query,\n",
        "            'retmax': max_results,\n",
        "            'retmode': 'json',\n",
        "            'email': PUBMED_EMAIL,\n",
        "            'tool': 'ai_co_scientist_enhanced'\n",
        "        }\n",
        "\n",
        "        search_response = requests.get(search_url, params=search_params, timeout=10)\n",
        "        search_response.raise_for_status()\n",
        "\n",
        "        search_data = search_response.json()\n",
        "        pmids = search_data.get('esearchresult', {}).get('idlist', [])\n",
        "\n",
        "        if not pmids:\n",
        "            logger.warning(f\"No PMIDs found for query: {query}\")\n",
        "            return []\n",
        "\n",
        "        time.sleep(RATE_LIMIT_DELAY)\n",
        "\n",
        "        # Step 2: Fetch article details\n",
        "        fetch_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
        "        fetch_params = {\n",
        "            'db': 'pubmed',\n",
        "            'id': ','.join(pmids),\n",
        "            'retmode': 'xml',\n",
        "            'email': PUBMED_EMAIL,\n",
        "            'tool': 'ai_co_scientist_enhanced'\n",
        "        }\n",
        "\n",
        "        fetch_response = requests.get(fetch_url, params=fetch_params, timeout=15)\n",
        "        fetch_response.raise_for_status()\n",
        "\n",
        "        # Parse XML\n",
        "        root = ET.fromstring(fetch_response.text)\n",
        "        articles = []\n",
        "\n",
        "        for article in root.findall('.//PubmedArticle'):\n",
        "            title_elem = article.find('.//ArticleTitle')\n",
        "            abstract_elem = article.find('.//AbstractText')\n",
        "            pmid_elem = article.find('.//PMID')\n",
        "\n",
        "            # Extract authors\n",
        "            authors = []\n",
        "            for author in article.findall('.//Author')[:3]:\n",
        "                last_name = author.find('.//LastName')\n",
        "                fore_name = author.find('.//ForeName')\n",
        "                if last_name is not None and fore_name is not None:\n",
        "                    authors.append(f\"{fore_name.text} {last_name.text}\")\n",
        "\n",
        "            # Extract journal info\n",
        "            journal_elem = article.find('.//Journal/Title')\n",
        "            year_elem = article.find('.//PubDate/Year')\n",
        "\n",
        "            title = title_elem.text if title_elem is not None else \"No title\"\n",
        "            abstract = abstract_elem.text if abstract_elem is not None else \"No abstract available\"\n",
        "            pmid = pmid_elem.text if pmid_elem is not None else \"Unknown\"\n",
        "            journal = journal_elem.text if journal_elem is not None else \"Unknown journal\"\n",
        "            year = year_elem.text if year_elem is not None else \"Unknown year\"\n",
        "\n",
        "            # Extract key findings\n",
        "            key_findings = []\n",
        "            if abstract and abstract != \"No abstract available\":\n",
        "                keywords = ['significant', 'demonstrated', 'showed', 'found', 'revealed']\n",
        "                sentences = abstract.split('.')\n",
        "                for sentence in sentences:\n",
        "                    if any(keyword in sentence.lower() for keyword in keywords):\n",
        "                        if 20 < len(sentence) < 150:\n",
        "                            key_findings.append(sentence.strip())\n",
        "                            if len(key_findings) >= 2:\n",
        "                                break\n",
        "\n",
        "            articles.append({\n",
        "                'title': title,\n",
        "                'abstract': abstract,\n",
        "                'authors': authors,\n",
        "                'journal': journal,\n",
        "                'year': year,\n",
        "                'pmid': pmid,\n",
        "                'source': 'PubMed',\n",
        "                'url': f\"https://pubmed.ncbi.nlm.nih.gov/{pmid}\",\n",
        "                'key_findings': key_findings,\n",
        "                'relevance_score': 0.9,\n",
        "                'citation_count': 0  # PubMed doesn't provide citation counts\n",
        "            })\n",
        "\n",
        "        logger.info(f\"Retrieved {len(articles)} articles from PubMed\")\n",
        "        return articles\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"PubMed search failed for '{query}': {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def search_arxiv(query: str, max_results: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Search ArXiv for preprints\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Searching ArXiv for: {query}\")\n",
        "\n",
        "        base_url = \"http://export.arxiv.org/api/query\"\n",
        "        params = {\n",
        "            'search_query': f'all:{query}',\n",
        "            'start': 0,\n",
        "            'max_results': max_results,\n",
        "            'sortBy': 'relevance',\n",
        "            'sortOrder': 'descending'\n",
        "        }\n",
        "\n",
        "        response = requests.get(base_url, params=params, timeout=15)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse XML response\n",
        "        root = ET.fromstring(response.text)\n",
        "        ns = {'atom': 'http://www.w3.org/2005/Atom'}\n",
        "        entries = root.findall('atom:entry', ns)\n",
        "\n",
        "        if not entries:\n",
        "            logger.warning(f\"No papers found in ArXiv for: {query}\")\n",
        "            return []\n",
        "\n",
        "        articles = []\n",
        "        for entry in entries:\n",
        "            title = entry.find('atom:title', ns).text.strip()\n",
        "            summary = entry.find('atom:summary', ns).text.strip()\n",
        "\n",
        "            # Extract authors\n",
        "            authors = []\n",
        "            for author in entry.findall('atom:author', ns):\n",
        "                name = author.find('atom:name', ns)\n",
        "                if name is not None:\n",
        "                    authors.append(name.text)\n",
        "\n",
        "            # Extract publication date\n",
        "            published = entry.find('atom:published', ns).text\n",
        "            year = published[:4] if published else \"Unknown\"\n",
        "\n",
        "            # Extract ArXiv URL\n",
        "            arxiv_url = entry.find('atom:id', ns).text\n",
        "\n",
        "            # Extract key findings\n",
        "            key_findings = []\n",
        "            if summary:\n",
        "                keywords = ['significant', 'demonstrate', 'show', 'find', 'reveal', 'propose']\n",
        "                sentences = summary.split('.')\n",
        "                for sentence in sentences:\n",
        "                    if any(keyword in sentence.lower() for keyword in keywords):\n",
        "                        if 20 < len(sentence) < 150:\n",
        "                            key_findings.append(sentence.strip())\n",
        "                            if len(key_findings) >= 2:\n",
        "                                break\n",
        "\n",
        "            articles.append({\n",
        "                'title': title,\n",
        "                'abstract': summary,\n",
        "                'authors': authors[:3],  # First 3 authors\n",
        "                'journal': 'ArXiv Preprint',\n",
        "                'year': year,\n",
        "                'source': 'ArXiv',\n",
        "                'url': arxiv_url,\n",
        "                'key_findings': key_findings,\n",
        "                'relevance_score': 0.75,  # Lower since preprints\n",
        "                'citation_count': 0  # ArXiv doesn't provide citation counts\n",
        "            })\n",
        "\n",
        "        logger.info(f\"Retrieved {len(articles)} preprints from ArXiv\")\n",
        "        return articles\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"ArXiv search failed for '{query}': {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def search_crossref(query: str, max_results: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Search CrossRef for academic papers\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Searching CrossRef for: {query}\")\n",
        "\n",
        "        base_url = \"https://api.crossref.org/works\"\n",
        "        headers = {\n",
        "            'User-Agent': f'AI-Co-Scientist/1.0 (mailto:{CROSSREF_EMAIL})'\n",
        "        }\n",
        "\n",
        "        params = {\n",
        "            'query': query,\n",
        "            'rows': max_results,\n",
        "            'sort': 'relevance',\n",
        "            'select': 'title,author,published-print,abstract,DOI,URL,container-title'\n",
        "        }\n",
        "\n",
        "        response = requests.get(base_url, headers=headers, params=params, timeout=10)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        items = data.get('message', {}).get('items', [])\n",
        "\n",
        "        if not items:\n",
        "            logger.warning(f\"No papers found in CrossRef for: {query}\")\n",
        "            return []\n",
        "\n",
        "        articles = []\n",
        "        for item in items:\n",
        "            # Extract title\n",
        "            title_list = item.get('title', ['No title'])\n",
        "            title = title_list[0] if title_list else 'No title'\n",
        "\n",
        "            # Extract authors\n",
        "            authors = []\n",
        "            for author in item.get('author', [])[:3]:\n",
        "                given = author.get('given', '')\n",
        "                family = author.get('family', '')\n",
        "                if given or family:\n",
        "                    authors.append(f\"{given} {family}\".strip())\n",
        "\n",
        "            # Extract publication date\n",
        "            pub_date = item.get('published-print', {})\n",
        "            year = None\n",
        "            if pub_date and 'date-parts' in pub_date:\n",
        "                year = pub_date['date-parts'][0][0] if pub_date['date-parts'][0] else None\n",
        "\n",
        "            # Extract journal\n",
        "            container = item.get('container-title', [])\n",
        "            journal = container[0] if container else 'Unknown journal'\n",
        "\n",
        "            articles.append({\n",
        "                'title': title,\n",
        "                'abstract': 'Abstract not available from CrossRef',\n",
        "                'authors': authors,\n",
        "                'journal': journal,\n",
        "                'year': str(year) if year else 'Unknown',\n",
        "                'source': 'CrossRef',\n",
        "                'url': item.get('URL', ''),\n",
        "                'key_findings': [],\n",
        "                'relevance_score': 0.7,\n",
        "                'citation_count': 0  # CrossRef doesn't provide citation counts directly\n",
        "            })\n",
        "\n",
        "        logger.info(f\"Retrieved {len(articles)} papers from CrossRef\")\n",
        "        return articles\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"CrossRef search failed for '{query}': {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def search_web_academic(query: str, max_results: int = 2) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Search web for academic sources using DuckDuckGo with enhanced rate limiting\"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Searching web for academic sources: {query}\")\n",
        "\n",
        "        # Add longer delay to avoid rate limiting\n",
        "        time.sleep(WEB_SEARCH_DELAY)  # Use configured delay for web search\n",
        "\n",
        "        search_wrapper = DuckDuckGoSearchAPIWrapper(\n",
        "            region=\"us-en\",\n",
        "            time=\"y\",  # Past year\n",
        "            max_results=max_results\n",
        "        )\n",
        "        search_tool = DuckDuckGoSearchRun(api_wrapper=search_wrapper)\n",
        "\n",
        "        # Enhanced query for academic sources\n",
        "        academic_query = f\"{query} site:pubmed.ncbi.nlm.nih.gov OR site:scholar.google.com OR site:arxiv.org OR site:researchgate.net\"\n",
        "\n",
        "        results = search_tool.run(academic_query)\n",
        "\n",
        "        if not results or len(results.strip()) < 50:\n",
        "            logger.warning(f\"No meaningful web results for: {query}\")\n",
        "            return []\n",
        "\n",
        "        # Parse results (simplified)\n",
        "        result_sections = [section.strip() for section in results.split('\\n\\n') if len(section.strip()) > 50]\n",
        "\n",
        "        articles = []\n",
        "        for i, section in enumerate(result_sections[:max_results]):\n",
        "            articles.append({\n",
        "                'title': f\"Web Academic Result {i+1}: {query}\",\n",
        "                'abstract': section[:500] + \"...\" if len(section) > 500 else section,\n",
        "                'authors': ['Web Search'],\n",
        "                'journal': 'Web Academic Sources',\n",
        "                'year': str(datetime.now().year),\n",
        "                'source': 'Web Search',\n",
        "                'url': '',\n",
        "                'key_findings': [],\n",
        "                'relevance_score': 0.6,  # Lower relevance for web search\n",
        "                'citation_count': 0\n",
        "            })\n",
        "\n",
        "        logger.info(f\"Retrieved {len(articles)} web academic results\")\n",
        "        return articles\n",
        "\n",
        "    except Exception as e:\n",
        "        # Check if it's a rate limiting error\n",
        "        if \"202\" in str(e) or \"rate\" in str(e).lower():\n",
        "            logger.warning(f\"Rate limited for web search '{query}' - this is normal, continuing with other sources\")\n",
        "        else:\n",
        "            logger.error(f\"Web academic search failed for '{query}': {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def multi_source_literature_search(queries: List[str]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Search multiple literature sources with intelligent fallbacks\"\"\"\n",
        "\n",
        "    if not queries:\n",
        "        raise ValueError(\"No search queries provided\")\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # Define search sources with their functions\n",
        "    search_sources = [\n",
        "        (\"PubMed\", search_pubmed),\n",
        "        (\"ArXiv\", search_arxiv),\n",
        "        (\"CrossRef\", search_crossref),\n",
        "        (\"Web Academic\", search_web_academic)\n",
        "    ]\n",
        "\n",
        "    for query in queries:\n",
        "        logger.info(f\"Processing query: {query}\")\n",
        "        query_results = []\n",
        "\n",
        "        for source_name, search_func in search_sources:\n",
        "            try:\n",
        "                # Skip PubMed if email not configured properly\n",
        "                if source_name == \"PubMed\" and (not PUBMED_EMAIL or PUBMED_EMAIL == \"your_email@example.com\"):\n",
        "                    logger.warning(\"Skipping PubMed - email not configured\")\n",
        "                    continue\n",
        "\n",
        "                results = search_func(query, max_results=MAX_RESULTS_PER_SOURCE)\n",
        "                if results:  # Only add if we got results\n",
        "                    query_results.extend(results)\n",
        "\n",
        "                # Stop if we have enough results\n",
        "                if len(query_results) >= TOTAL_MAX_RESULTS:\n",
        "                    break\n",
        "\n",
        "                # Use different delays for different sources\n",
        "                if source_name == \"Web Academic\":\n",
        "                    time.sleep(WEB_SEARCH_DELAY)  # Longer delay for web search\n",
        "                else:\n",
        "                    time.sleep(RATE_LIMIT_DELAY)  # Standard delay for APIs\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"{source_name} failed for query '{query}': {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        all_results.extend(query_results)\n",
        "\n",
        "    # Remove duplicates based on title similarity\n",
        "    unique_results = []\n",
        "    seen_titles = set()\n",
        "\n",
        "    for result in all_results:\n",
        "        title_key = result['title'].lower().strip()\n",
        "        if title_key not in seen_titles and title_key != \"no title\":\n",
        "            seen_titles.add(title_key)\n",
        "            unique_results.append(result)\n",
        "\n",
        "    # Sort by relevance score and citation count\n",
        "    unique_results.sort(key=lambda x: (x['relevance_score'], x['citation_count']), reverse=True)\n",
        "\n",
        "    if not unique_results:\n",
        "        raise RuntimeError(\"Multi-source search returned no results. Try broader search terms.\")\n",
        "\n",
        "    logger.info(f\"Multi-source search completed: {len(unique_results)} unique results\")\n",
        "    return unique_results[:TOTAL_MAX_RESULTS]\n",
        "\n",
        "# ============================================================================\n",
        "# ENHANCED GENERATION AGENT (Simplified for demo)\n",
        "# ============================================================================\n",
        "\n",
        "class MockLLM:\n",
        "    \"\"\"Mock LLM for demonstration purposes\"\"\"\n",
        "\n",
        "    def invoke(self, messages):\n",
        "        class MockResponse:\n",
        "            def __init__(self, content):\n",
        "                self.content = content\n",
        "\n",
        "        # Simple mock responses based on the prompt content\n",
        "        prompt = messages[0][1] if isinstance(messages[0], tuple) else str(messages[0])\n",
        "\n",
        "        if \"Generate\" in prompt and \"search queries\" in prompt:\n",
        "            return MockResponse(\"\"\"1. AI personalized medicine neurodegenerative diseases\n",
        "2. machine learning precision medicine Alzheimer\n",
        "3. computational biomarkers neurodegeneration\n",
        "4. artificial intelligence clinical decision support\"\"\")\n",
        "\n",
        "        elif \"synthesize\" in prompt.lower():\n",
        "            return MockResponse(\"\"\"Based on the multi-source literature review, several key themes emerge:\n",
        "\n",
        "1. **Current AI Applications**: Machine learning models are showing promise in early detection of neurodegenerative diseases through biomarker analysis and neuroimaging.\n",
        "\n",
        "2. **Data Integration Challenges**: Current approaches struggle with integrating diverse data types (genomic, proteomic, imaging, clinical) into unified predictive models.\n",
        "\n",
        "3. **Personalization Gaps**: Most existing models are population-based rather than truly personalized, limiting their clinical utility.\n",
        "\n",
        "4. **Translational Barriers**: Despite promising research results, few AI-driven approaches have successfully transitioned to clinical practice.\n",
        "\n",
        "5. **Ethical Considerations**: Data privacy, algorithmic bias, and informed consent remain significant challenges in implementation.\"\"\")\n",
        "\n",
        "        else:  # hypothesis generation\n",
        "            return MockResponse(\"\"\"**Novel Hypotheses for AI-Driven Personalized Medicine in Neurodegeneration:**\n",
        "\n",
        "**Hypothesis 1: Multi-Modal Temporal Fusion**\n",
        "A federated learning approach combining longitudinal biomarker data, neuroimaging, and wearable sensor data can predict disease progression 2-3 years earlier than current methods while preserving patient privacy.\n",
        "\n",
        "**Hypothesis 2: Personalized Intervention Optimization**\n",
        "AI-driven digital twins of individual patients can simulate treatment responses across multiple therapeutic modalities, enabling personalized combination therapy selection with improved efficacy.\n",
        "\n",
        "**Hypothesis 3: Explainable AI for Clinical Trust**\n",
        "Integration of causal inference methods with deep learning can create interpretable models that not only predict outcomes but explain the biological mechanisms driving predictions, increasing physician adoption.\n",
        "\n",
        "**Key Experiments:**\n",
        "- Prospective cohort validation across diverse populations\n",
        "- Randomized controlled trials comparing AI-guided vs. standard care\n",
        "- Regulatory pathway development for AI-driven diagnostic tools\"\"\")\n",
        "\n",
        "# Create the agent class\n",
        "class EnhancedGenerationAgent:\n",
        "    \"\"\"Enhanced Generation Agent with multi-source literature search\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.llm = MockLLM()  # In practice, use actual LLM\n",
        "\n",
        "    def generate_search_queries(self, state: GenerationState) -> GenerationState:\n",
        "        \"\"\"Generate optimized search queries for multiple sources\"\"\"\n",
        "        print(f\"🔍 Generating multi-source search queries...\")\n",
        "\n",
        "        prompt = f\"\"\"Generate 3-4 effective search queries for academic literature about: {state.research_goal}\n",
        "\n",
        "Consider these constraints: {', '.join(state.constraints)}\n",
        "\n",
        "Create simple, effective queries for different academic databases\"\"\"\n",
        "\n",
        "        response = self.llm.invoke([(\"human\", prompt)])\n",
        "\n",
        "        # Parse queries with better cleaning\n",
        "        queries = []\n",
        "        lines = response.content.split('\\n')\n",
        "\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            # Remove markdown formatting and extract query\n",
        "            line = line.replace('*', '').replace('_', '')\n",
        "\n",
        "            # Look for numbered items\n",
        "            if any(line.startswith(str(i) + '.') for i in range(1, 10)):\n",
        "                # Extract everything after the number and period\n",
        "                query = line.split('.', 1)[1].strip() if '.' in line else line\n",
        "                query = query.replace('(', '').replace(')', '').strip()\n",
        "\n",
        "                if query and len(query) > 3:\n",
        "                    queries.append(query)\n",
        "\n",
        "        # Fallback queries if parsing failed\n",
        "        if not queries:\n",
        "            base_terms = state.research_goal.lower().split()[:3]\n",
        "            queries = [\n",
        "                ' '.join(base_terms),\n",
        "                f\"{base_terms[0]} treatment\" if len(base_terms) > 0 else \"treatment\",\n",
        "                f\"{base_terms[0]} therapy\" if len(base_terms) > 0 else \"therapy\",\n",
        "                \"clinical trial\"\n",
        "            ]\n",
        "\n",
        "        state.search_queries = queries[:4]\n",
        "        state.status = \"queries_generated\"\n",
        "\n",
        "        print(f\"✅ Generated {len(state.search_queries)} optimized queries\")\n",
        "        for i, q in enumerate(state.search_queries, 1):\n",
        "            print(f\"   {i}. {q}\")\n",
        "\n",
        "        return state\n",
        "\n",
        "    def literature_exploration(self, state: GenerationState) -> GenerationState:\n",
        "        \"\"\"Search literature using multiple sources\"\"\"\n",
        "        print(f\"📚 Searching multiple literature sources...\")\n",
        "\n",
        "        try:\n",
        "            findings = multi_source_literature_search(state.search_queries)\n",
        "            state.literature_findings = findings\n",
        "            state.status = \"literature_explored\"\n",
        "\n",
        "            # Print source breakdown\n",
        "            source_counts = {}\n",
        "            for finding in findings:\n",
        "                source = finding['source']\n",
        "                source_counts[source] = source_counts.get(source, 0) + 1\n",
        "\n",
        "            print(f\"✅ Found {len(findings)} papers from {len(source_counts)} sources:\")\n",
        "            for source, count in source_counts.items():\n",
        "                print(f\"   • {source}: {count} papers\")\n",
        "\n",
        "            return state\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Literature search failed: {str(e)}\")\n",
        "            raise RuntimeError(f\"Literature search failed: {str(e)}\")\n",
        "\n",
        "    def synthesize_knowledge(self, state: GenerationState) -> GenerationState:\n",
        "        \"\"\"Enhanced knowledge synthesis with source diversity\"\"\"\n",
        "        print(\"🧠 Synthesizing knowledge from multiple sources...\")\n",
        "\n",
        "        if not state.literature_findings:\n",
        "            print(\"⚠️ No literature findings to synthesize\")\n",
        "            state.synthesized_knowledge = \"No literature findings available for synthesis.\"\n",
        "            state.status = \"knowledge_synthesized\"\n",
        "            return state\n",
        "\n",
        "        # Create structured findings text\n",
        "        findings_text = \"Literature findings from multiple sources...\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke([\n",
        "                (\"human\", f\"\"\"Synthesize this diverse research for: {state.research_goal}\n",
        "\n",
        "Literature from Multiple Sources:\n",
        "{findings_text}\n",
        "\n",
        "Provide a comprehensive synthesis...\"\"\")\n",
        "            ])\n",
        "\n",
        "            state.synthesized_knowledge = response.content\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Knowledge synthesis failed: {str(e)}\")\n",
        "            state.synthesized_knowledge = f\"Knowledge synthesis encountered an error. Based on {len(state.literature_findings)} papers from multiple sources.\"\n",
        "\n",
        "        state.status = \"knowledge_synthesized\"\n",
        "        print(\"✅ Multi-source knowledge synthesis complete\")\n",
        "        return state\n",
        "\n",
        "    def generate_hypotheses(self, state: GenerationState) -> GenerationState:\n",
        "        \"\"\"Generate hypotheses based on multi-source synthesis\"\"\"\n",
        "        print(\"💡 Generating hypotheses from multi-source research...\")\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke([\n",
        "                (\"human\", f\"\"\"Generate novel hypotheses for: {state.research_goal}\n",
        "\n",
        "Based on: {state.synthesized_knowledge}\"\"\")\n",
        "            ])\n",
        "\n",
        "            hypothesis_content = response.content\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Hypothesis generation failed: {str(e)}\")\n",
        "            hypothesis_content = f\"Hypothesis generation encountered an error.\"\n",
        "\n",
        "        state.generated_proposals = [{\n",
        "            \"id\": \"multi_source_hypothesis\",\n",
        "            \"content\": hypothesis_content,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"based_on_sources\": len(state.literature_findings),\n",
        "            \"source_diversity\": len(set(f['source'] for f in state.literature_findings)) if state.literature_findings else 0\n",
        "        }]\n",
        "\n",
        "        state.status = \"hypotheses_generated\"\n",
        "        state.iteration += 1\n",
        "\n",
        "        source_count = len(state.literature_findings) if state.literature_findings else 0\n",
        "        source_diversity = len(set(f['source'] for f in state.literature_findings)) if state.literature_findings else 0\n",
        "\n",
        "        print(f\"✅ Generated hypotheses based on {source_count} papers from {source_diversity} sources\")\n",
        "        return state\n",
        "\n",
        "    def run_complete_workflow(self, research_goal: str, constraints: List[str] = None) -> GenerationState:\n",
        "        \"\"\"\n",
        "        Run the complete literature search workflow - Interface Contract Method\n",
        "\n",
        "        This method provides a unified interface for running the entire workflow\n",
        "        as specified in the agent interface contract.\n",
        "        \"\"\"\n",
        "        # Initialize state\n",
        "        state = GenerationState(\n",
        "            research_goal=research_goal,\n",
        "            constraints=constraints or []\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            # Run each step of the workflow in sequence\n",
        "            print(\"🔍 Starting complete literature search workflow...\")\n",
        "\n",
        "            # Step 1: Generate search queries\n",
        "            state = self.generate_search_queries(state)\n",
        "\n",
        "            # Step 2: Search literature\n",
        "            state = self.literature_exploration(state)\n",
        "\n",
        "            # Step 3: Synthesize knowledge\n",
        "            state = self.synthesize_knowledge(state)\n",
        "\n",
        "            # Step 4: Generate hypotheses\n",
        "            state = self.generate_hypotheses(state)\n",
        "\n",
        "            print(\"✅ Complete workflow finished successfully\")\n",
        "            return state\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Workflow failed at step: {state.status}\")\n",
        "            print(f\"Error: {str(e)}\")\n",
        "\n",
        "            # Return state with error information\n",
        "            state.status = f\"failed_at_{state.status}\"\n",
        "            state.generated_proposals = [{\n",
        "                \"id\": \"workflow_error\",\n",
        "                \"content\": f\"Workflow failed during {state.status}: {str(e)}\",\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"based_on_sources\": len(state.literature_findings),\n",
        "                \"source_diversity\": len(set(f['source'] for f in state.literature_findings)) if state.literature_findings else 0\n",
        "            }]\n",
        "\n",
        "            return state\n",
        "\n",
        "# ============================================================================\n",
        "# WORKFLOW CREATION AND EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "def create_enhanced_workflow():\n",
        "    \"\"\"Create enhanced workflow with multi-source search\"\"\"\n",
        "\n",
        "    agent = EnhancedGenerationAgent()\n",
        "    workflow = StateGraph(GenerationState)\n",
        "\n",
        "    workflow.add_node(\"generate_queries\", agent.generate_search_queries)\n",
        "    workflow.add_node(\"explore_literature\", agent.literature_exploration)\n",
        "    workflow.add_node(\"synthesize_knowledge\", agent.synthesize_knowledge)\n",
        "    workflow.add_node(\"generate_hypotheses\", agent.generate_hypotheses)\n",
        "\n",
        "    workflow.add_edge(START, \"generate_queries\")\n",
        "    workflow.add_edge(\"generate_queries\", \"explore_literature\")\n",
        "    workflow.add_edge(\"explore_literature\", \"synthesize_knowledge\")\n",
        "    workflow.add_edge(\"synthesize_knowledge\", \"generate_hypotheses\")\n",
        "    workflow.add_edge(\"generate_hypotheses\", END)\n",
        "\n",
        "    return workflow.compile(checkpointer=MemorySaver())\n",
        "\n",
        "def run_enhanced_agent(research_goal: str, constraints: List[str] = None):\n",
        "    \"\"\"Run enhanced multi-source generation agent\"\"\"\n",
        "\n",
        "    print(\"🚀 Enhanced AI Co-Scientist with Multi-Source Literature Search\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"📊 Configured Sources: PubMed, ArXiv, CrossRef, Web Academic\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    app = create_enhanced_workflow()\n",
        "\n",
        "    initial_state = GenerationState(\n",
        "        research_goal=research_goal,\n",
        "        constraints=constraints or []\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        final_state = app.invoke(initial_state, {\"configurable\": {\"thread_id\": \"enhanced_1\"}})\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"🎉 ENHANCED SEARCH SUCCESS!\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"\\n📋 Research Goal: {final_state.get('research_goal', 'Unknown')}\")\n",
        "\n",
        "        # Source breakdown with safety checks\n",
        "        literature_findings = final_state.get('literature_findings', [])\n",
        "        source_counts = {}\n",
        "\n",
        "        if literature_findings:\n",
        "            for paper in literature_findings:\n",
        "                source = paper.get('source', 'Unknown')\n",
        "                source_counts[source] = source_counts.get(source, 0) + 1\n",
        "\n",
        "        print(f\"\\n📚 Literature Found ({len(literature_findings)} papers):\")\n",
        "        if source_counts:\n",
        "            for source, count in source_counts.items():\n",
        "                print(f\"   • {source}: {count} papers\")\n",
        "        else:\n",
        "            print(\"   • No papers found\")\n",
        "\n",
        "        if literature_findings and len(literature_findings) > 0:\n",
        "            print(f\"\\n🔝 Top Papers by Relevance:\")\n",
        "            for i, paper in enumerate(literature_findings[:3], 1):\n",
        "                title = paper.get('title', 'No title')\n",
        "                source = paper.get('source', 'Unknown')\n",
        "                citations = paper.get('citation_count', 0)\n",
        "                print(f\"   {i}. {title}\")\n",
        "                print(f\"      Source: {source} | Citations: {citations}\")\n",
        "\n",
        "        proposals = final_state.get('generated_proposals', [])\n",
        "        if proposals and len(proposals) > 0:\n",
        "            print(f\"\\n💡 Generated Hypotheses:\")\n",
        "            content = proposals[0].get('content', 'No content available')\n",
        "            preview = content[:300] + \"...\" if len(content) > 300 else content\n",
        "            print(f\"   {preview}\")\n",
        "        else:\n",
        "            print(f\"\\n💡 No hypotheses generated\")\n",
        "\n",
        "        return final_state\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ Enhanced search failed: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test with a complex research goal\n",
        "    research_goal = \"Develop AI-driven personalized medicine approaches for neurodegenerative diseases\"\n",
        "    constraints = [\n",
        "        \"Must integrate multiple data modalities\",\n",
        "        \"Consider ethical implications\",\n",
        "        \"Focus on translational applications\"\n",
        "    ]\n",
        "\n",
        "    print(\"🧪 Testing Enhanced Multi-Source Agent...\")\n",
        "    result = run_enhanced_agent(research_goal, constraints)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af-8uN7xEfL4",
        "outputId": "42e0d431-5a94-499e-98ef-2540fb4d759c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Using real EnhancedGenerationAgent\n",
            "🔬 TESTING LITERATURE SEARCH AGENT INTERFACE\n",
            "=======================================================\n",
            "📥 INPUT TEST:\n",
            "   Research Goal: Develop AI-driven personalized medicine for neurodegeneration\n",
            "   Constraints: ['Must integrate multiple data modalities', 'Focus on translational applications', 'Consider ethical implications']\n",
            "🔍 Starting complete literature search workflow...\n",
            "🔍 Generating multi-source search queries...\n",
            "✅ Generated 4 optimized queries\n",
            "   1. AI personalized medicine neurodegenerative diseases\n",
            "   2. machine learning precision medicine Alzheimer\n",
            "   3. computational biomarkers neurodegeneration\n",
            "   4. artificial intelligence clinical decision support\n",
            "📚 Searching multiple literature sources...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:Rate limited for web search 'AI personalized medicine neurodegenerative diseases' - this is normal, continuing with other sources\n",
            "WARNING:__main__:Rate limited for web search 'machine learning precision medicine Alzheimer' - this is normal, continuing with other sources\n",
            "WARNING:__main__:Rate limited for web search 'computational biomarkers neurodegeneration' - this is normal, continuing with other sources\n",
            "WARNING:__main__:Rate limited for web search 'artificial intelligence clinical decision support' - this is normal, continuing with other sources\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Found 12 papers from 1 sources:\n",
            "   • PubMed: 12 papers\n",
            "🧠 Synthesizing knowledge from multiple sources...\n",
            "✅ Multi-source knowledge synthesis complete\n",
            "💡 Generating hypotheses from multi-source research...\n",
            "✅ Generated hypotheses based on 12 papers from 1 sources\n",
            "✅ Complete workflow finished successfully\n",
            "\n",
            "⏱️ EXECUTION: Completed in 59.77 seconds\n",
            "\n",
            "📤 OUTPUT VALIDATION:\n",
            "   ✅ All required GenerationState fields present\n",
            "\n",
            "📊 DATA STRUCTURE VALIDATION:\n",
            "   ✅ Search queries: 4 generated\n",
            "   ✅ Literature findings: 12 papers with correct structure\n",
            "       Sources: {'PubMed'}\n",
            "   ✅ Generated proposals: 1 hypotheses with correct structure\n",
            "   ✅ Status: hypotheses_generated\n",
            "\n",
            "📋 SAMPLE OUTPUT:\n",
            "   Proposal ID: multi_source_hypothesis\n",
            "   Content: **Novel Hypotheses for AI-Driven Personalized Medicine in Neurodegeneration:**\n",
            "\n",
            "**Hypothesis 1: Mult...\n",
            "   Based on 12 sources from 1 different databases\n",
            "\n",
            "✅ INTERFACE CONTRACT VALIDATION: PASSED\n",
            "\n",
            "=======================================================\n",
            "🎉 LITERATURE SEARCH AGENT INTERFACE: VALIDATED\n",
            "🔗 Ready for integration with other agents\n",
            "=======================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 🔬 Literature Search Agent - Interface Validation Test\n",
        "\n",
        "# ============================================================================\n",
        "# MINIMAL TEST SHIM FOR LITERATURE SEARCH AGENT\n",
        "# Validates agent works according to declared interface contract\n",
        "# ============================================================================\n",
        "\n",
        "import time\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Quick setup (assumes main agent code is available)\n",
        "try:\n",
        "    # Try to use the real agent from the notebook\n",
        "    agent = EnhancedGenerationAgent()\n",
        "    print(\"✅ Using real EnhancedGenerationAgent\")\n",
        "except NameError:\n",
        "    # Fallback to mock agent that follows the interface\n",
        "    print(\"⚠️ Real agent not found, using mock agent for interface validation\")\n",
        "\n",
        "    @dataclass\n",
        "    class MockGenerationState:\n",
        "        research_goal: str = \"\"\n",
        "        constraints: List[str] = field(default_factory=list)\n",
        "        search_queries: List[str] = field(default_factory=list)\n",
        "        literature_findings: List[Dict[str, Any]] = field(default_factory=list)\n",
        "        synthesized_knowledge: str = \"\"\n",
        "        generated_proposals: List[Dict[str, Any]] = field(default_factory=list)\n",
        "        status: str = \"initialized\"\n",
        "        iteration: int = 0\n",
        "\n",
        "    class MockEnhancedGenerationAgent:\n",
        "        def run_complete_workflow(self, research_goal: str, constraints: List[str] = None):\n",
        "            \"\"\"Mock implementation following the interface contract\"\"\"\n",
        "            state = MockGenerationState(research_goal=research_goal, constraints=constraints or [])\n",
        "\n",
        "            # Mock search queries\n",
        "            state.search_queries = [\n",
        "                f\"{research_goal.split()[0]} mechanisms\",\n",
        "                f\"{research_goal.split()[-1] if len(research_goal.split()) > 1 else 'therapy'} approaches\",\n",
        "                \"clinical applications\",\n",
        "                \"novel targets\"\n",
        "            ]\n",
        "\n",
        "            # Mock literature findings (follows exact interface spec)\n",
        "            state.literature_findings = [\n",
        "                {\n",
        "                    'title': f'Novel approaches to {research_goal.lower()}',\n",
        "                    'abstract': f'This study investigates {research_goal.lower()} using innovative methodologies...',\n",
        "                    'authors': ['Smith, J.', 'Johnson, A.', 'Williams, B.'],\n",
        "                    'journal': 'Nature Medicine',\n",
        "                    'year': '2024',\n",
        "                    'source': 'PubMed',\n",
        "                    'url': 'https://pubmed.ncbi.nlm.nih.gov/mock123',\n",
        "                    'key_findings': ['Significant improvement observed', 'Novel mechanism identified'],\n",
        "                    'relevance_score': 0.92,\n",
        "                    'citation_count': 45\n",
        "                },\n",
        "                {\n",
        "                    'title': f'Machine learning applications in {research_goal.lower()}',\n",
        "                    'abstract': f'We present a computational approach to {research_goal.lower()}...',\n",
        "                    'authors': ['Chen, L.', 'Davis, M.'],\n",
        "                    'journal': 'ArXiv Preprint',\n",
        "                    'year': '2024',\n",
        "                    'source': 'ArXiv',\n",
        "                    'url': 'https://arxiv.org/abs/2024.mock',\n",
        "                    'key_findings': ['AI model achieved 95% accuracy', 'Outperformed existing methods'],\n",
        "                    'relevance_score': 0.87,\n",
        "                    'citation_count': 12\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            # Mock knowledge synthesis\n",
        "            state.synthesized_knowledge = f\"\"\"\n",
        "            Based on analysis of {len(state.literature_findings)} papers from multiple sources,\n",
        "            current research in {research_goal.lower()} shows promising developments in both\n",
        "            experimental and computational approaches. Key themes include novel therapeutic\n",
        "            targets, AI-driven personalization, and translational applications.\n",
        "            \"\"\"\n",
        "\n",
        "            # Mock generated proposals (follows exact interface spec)\n",
        "            state.generated_proposals = [\n",
        "                {\n",
        "                    'id': f'lit_search_hyp_1_{int(time.time())}',\n",
        "                    'content': f'Novel hypothesis 1: Targeting specific molecular pathways in {research_goal.lower()} using personalized biomarker-guided approaches.',\n",
        "                    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S'),\n",
        "                    'based_on_sources': len(state.literature_findings),\n",
        "                    'source_diversity': len(set(f['source'] for f in state.literature_findings))\n",
        "                },\n",
        "                {\n",
        "                    'id': f'lit_search_hyp_2_{int(time.time())}',\n",
        "                    'content': f'Novel hypothesis 2: Combining AI-driven analysis with experimental validation to accelerate {research_goal.lower()} research.',\n",
        "                    'timestamp': time.strftime('%Y-%m-%dT%H:%M:%S'),\n",
        "                    'based_on_sources': len(state.literature_findings),\n",
        "                    'source_diversity': len(set(f['source'] for f in state.literature_findings))\n",
        "                }\n",
        "            ]\n",
        "\n",
        "            state.status = \"hypotheses_generated\"\n",
        "            state.iteration = 1\n",
        "            return state\n",
        "\n",
        "    agent = MockEnhancedGenerationAgent()\n",
        "\n",
        "# ============================================================================\n",
        "# INTERFACE CONTRACT VALIDATION TEST\n",
        "# ============================================================================\n",
        "\n",
        "def test_literature_search_interface():\n",
        "    \"\"\"\n",
        "    Tests the Literature Search Agent against its declared interface contract\n",
        "    \"\"\"\n",
        "    print(\"🔬 TESTING LITERATURE SEARCH AGENT INTERFACE\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    # Test inputs (as specified in interface)\n",
        "    test_research_goal = \"Develop AI-driven personalized medicine for neurodegeneration\"\n",
        "    test_constraints = [\n",
        "        \"Must integrate multiple data modalities\",\n",
        "        \"Focus on translational applications\",\n",
        "        \"Consider ethical implications\"\n",
        "    ]\n",
        "\n",
        "    print(f\"📥 INPUT TEST:\")\n",
        "    print(f\"   Research Goal: {test_research_goal}\")\n",
        "    print(f\"   Constraints: {test_constraints}\")\n",
        "\n",
        "    # Run the agent\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        result = agent.run_complete_workflow(test_research_goal, test_constraints)\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        print(f\"\\n⏱️ EXECUTION: Completed in {duration:.2f} seconds\")\n",
        "\n",
        "        # Validate output structure\n",
        "        print(f\"\\n📤 OUTPUT VALIDATION:\")\n",
        "\n",
        "        # Check GenerationState structure\n",
        "        required_fields = ['search_queries', 'literature_findings', 'synthesized_knowledge',\n",
        "                          'generated_proposals', 'status']\n",
        "        missing_fields = []\n",
        "\n",
        "        for field in required_fields:\n",
        "            if not hasattr(result, field):\n",
        "                missing_fields.append(field)\n",
        "\n",
        "        if missing_fields:\n",
        "            print(f\"   ❌ Missing required fields: {missing_fields}\")\n",
        "            return False\n",
        "        else:\n",
        "            print(f\"   ✅ All required GenerationState fields present\")\n",
        "\n",
        "        # Validate data structures\n",
        "        print(f\"\\n📊 DATA STRUCTURE VALIDATION:\")\n",
        "\n",
        "        # Check search_queries\n",
        "        if isinstance(result.search_queries, list) and len(result.search_queries) > 0:\n",
        "            print(f\"   ✅ Search queries: {len(result.search_queries)} generated\")\n",
        "        else:\n",
        "            print(f\"   ❌ Search queries: Invalid structure or empty\")\n",
        "\n",
        "        # Check literature_findings structure\n",
        "        if isinstance(result.literature_findings, list) and len(result.literature_findings) > 0:\n",
        "            # Validate first finding structure\n",
        "            first_finding = result.literature_findings[0]\n",
        "            required_finding_fields = ['title', 'abstract', 'authors', 'journal', 'year',\n",
        "                                     'source', 'url', 'key_findings', 'relevance_score', 'citation_count']\n",
        "\n",
        "            missing_finding_fields = [f for f in required_finding_fields if f not in first_finding]\n",
        "\n",
        "            if missing_finding_fields:\n",
        "                print(f\"   ❌ Literature findings: Missing fields {missing_finding_fields}\")\n",
        "            else:\n",
        "                print(f\"   ✅ Literature findings: {len(result.literature_findings)} papers with correct structure\")\n",
        "                print(f\"       Sources: {set(f['source'] for f in result.literature_findings)}\")\n",
        "        else:\n",
        "            print(f\"   ❌ Literature findings: Invalid structure or empty\")\n",
        "\n",
        "        # Check generated_proposals structure\n",
        "        if isinstance(result.generated_proposals, list) and len(result.generated_proposals) > 0:\n",
        "            first_proposal = result.generated_proposals[0]\n",
        "            required_proposal_fields = ['id', 'content', 'timestamp', 'based_on_sources', 'source_diversity']\n",
        "\n",
        "            missing_proposal_fields = [f for f in required_proposal_fields if f not in first_proposal]\n",
        "\n",
        "            if missing_proposal_fields:\n",
        "                print(f\"   ❌ Generated proposals: Missing fields {missing_proposal_fields}\")\n",
        "            else:\n",
        "                print(f\"   ✅ Generated proposals: {len(result.generated_proposals)} hypotheses with correct structure\")\n",
        "        else:\n",
        "            print(f\"   ❌ Generated proposals: Invalid structure or empty\")\n",
        "\n",
        "        # Check status\n",
        "        if hasattr(result, 'status') and result.status:\n",
        "            print(f\"   ✅ Status: {result.status}\")\n",
        "        else:\n",
        "            print(f\"   ❌ Status: Missing or empty\")\n",
        "\n",
        "        # Sample output display\n",
        "        print(f\"\\n📋 SAMPLE OUTPUT:\")\n",
        "        if result.generated_proposals:\n",
        "            sample_proposal = result.generated_proposals[0]\n",
        "            print(f\"   Proposal ID: {sample_proposal['id']}\")\n",
        "            print(f\"   Content: {sample_proposal['content'][:100]}...\")\n",
        "            print(f\"   Based on {sample_proposal['based_on_sources']} sources from {sample_proposal['source_diversity']} different databases\")\n",
        "\n",
        "        print(f\"\\n✅ INTERFACE CONTRACT VALIDATION: PASSED\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n❌ EXECUTION FAILED: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# ============================================================================\n",
        "# RUN THE INTERFACE TEST\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    success = test_literature_search_interface()\n",
        "\n",
        "    print(f\"\\n{'='*55}\")\n",
        "    if success:\n",
        "        print(\"🎉 LITERATURE SEARCH AGENT INTERFACE: VALIDATED\")\n",
        "        print(\"🔗 Ready for integration with other agents\")\n",
        "    else:\n",
        "        print(\"🚨 LITERATURE SEARCH AGENT INTERFACE: VALIDATION FAILED\")\n",
        "        print(\"🔧 Requires fixes before integration\")\n",
        "    print(\"=\"*55)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNtZrhutOgBA",
        "outputId": "be71a5f9-9343-4bbb-b394-81ffdfa2396e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🛡️ TESTING ROBUST REFLECTION AGENT\n",
            "=======================================================\n",
            "📋 Testing with 2 enhanced hypotheses\n",
            "\n",
            "🔍 TESTING ROBUST BATCH REVIEW\n",
            "----------------------------------------\n",
            "✅ Robust batch review completed\n",
            "   Total reviews: 2\n",
            "   Quality flags: 1\n",
            "\n",
            "📊 ROBUST BATCH SUMMARY\n",
            "----------------------------------------\n",
            "ROBUST REFLECTION AGENT BATCH SUMMARY\n",
            "\n",
            "📊 REVIEW OVERVIEW:\n",
            "• Total hypotheses reviewed: 2\n",
            "• Heuristic reviews: 0\n",
            "• Quick reviews: 0\n",
            "• Detailed reviews: 2\n",
            "\n",
            "📈 PERFORMANCE METRICS:\n",
            "• Mean overall score: 4.27/10\n",
            "• Score range: 1.0 - 7.5\n",
            "• Reviewer confidence: 7.0/10\n",
            "\n",
            "🎯 SCORE DISTRIBUTION:\n",
            "• Excellent (≥8.5): 0 hypotheses\n",
            "• High (7.5-8.4): 1 hypotheses\n",
            "• Moderate (5.0-7.4): 0 hypotheses\n",
            "• Low (<5.0): 1 hypotheses\n",
            "\n",
            "⚠️ QUALITY INDICATORS: LOW_AVERAGE_QUALITY\n",
            "\n",
            "🏆 TOP HYPOTHESIS: hypothesis_ai_personalized (Score: 7.55/10)\n",
            "\n",
            "💡 NEXT STEPS:\n",
            "• REVISE: Low quality scores - return to Generation Agent\n",
            "\n",
            "🔬 SAMPLE ROBUST REVIEW\n",
            "----------------------------------------\n",
            "Hypothesis: hypothesis_epigenetic\n",
            "Overall Score: 1.00/10\n",
            "Review Type: detailed\n",
            "Confidence: 7.0/10\n",
            "\n",
            "Criteria Breakdown:\n",
            "  Novelty: 1.0/10\n",
            "  Feasibility: 1.0/10\n",
            "  Scientific Rigor: 1.0/10\n",
            "  Impact Potential: 1.0/10\n",
            "  Testability: 1.0/10\n",
            "\n",
            "Robust Strengths (4):\n",
            "  1. Scientific Rigor (1-10): 7\n",
            "   [The scientific rigor is acceptable but needs strengthening\n",
            "  2. ]\n",
            "\n",
            "COMPREHENSIVE ASSESSMENT:\n",
            "\n",
            "Major Strengths:\n",
            "• The hypothesis addresses a significant unmet medical need (liver fibrosis)\n",
            "  3. Specifying the genes targeted by DNMT3A in HSCs that contribute to fibrosis would strengthen the hypothesis\n",
            "\n",
            "Identified Weaknesses (4):\n",
            "  1. Key Concerns:\n",
            "• The mechanism of action is not fully elucidated\n",
            "  2. • Specificity of DNMT3A inhibition in HSCs in vivo is a major concern\n",
            "  3. The experimental plan is generally sound, but needs further refinement to address concerns about specificity, mechanism of action, and statistical rig\n",
            "\n",
            "Overall Assessment:\n",
            "  DETAILED EVALUATION:\n",
            "\n",
            "1. Novelty (1-10): 7\n",
            "   [The hypothesis presents a reasonably novel approach by focusing on DNMT3A inhibition in hepatic stellate cells (HSCs) for fibrosis resolution. While epig...\n",
            "\n",
            "=======================================================\n",
            "🎉 ROBUST REFLECTION AGENT TESTING COMPLETE!\n",
            "🛡️ Enhanced error handling and flexible parsing working\n"
          ]
        }
      ],
      "source": [
        "#@title Robust Reflection Agent - Flexible LLM Output Parsing\n",
        "# ===========================================================================\n",
        "# ROBUST REFLECTION AGENT IMPLEMENTATION\n",
        "# Flexible Parsing to Handle Real LLM Output Variations\n",
        "# ===========================================================================\n",
        "\n",
        "# ===========================================================================\n",
        "# DATA MODELS\n",
        "# ===========================================================================\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import datetime # FIX: Changed import to be explicit\n",
        "import json\n",
        "import statistics\n",
        "import re\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class ReviewCriteria:\n",
        "    novelty_score: float\n",
        "    feasibility_score: float\n",
        "    scientific_rigor_score: float\n",
        "    impact_potential_score: float\n",
        "    testability_score: float\n",
        "    novelty_reasoning: str\n",
        "    feasibility_reasoning: str\n",
        "    scientific_rigor_reasoning: str\n",
        "    impact_potential_reasoning: str\n",
        "    testability_reasoning: str\n",
        "\n",
        "@dataclass\n",
        "class HypothesisReview:\n",
        "    hypothesis_id: str\n",
        "    hypothesis_text: str\n",
        "    criteria: ReviewCriteria\n",
        "    overall_score: float\n",
        "    overall_assessment: str\n",
        "    strengths: List[str]\n",
        "    weaknesses: List[str]\n",
        "    recommendations: List[str]\n",
        "    confidence_level: float\n",
        "    review_timestamp: str\n",
        "    reviewer_type: str\n",
        "\n",
        "@dataclass\n",
        "class ReflectionState:\n",
        "    hypothesis_reviews: List[HypothesisReview] = field(default_factory=list)\n",
        "    review_statistics: Optional[Dict[str, float]] = None\n",
        "    quality_flags: Optional[List[str]] = None\n",
        "    batch_summary: Optional[str] = None\n",
        "\n",
        "\n",
        "class RobustReflectionAgent:\n",
        "    \"\"\"\n",
        "    Robust Reflection Agent that can handle various LLM output formats\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.criteria_weights = {\n",
        "            'novelty_score': 0.25,\n",
        "            'feasibility_score': 0.20,\n",
        "            'scientific_rigor_score': 0.25,\n",
        "            'impact_potential_score': 0.20,\n",
        "            'testability_score': 0.10\n",
        "        }\n",
        "\n",
        "    def quick_review(self, hypothesis_text: str, hypothesis_id: str) -> HypothesisReview:\n",
        "        \"\"\"Simplified quick review with more flexible parsing\"\"\"\n",
        "\n",
        "        logger.info(f\"Performing quick review of hypothesis: {hypothesis_id}\")\n",
        "\n",
        "        quick_prompt = f\"\"\"You are a scientific peer reviewer. Please evaluate this hypothesis:\n",
        "\n",
        "HYPOTHESIS: {hypothesis_text}\n",
        "\n",
        "Please provide your evaluation in this format:\n",
        "\n",
        "**SCORES (1-10 scale):**\n",
        "Novelty: [score] - [brief reason]\n",
        "Feasibility: [score] - [brief reason]\n",
        "Scientific Rigor: [score] - [brief reason]\n",
        "Impact Potential: [score] - [brief reason]\n",
        "Testability: [score] - [brief reason]\n",
        "\n",
        "**STRENGTHS:**\n",
        "• [strength 1]\n",
        "• [strength 2]\n",
        "• [strength 3]\n",
        "\n",
        "**WEAKNESSES:**\n",
        "• [weakness 1]\n",
        "• [weakness 2]\n",
        "• [weakness 3]\n",
        "\n",
        "**RECOMMENDATIONS:**\n",
        "• [recommendation 1]\n",
        "• [recommendation 2]\n",
        "• [recommendation 3]\n",
        "\n",
        "**OVERALL:** [2-3 sentence summary assessment]\n",
        "\n",
        "**CONFIDENCE:** [1-10] in your review accuracy\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke([(\"human\", quick_prompt)])\n",
        "            return self._parse_flexible_review(response.content, hypothesis_text, hypothesis_id, \"quick\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Quick review failed for {hypothesis_id}: {str(e)}\")\n",
        "            return self._create_enhanced_fallback_review(hypothesis_text, hypothesis_id, \"quick\")\n",
        "\n",
        "    def detailed_review(self, hypothesis_text: str, hypothesis_id: str, research_goal: str = \"\") -> HypothesisReview:\n",
        "        \"\"\"Detailed review with flexible parsing\"\"\"\n",
        "\n",
        "        logger.info(f\"Performing detailed review of hypothesis: {hypothesis_id}\")\n",
        "\n",
        "        detailed_prompt = f\"\"\"You are a senior scientific peer reviewer. Conduct a comprehensive evaluation of this hypothesis.\n",
        "\n",
        "RESEARCH CONTEXT: {research_goal}\n",
        "\n",
        "HYPOTHESIS: {hypothesis_text}\n",
        "\n",
        "Please structure your review as follows:\n",
        "\n",
        "**DETAILED EVALUATION:**\n",
        "\n",
        "1. **Novelty (1-10):** [score]\n",
        "   [Detailed assessment of originality and innovation]\n",
        "\n",
        "2. **Feasibility (1-10):** [score]\n",
        "   [Analysis of technical achievability and resource requirements]\n",
        "\n",
        "3. **Scientific Rigor (1-10):** [score]\n",
        "   [Evaluation of methodological soundness and evidence base]\n",
        "\n",
        "4. **Impact Potential (1-10):** [score]\n",
        "   [Assessment of significance and broader implications]\n",
        "\n",
        "5. **Testability (1-10):** [score]\n",
        "   [Analysis of experimental validation approaches]\n",
        "\n",
        "**COMPREHENSIVE ASSESSMENT:**\n",
        "\n",
        "**Major Strengths:**\n",
        "• [detailed strength 1]\n",
        "• [detailed strength 2]\n",
        "• [detailed strength 3]\n",
        "• [detailed strength 4]\n",
        "\n",
        "**Key Concerns:**\n",
        "• [detailed concern 1]\n",
        "• [detailed concern 2]\n",
        "• [detailed concern 3]\n",
        "• [detailed concern 4]\n",
        "\n",
        "**Improvement Recommendations:**\n",
        "• [specific recommendation 1]\n",
        "• [specific recommendation 2]\n",
        "• [specific recommendation 3]\n",
        "• [specific recommendation 4]\n",
        "\n",
        "**Summary:** [3-4 sentence overall evaluation]\n",
        "\n",
        "**Review Confidence:** [1-10]\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke([(\"human\", detailed_prompt)])\n",
        "            return self._parse_flexible_review(response.content, hypothesis_text, hypothesis_id, \"detailed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Detailed review failed for {hypothesis_id}: {str(e)}\")\n",
        "            return self._create_enhanced_fallback_review(hypothesis_text, hypothesis_id, \"detailed\")\n",
        "\n",
        "    def _parse_flexible_review(self, response_text: str, hypothesis_text: str,\n",
        "                              hypothesis_id: str, review_type: str) -> HypothesisReview:\n",
        "        \"\"\"Flexible parsing that handles various LLM output formats\"\"\"\n",
        "\n",
        "        logger.info(f\"Parsing review response for {hypothesis_id}\")\n",
        "\n",
        "        try:\n",
        "            # Clean the response text\n",
        "            cleaned_text = response_text.replace('*', '').replace('#', '').replace('`', '')\n",
        "\n",
        "            # Extract scores using multiple pattern attempts\n",
        "            scores = self._extract_scores_flexible(cleaned_text)\n",
        "\n",
        "            # Extract confidence\n",
        "            confidence = self._extract_confidence_flexible(cleaned_text)\n",
        "\n",
        "            # Extract lists using flexible patterns\n",
        "            strengths = self._extract_lists_flexible(cleaned_text, [\"strength\", \"strengths\", \"positive\", \"advantages\"])\n",
        "            weaknesses = self._extract_lists_flexible(cleaned_text, [\"weakness\", \"weaknesses\", \"concern\", \"concerns\", \"limitation\", \"limitations\"])\n",
        "            recommendations = self._extract_lists_flexible(cleaned_text, [\"recommendation\", \"recommendations\", \"suggest\", \"improve\"])\n",
        "\n",
        "            # Extract overall assessment\n",
        "            overall_assessment = self._extract_overall_flexible(cleaned_text)\n",
        "\n",
        "            # Create criteria with extracted reasoning\n",
        "            criteria = ReviewCriteria(\n",
        "                novelty_score=scores.get('novelty', 6.0),\n",
        "                feasibility_score=scores.get('feasibility', 6.0),\n",
        "                scientific_rigor_score=scores.get('rigor', 6.0),\n",
        "                impact_potential_score=scores.get('impact', 6.0),\n",
        "                testability_score=scores.get('testability', 6.0),\n",
        "                novelty_reasoning=f\"Novelty assessment: {scores.get('novelty', 6.0)}/10\",\n",
        "                feasibility_reasoning=f\"Feasibility assessment: {scores.get('feasibility', 6.0)}/10\",\n",
        "                scientific_rigor_reasoning=f\"Scientific rigor assessment: {scores.get('rigor', 6.0)}/10\",\n",
        "                impact_potential_reasoning=f\"Impact potential assessment: {scores.get('impact', 6.0)}/10\",\n",
        "                testability_reasoning=f\"Testability assessment: {scores.get('testability', 6.0)}/10\"\n",
        "            )\n",
        "\n",
        "            # Calculate weighted overall score\n",
        "            overall_score = (\n",
        "                scores.get('novelty', 6.0) * 0.25 +\n",
        "                scores.get('feasibility', 6.0) * 0.20 +\n",
        "                scores.get('rigor', 6.0) * 0.25 +\n",
        "                scores.get('impact', 6.0) * 0.20 +\n",
        "                scores.get('testability', 6.0) * 0.10\n",
        "            )\n",
        "\n",
        "            return HypothesisReview(\n",
        "                hypothesis_id=hypothesis_id,\n",
        "                hypothesis_text=hypothesis_text[:500] + \"...\" if len(hypothesis_text) > 500 else hypothesis_text,\n",
        "                criteria=criteria,\n",
        "                overall_score=overall_score,\n",
        "                overall_assessment=overall_assessment,\n",
        "                strengths=strengths,\n",
        "                weaknesses=weaknesses,\n",
        "                recommendations=recommendations,\n",
        "                confidence_level=confidence,\n",
        "                # FIX: Corrected datetime call\n",
        "                review_timestamp=datetime.datetime.now().isoformat(),\n",
        "                reviewer_type=review_type\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Flexible parsing failed for {hypothesis_id}: {str(e)}\")\n",
        "            # If all parsing fails, create enhanced fallback\n",
        "            return self._create_enhanced_fallback_review(hypothesis_text, hypothesis_id, review_type)\n",
        "\n",
        "    def _extract_scores_flexible(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\"Extract scores using multiple flexible patterns\"\"\"\n",
        "\n",
        "        scores = {}\n",
        "\n",
        "        # Define multiple patterns for each criterion\n",
        "        pattern_sets = {\n",
        "            'novelty': [\n",
        "                r'novelty.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'original.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'innovation.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'novel.*?(\\\\d+(?:\\\\.\\\\d+)?)'\n",
        "            ],\n",
        "            'feasibility': [\n",
        "                r'feasibility.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'feasible.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'achievable.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'practical.*?(\\\\d+(?:\\\\.\\\\d+)?)'\n",
        "            ],\n",
        "            'rigor': [\n",
        "                r'rigor.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'rigorous.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'scientific.*?rigor.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'methodological.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'evidence.*?(\\\\d+(?:\\\\.\\\\d+)?)'\n",
        "            ],\n",
        "            'impact': [\n",
        "                r'impact.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'significance.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'important.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'potential.*?(\\\\d+(?:\\\\.\\\\d+)?)'\n",
        "            ],\n",
        "            'testability': [\n",
        "                r'testability.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'testable.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'validation.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "                r'experimental.*?(\\\\d+(?:\\\\.\\\\d+)?)'\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        for criterion, patterns in pattern_sets.items():\n",
        "            score_found = False\n",
        "            for pattern in patterns:\n",
        "                match = re.search(pattern, text.lower())\n",
        "                if match:\n",
        "                    try:\n",
        "                        score = float(match.group(1))\n",
        "                        if 1 <= score <= 10:  # Validate score range\n",
        "                            scores[criterion] = score\n",
        "                            score_found = True\n",
        "                            break\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "            if not score_found:\n",
        "                # Look for any number in proximity to criterion words\n",
        "                criterion_word = criterion if criterion != 'rigor' else 'rigor'\n",
        "                proximity_pattern = f'{criterion_word}.{{0,50}}?(\\\\d+)'\n",
        "                match = re.search(proximity_pattern, text.lower())\n",
        "                if match:\n",
        "                    try:\n",
        "                        score = float(match.group(1))\n",
        "                        if 1 <= score <= 10:\n",
        "                            scores[criterion] = score\n",
        "                        else:\n",
        "                            scores[criterion] = 6.0  # Default\n",
        "                    except ValueError:\n",
        "                        scores[criterion] = 6.0\n",
        "                else:\n",
        "                    scores[criterion] = 6.0\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def _extract_confidence_flexible(self, text: str) -> float:\n",
        "        \"\"\"Extract confidence score using flexible patterns\"\"\"\n",
        "\n",
        "        patterns = [\n",
        "            r'confidence.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "            r'confident.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "            r'certainty.*?(\\\\d+(?:\\\\.\\\\d+)?)',\n",
        "            r'sure.*?(\\\\d+(?:\\\\.\\\\d+)?)'\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text.lower())\n",
        "            if match:\n",
        "                try:\n",
        "                    confidence = float(match.group(1))\n",
        "                    if 1 <= confidence <= 10:\n",
        "                        return confidence\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "        return 7.0  # Default confidence\n",
        "\n",
        "    def _extract_lists_flexible(self, text: str, keywords: List[str]) -> List[str]:\n",
        "        \"\"\"Extract bulleted or numbered lists using flexible patterns\"\"\"\n",
        "\n",
        "        items = []\n",
        "\n",
        "        for keyword in keywords:\n",
        "            # Find section containing the keyword\n",
        "            lines = text.split('\\\\n')\n",
        "            section_started = False\n",
        "\n",
        "            for i, line in enumerate(lines):\n",
        "                if keyword.lower() in line.lower() and ':' in line:\n",
        "                    section_started = True\n",
        "                    continue\n",
        "\n",
        "                if section_started:\n",
        "                    line = line.strip()\n",
        "\n",
        "                    # Stop if we hit another section header\n",
        "                    if line and line.endswith(':') and any(word in line.lower() for word in ['score', 'assessment', 'evaluation', 'summary']):\n",
        "                        break\n",
        "\n",
        "                    # Extract bullet points or numbered items\n",
        "                    if re.match(r'^[-•*]\\\\s+', line) or re.match(r'^\\\\d+\\\\.\\\\s+', line):\n",
        "                        cleaned_line = re.sub(r'^[-•*\\\\d.]\\\\s*', '', line)\n",
        "                        if len(cleaned_line) > 5:  # Ensure meaningful content\n",
        "                            items.append(cleaned_line[:150])  # Limit length\n",
        "\n",
        "                    # Also catch lines that start with text but are clearly list items\n",
        "                    elif line and not line.startswith(('**', '#', 'Score', 'Assessment')):\n",
        "                        if 10 < len(line) < 200:  # Reasonable length for list item\n",
        "                            items.append(line[:150])\n",
        "\n",
        "            if items:  # If we found items with this keyword, use them\n",
        "                break\n",
        "\n",
        "        # If no structured lists found, try to extract from general text\n",
        "        if not items:\n",
        "            # Look for sentences that contain action words or assessment language\n",
        "            sentences = re.split(r'[.!?]+', text)\n",
        "            for sentence in sentences:\n",
        "                sentence = sentence.strip()\n",
        "                if (any(keyword.lower() in sentence.lower() for keyword in keywords) and\n",
        "                    len(sentence) > 15 and len(sentence) < 200):\n",
        "                    items.append(sentence[:150])\n",
        "\n",
        "        return items[:4] if items else [f\"No specific {keywords[0]} identified\"]\n",
        "\n",
        "    def _extract_overall_flexible(self, text: str) -> str:\n",
        "        \"\"\"Extract overall assessment using flexible patterns\"\"\"\n",
        "\n",
        "        keywords = [\"overall\", \"summary\", \"assessment\", \"conclusion\", \"evaluation\"]\n",
        "\n",
        "        for keyword in keywords:\n",
        "            # Look for keyword followed by colon\n",
        "            pattern = f'{keyword}.*?:(.*?)(?:[A-Z]{{2,}}.*?:|$)'\n",
        "            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                assessment = match.group(1).strip()\n",
        "                if len(assessment) > 20:\n",
        "                    return assessment[:400] + \"...\" if len(assessment) > 400 else assessment\n",
        "\n",
        "        # Fallback: look for any substantial paragraph\n",
        "        paragraphs = [p.strip() for p in text.split('\\\\n\\\\n') if len(p.strip()) > 50]\n",
        "        if paragraphs:\n",
        "            # Take the longest paragraph as likely assessment\n",
        "            longest = max(paragraphs, key=len)\n",
        "            return longest[:400] + \"...\" if len(longest) > 400 else longest\n",
        "\n",
        "        return \"Overall assessment could not be extracted from review.\"\n",
        "\n",
        "    def _create_enhanced_fallback_review(self, hypothesis_text: str, hypothesis_id: str, review_type: str) -> HypothesisReview:\n",
        "        \"\"\"Create an enhanced fallback review with better default scoring\"\"\"\n",
        "\n",
        "        # Analyze hypothesis text to provide more realistic scores\n",
        "        text_lower = hypothesis_text.lower()\n",
        "\n",
        "        # Heuristic scoring based on content analysis\n",
        "        novelty_score = 6.5 if any(word in text_lower for word in ['novel', 'new', 'innovative', 'breakthrough']) else 5.5\n",
        "        feasibility_score = 7.0 if any(word in text_lower for word in ['demonstrated', 'established', 'proven', 'validated']) else 6.0\n",
        "        rigor_score = 7.0 if any(word in text_lower for word in ['literature', 'evidence', 'studies', 'research']) else 5.5\n",
        "        impact_score = 7.5 if any(word in text_lower for word in ['therapeutic', 'clinical', 'treatment', 'therapy']) else 6.0\n",
        "        testability_score = 7.0 if any(word in text_lower for word in ['experiment', 'test', 'measure', 'assay']) else 6.0\n",
        "\n",
        "        criteria = ReviewCriteria(\n",
        "            novelty_score=novelty_score,\n",
        "            feasibility_score=feasibility_score,\n",
        "            scientific_rigor_score=rigor_score,\n",
        "            impact_potential_score=impact_score,\n",
        "            testability_score=testability_score,\n",
        "            novelty_reasoning=f\"Heuristic assessment based on text analysis: {novelty_score}/10\",\n",
        "            feasibility_reasoning=f\"Heuristic assessment based on text analysis: {feasibility_score}/10\",\n",
        "            scientific_rigor_reasoning=f\"Heuristic assessment based on text analysis: {rigor_score}/10\",\n",
        "            impact_potential_reasoning=f\"Heuristic assessment based on text analysis: {impact_score}/10\",\n",
        "            testability_reasoning=f\"Heuristic assessment based on text analysis: {testability_score}/10\"\n",
        "        )\n",
        "\n",
        "        # Calculate weighted overall score\n",
        "        overall_score = (novelty_score * 0.25 + feasibility_score * 0.20 +\n",
        "                        rigor_score * 0.25 + impact_score * 0.20 + testability_score * 0.10)\n",
        "\n",
        "        # Generate heuristic strengths and weaknesses\n",
        "        strengths = [\"Addresses important research question\"]\n",
        "        if \"epigenetic\" in text_lower:\n",
        "            strengths.append(\"Focuses on promising epigenetic mechanisms\")\n",
        "        if \"therapeutic\" in text_lower or \"treatment\" in text_lower:\n",
        "            strengths.append(\"Has clear therapeutic relevance\")\n",
        "        if \"experiment\" in text_lower:\n",
        "            strengths.append(\"Includes experimental validation approach\")\n",
        "\n",
        "        weaknesses = [\"Requires detailed expert review for accurate assessment\"]\n",
        "        if len(hypothesis_text) < 200:\n",
        "            weaknesses.append(\"Limited detail in hypothesis description\")\n",
        "        if \"novel\" not in text_lower and \"new\" not in text_lower:\n",
        "            weaknesses.append(\"Novelty could be better articulated\")\n",
        "\n",
        "        recommendations = [\"Conduct detailed expert peer review\"]\n",
        "        if \"literature\" not in text_lower:\n",
        "            recommendations.append(\"Strengthen literature foundation\")\n",
        "        if \"experiment\" not in text_lower:\n",
        "            recommendations.append(\"Develop specific experimental protocols\")\n",
        "\n",
        "        return HypothesisReview(\n",
        "            hypothesis_id=hypothesis_id,\n",
        "            hypothesis_text=hypothesis_text[:300] + \"...\" if len(hypothesis_text) > 300 else hypothesis_text,\n",
        "            criteria=criteria,\n",
        "            overall_score=overall_score,\n",
        "            overall_assessment=f\"Automated heuristic assessment. The hypothesis shows promise with an overall score of {overall_score:.1f}/10. Manual expert review recommended for detailed evaluation.\",\n",
        "            strengths=strengths,\n",
        "            weaknesses=weaknesses,\n",
        "            recommendations=recommendations,\n",
        "            confidence_level=5.0,  # Medium confidence for heuristic assessment\n",
        "            # FIX: Corrected datetime call\n",
        "            review_timestamp=datetime.datetime.now().isoformat(),\n",
        "            reviewer_type=review_type + \"_heuristic\"\n",
        "        )\n",
        "\n",
        "    def adaptive_batch_review(self, hypotheses: List[Dict[str, Any]], research_goal: str = \"\",\n",
        "                            quick_threshold: float = 6.5, detailed_threshold: float = 7.5) -> ReflectionState:\n",
        "        \"\"\"Robust batch review with enhanced error handling\"\"\"\n",
        "\n",
        "        logger.info(f\"Starting robust batch review of {len(hypotheses)} hypotheses\")\n",
        "\n",
        "        reflection_state = ReflectionState()\n",
        "\n",
        "        for i, hypothesis in enumerate(hypotheses):\n",
        "            hypothesis_id = hypothesis.get('id', f\"hypothesis_{i+1}\")\n",
        "            hypothesis_text = hypothesis.get('content', str(hypothesis))\n",
        "\n",
        "            # Always start with quick review\n",
        "            quick_review = self.quick_review(hypothesis_text, hypothesis_id)\n",
        "\n",
        "            # Decide on detailed review based on score and review quality\n",
        "            if (quick_review.overall_score >= detailed_threshold and\n",
        "                not quick_review.reviewer_type.endswith('_heuristic')):\n",
        "                logger.info(f\"Hypothesis {hypothesis_id} scored {quick_review.overall_score:.1f}, performing detailed review\")\n",
        "                detailed_review = self.detailed_review(hypothesis_text, hypothesis_id, research_goal)\n",
        "                reflection_state.hypothesis_reviews.append(detailed_review)\n",
        "            else:\n",
        "                logger.info(f\"Hypothesis {hypothesis_id} scored {quick_review.overall_score:.1f}, using quick review\")\n",
        "                reflection_state.hypothesis_reviews.append(quick_review)\n",
        "\n",
        "        # Generate comprehensive analytics\n",
        "        reflection_state.review_statistics = self._compute_robust_statistics(reflection_state.hypothesis_reviews)\n",
        "        reflection_state.quality_flags = self._identify_robust_quality_flags(reflection_state.hypothesis_reviews)\n",
        "        reflection_state.batch_summary = self._generate_robust_summary(reflection_state)\n",
        "\n",
        "        logger.info(f\"Robust batch review completed: {len(reflection_state.hypothesis_reviews)} reviews generated\")\n",
        "        return reflection_state\n",
        "\n",
        "    def _compute_robust_statistics(self, reviews: List[HypothesisReview]) -> Dict[str, float]:\n",
        "        \"\"\"Compute robust statistics with better error handling\"\"\"\n",
        "\n",
        "        if not reviews:\n",
        "            return {'error': 'No reviews available'}\n",
        "\n",
        "        overall_scores = [r.overall_score for r in reviews if r.overall_score is not None]\n",
        "        confidence_scores = [r.confidence_level for r in reviews if r.confidence_level is not None]\n",
        "\n",
        "        if not overall_scores:\n",
        "            return {'error': 'No valid scores found'}\n",
        "\n",
        "        criteria_scores = {\n",
        "            'novelty': [r.criteria.novelty_score for r in reviews if r.criteria and r.criteria.novelty_score is not None],\n",
        "            'feasibility': [r.criteria.feasibility_score for r in reviews if r.criteria and r.criteria.feasibility_score is not None],\n",
        "            'rigor': [r.criteria.scientific_rigor_score for r in reviews if r.criteria and r.criteria.scientific_rigor_score is not None],\n",
        "            'impact': [r.criteria.impact_potential_score for r in reviews if r.criteria and r.criteria.impact_potential_score is not None],\n",
        "            'testability': [r.criteria.testability_score for r in reviews if r.criteria and r.criteria.testability_score is not None]\n",
        "        }\n",
        "\n",
        "        stats = {\n",
        "            'mean_overall_score': statistics.mean(overall_scores),\n",
        "            'median_overall_score': statistics.median(overall_scores),\n",
        "            'std_overall_score': statistics.stdev(overall_scores) if len(overall_scores) > 1 else 0,\n",
        "            'max_overall_score': max(overall_scores),\n",
        "            'min_overall_score': min(overall_scores),\n",
        "            'mean_confidence': statistics.mean(confidence_scores) if confidence_scores else 5.0,\n",
        "            'excellent_count': len([s for s in overall_scores if s >= 8.5]),\n",
        "            'high_scoring_count': len([s for s in overall_scores if s >= 7.5]),\n",
        "            'moderate_scoring_count': len([s for s in overall_scores if 5.0 <= s < 7.5]),\n",
        "            'low_scoring_count': len([s for s in overall_scores if s < 5.0]),\n",
        "            'detailed_review_count': len([r for r in reviews if r.reviewer_type == \"detailed\"]),\n",
        "            'quick_review_count': len([r for r in reviews if r.reviewer_type == \"quick\"]),\n",
        "            'heuristic_review_count': len([r for r in reviews if \"heuristic\" in r.reviewer_type])\n",
        "        }\n",
        "\n",
        "        # Add criteria-specific statistics safely\n",
        "        for criterion, scores in criteria_scores.items():\n",
        "            if scores:\n",
        "                stats[f'mean_{criterion}'] = statistics.mean(scores)\n",
        "                stats[f'max_{criterion}'] = max(scores)\n",
        "                stats[f'min_{criterion}'] = min(scores)\n",
        "            else:\n",
        "                stats[f'mean_{criterion}'] = 6.0\n",
        "                stats[f'max_{criterion}'] = 6.0\n",
        "                stats[f'min_{criterion}'] = 6.0\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def _identify_robust_quality_flags(self, reviews: List[HypothesisReview]) -> List[str]:\n",
        "        \"\"\"Identify quality flags with robust error handling\"\"\"\n",
        "\n",
        "        flags = []\n",
        "\n",
        "        if not reviews:\n",
        "            flags.append(\"NO_REVIEWS_GENERATED\")\n",
        "            return flags\n",
        "\n",
        "        # Count review types\n",
        "        heuristic_count = len([r for r in reviews if \"heuristic\" in r.reviewer_type])\n",
        "        total_count = len(reviews)\n",
        "\n",
        "        if heuristic_count == total_count:\n",
        "            flags.append(\"ALL_HEURISTIC_REVIEWS\")\n",
        "        elif heuristic_count > total_count * 0.5:\n",
        "            flags.append(\"MAJORITY_HEURISTIC_REVIEWS\")\n",
        "\n",
        "        # Check scores safely\n",
        "        overall_scores = [r.overall_score for r in reviews if r.overall_score is not None]\n",
        "        confidence_scores = [r.confidence_level for r in reviews if r.confidence_level is not None]\n",
        "\n",
        "        if overall_scores:\n",
        "            mean_score = statistics.mean(overall_scores)\n",
        "            if mean_score < 5.0:\n",
        "                flags.append(\"LOW_AVERAGE_QUALITY\")\n",
        "            elif mean_score >= 8.0:\n",
        "                flags.append(\"HIGH_QUALITY_BATCH\")\n",
        "\n",
        "        if confidence_scores:\n",
        "            mean_confidence = statistics.mean(confidence_scores)\n",
        "            if mean_confidence < 6.0:\n",
        "                flags.append(\"LOW_REVIEWER_CONFIDENCE\")\n",
        "            elif mean_confidence >= 8.5:\n",
        "                flags.append(\"HIGH_REVIEWER_CONFIDENCE\")\n",
        "\n",
        "        return flags\n",
        "\n",
        "    def _generate_robust_summary(self, reflection_state: ReflectionState) -> str:\n",
        "        \"\"\"Generate robust summary with safe data access\"\"\"\n",
        "\n",
        "        stats = reflection_state.review_statistics\n",
        "        flags = reflection_state.quality_flags\n",
        "        reviews = reflection_state.hypothesis_reviews\n",
        "\n",
        "        if not reviews:\n",
        "            return \"No reviews were generated. Please check the input data and try again.\"\n",
        "\n",
        "        # Safe access to statistics\n",
        "        total_reviews = len(reviews)\n",
        "        mean_score = stats.get('mean_overall_score', 0)\n",
        "        mean_confidence = stats.get('mean_confidence', 0)\n",
        "\n",
        "        summary = f\"\"\"\n",
        "ROBUST REFLECTION AGENT BATCH SUMMARY\n",
        "\n",
        "📊 REVIEW OVERVIEW:\n",
        "• Total hypotheses reviewed: {total_reviews}\n",
        "• Heuristic reviews: {stats.get('heuristic_review_count', 0)}\n",
        "• Quick reviews: {stats.get('quick_review_count', 0)}\n",
        "• Detailed reviews: {stats.get('detailed_review_count', 0)}\n",
        "\n",
        "📈 PERFORMANCE METRICS:\n",
        "• Mean overall score: {mean_score:.2f}/10\n",
        "• Score range: {stats.get('min_overall_score', 0):.1f} - {stats.get('max_overall_score', 0):.1f}\n",
        "• Reviewer confidence: {mean_confidence:.1f}/10\n",
        "\n",
        "🎯 SCORE DISTRIBUTION:\n",
        "• Excellent (≥8.5): {stats.get('excellent_count', 0)} hypotheses\n",
        "• High (7.5-8.4): {stats.get('high_scoring_count', 0)} hypotheses\n",
        "• Moderate (5.0-7.4): {stats.get('moderate_scoring_count', 0)} hypotheses\n",
        "• Low (<5.0): {stats.get('low_scoring_count', 0)} hypotheses\n",
        "\n",
        "⚠️ QUALITY INDICATORS: {', '.join(flags) if flags else 'None detected'}\n",
        "\n",
        "🏆 TOP HYPOTHESIS: {self._get_top_hypothesis_summary(reviews)}\n",
        "\n",
        "💡 NEXT STEPS:\n",
        "{self._generate_next_steps_recommendations(stats, flags)}\n",
        "\"\"\"\n",
        "\n",
        "        return summary.strip()\n",
        "\n",
        "    def _get_top_hypothesis_summary(self, reviews: List[HypothesisReview]) -> str:\n",
        "        \"\"\"Safely get top hypothesis summary\"\"\"\n",
        "        if not reviews:\n",
        "            return \"No reviews available\"\n",
        "\n",
        "        valid_reviews = [r for r in reviews if r.overall_score is not None]\n",
        "        if not valid_reviews:\n",
        "            return \"No valid scores available\"\n",
        "\n",
        "        top_review = max(valid_reviews, key=lambda r: r.overall_score)\n",
        "        return f\"{top_review.hypothesis_id} (Score: {top_review.overall_score:.2f}/10)\"\n",
        "\n",
        "    def _generate_next_steps_recommendations(self, stats: Dict[str, float], flags: List[str]) -> str:\n",
        "        \"\"\"Generate actionable next steps\"\"\"\n",
        "\n",
        "        recommendations = []\n",
        "\n",
        "        mean_score = stats.get('mean_overall_score', 0)\n",
        "        heuristic_count = stats.get('heuristic_review_count', 0)\n",
        "        total_count = stats.get('heuristic_review_count', 0) + stats.get('quick_review_count', 0) + stats.get('detailed_review_count', 0)\n",
        "\n",
        "        # Primary recommendations based on review quality\n",
        "        if \"ALL_HEURISTIC_REVIEWS\" in flags:\n",
        "            recommendations.append(\"• CRITICAL: All reviews were heuristic - improve LLM prompt parsing\")\n",
        "        elif \"MAJORITY_HEURISTIC_REVIEWS\" in flags:\n",
        "            recommendations.append(\"• IMPROVE: Majority heuristic reviews - enhance LLM response structure\")\n",
        "\n",
        "        # Score-based recommendations\n",
        "        if mean_score >= 7.5:\n",
        "            recommendations.append(\"• PROCEED: High quality hypotheses - ready for Ranking Agent\")\n",
        "        elif mean_score >= 6.0:\n",
        "            recommendations.append(\"• CONSIDER: Moderate quality - may benefit from Evolution Agent\")\n",
        "        else:\n",
        "            recommendations.append(\"• REVISE: Low quality scores - return to Generation Agent\")\n",
        "\n",
        "        # Confidence-based recommendations\n",
        "        if \"LOW_REVIEWER_CONFIDENCE\" in flags:\n",
        "            recommendations.append(\"• VALIDATE: Low confidence suggests need for human expert review\")\n",
        "\n",
        "        # Default recommendation\n",
        "        if not recommendations:\n",
        "            recommendations.append(\"• CONTINUE: Proceed with current workflow\")\n",
        "\n",
        "        return '\\\\n'.join(recommendations)\n",
        "\n",
        "# ===========================================================================\n",
        "# TESTING THE ROBUST AGENT\n",
        "# ===========================================================================\n",
        "\n",
        "def test_robust_reflection_agent():\n",
        "    \"\"\"Test the robust reflection agent with flexible parsing\"\"\"\n",
        "\n",
        "    print(\"🛡️ TESTING ROBUST REFLECTION AGENT\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    # Test with the same hypotheses but expect better parsing\n",
        "    test_hypotheses = [\n",
        "        {\n",
        "            \"id\": \"hypothesis_epigenetic\",\n",
        "            \"content\": \"\"\"\n",
        "            Novel Hypothesis: Epigenetic Reprogramming of Hepatic Stellate Cells\n",
        "\n",
        "            We hypothesize that targeted inhibition of DNMT3A in activated hepatic stellate cells will reverse pathological methylation patterns and restore their quiescent phenotype, leading to resolution of liver fibrosis. This innovative approach leverages the reversible nature of epigenetic modifications.\n",
        "\n",
        "            Scientific Rationale:\n",
        "            - DNMT3A upregulation is consistently demonstrated in fibrotic liver tissue\n",
        "            - Hypermethylation of anti-fibrotic genes like PPAR-γ correlates with stellate cell activation\n",
        "            - Epigenetic targets offer therapeutic advantages due to reversibility\n",
        "\n",
        "            Experimental Validation:\n",
        "            1. In vitro DNMT3A knockdown in activated human stellate cells\n",
        "            2. Genome-wide methylation profiling using bisulfite sequencing\n",
        "            3. In vivo delivery using stellate cell-specific nanoparticles\n",
        "            4. Assessment of fibrosis markers and liver function recovery\n",
        "\n",
        "            Expected Impact: Could lead to first-in-class epigenetic therapy for liver fibrosis with potential for clinical translation within 3-5 years.\n",
        "            \"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"hypothesis_ai_personalized\",\n",
        "            \"content\": \"\"\"\n",
        "            Breakthrough Hypothesis: AI-Driven Personalized Epigenetic Therapy\n",
        "\n",
        "            We propose developing a machine learning platform that analyzes individual patient epigenetic profiles to predict optimal combination therapies targeting multiple epigenetic enzymes (DNMT, HDAC, BRD4) for personalized liver fibrosis treatment.\n",
        "\n",
        "            Innovation: This represents the first AI-guided approach to epigenetic combination therapy, addressing the critical need for personalized medicine in liver fibrosis treatment.\n",
        "\n",
        "            Technical Approach:\n",
        "            - Multi-omics profiling of fibrosis patients (methylome, transcriptome, clinical data)\n",
        "            - Deep learning algorithms for treatment response prediction\n",
        "            - Validation in patient-derived organoid models\n",
        "            - Clinical pilot study with AI-recommended therapies\n",
        "\n",
        "            Transformative Potential: Could revolutionize liver fibrosis treatment by enabling precision medicine approaches with significantly improved patient outcomes.\n",
        "            \"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        robust_agent = RobustReflectionAgent(llm)\n",
        "    except NameError:\n",
        "        print(\"❌ Error: LLM not available. Run setup cell first.\")\n",
        "        return\n",
        "\n",
        "    print(f\"📋 Testing with {len(test_hypotheses)} enhanced hypotheses\")\n",
        "\n",
        "    # Test robust batch review\n",
        "    print(\"\\n🔍 TESTING ROBUST BATCH REVIEW\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    research_goal = \"Discover novel epigenetic targets for liver fibrosis treatment\"\n",
        "\n",
        "    batch_results = robust_agent.adaptive_batch_review(\n",
        "        test_hypotheses,\n",
        "        research_goal,\n",
        "        quick_threshold=5.0,\n",
        "        detailed_threshold=7.0\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Robust batch review completed\")\n",
        "    print(f\"   Total reviews: {len(batch_results.hypothesis_reviews)}\")\n",
        "    print(f\"   Quality flags: {len(batch_results.quality_flags)}\")\n",
        "\n",
        "    # Display robust summary\n",
        "    print(\"\\n📊 ROBUST BATCH SUMMARY\")\n",
        "    print(\"-\" * 40)\n",
        "    print(batch_results.batch_summary)\n",
        "\n",
        "    # Show sample review with robust parsing\n",
        "    print(\"\\n🔬 SAMPLE ROBUST REVIEW\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    if batch_results.hypothesis_reviews:\n",
        "        sample_review = batch_results.hypothesis_reviews[0]\n",
        "        print(f\"Hypothesis: {sample_review.hypothesis_id}\")\n",
        "        print(f\"Overall Score: {sample_review.overall_score:.2f}/10\")\n",
        "        print(f\"Review Type: {sample_review.reviewer_type}\")\n",
        "        print(f\"Confidence: {sample_review.confidence_level:.1f}/10\")\n",
        "\n",
        "        print(f\"\\nCriteria Breakdown:\")\n",
        "        print(f\"  Novelty: {sample_review.criteria.novelty_score:.1f}/10\")\n",
        "        print(f\"  Feasibility: {sample_review.criteria.feasibility_score:.1f}/10\")\n",
        "        print(f\"  Scientific Rigor: {sample_review.criteria.scientific_rigor_score:.1f}/10\")\n",
        "        print(f\"  Impact Potential: {sample_review.criteria.impact_potential_score:.1f}/10\")\n",
        "        print(f\"  Testability: {sample_review.criteria.testability_score:.1f}/10\")\n",
        "\n",
        "        print(f\"\\nRobust Strengths ({len(sample_review.strengths)}):\")\n",
        "        for i, strength in enumerate(sample_review.strengths[:3], 1):\n",
        "            print(f\"  {i}. {strength}\")\n",
        "\n",
        "        print(f\"\\nIdentified Weaknesses ({len(sample_review.weaknesses)}):\")\n",
        "        for i, weakness in enumerate(sample_review.weaknesses[:3], 1):\n",
        "            print(f\"  {i}. {weakness}\")\n",
        "\n",
        "        print(f\"\\nOverall Assessment:\")\n",
        "        print(f\"  {sample_review.overall_assessment[:200]}...\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 55)\n",
        "    print(\"🎉 ROBUST REFLECTION AGENT TESTING COMPLETE!\")\n",
        "    print(\"🛡️ Enhanced error handling and flexible parsing working\")\n",
        "\n",
        "    return batch_results\n",
        "\n",
        "# Run the robust test\n",
        "if __name__ == \"__main__\":\n",
        "    robust_results = test_robust_reflection_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBq0rM8-Rszy",
        "outputId": "99818fcc-faf2-4052-e196-ca9e50c9c3fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏆 TESTING RANKING AGENT\n",
            "==================================================\n",
            "📋 Setting up test data...\n",
            "✅ Created mock reflection state with 4 hypotheses\n",
            "\n",
            "1️⃣ TESTING ELO RATING SYSTEM\n",
            "------------------------------\n",
            "Expected score (1200 vs 1300): 0.360\n",
            "Rating update after A wins: 1220.5, 1279.5\n",
            "\n",
            "2️⃣ TESTING PAIRWISE COMPARISON\n",
            "------------------------------\n",
            "✅ Pairwise comparison completed\n",
            "   Comparison: hyp_epigenetic_1 vs hyp_ai_personalized\n",
            "   Winner: hyp_epigenetic_1\n",
            "   Confidence: 7.0/10\n",
            "   Reflection Influence: 5.0/10\n",
            "   Reasoning: ** Although Hypothesis B has the potential to be revolutionary, Hypothesis A provides a more grounde...\n",
            "\n",
            "3️⃣ TESTING TOURNAMENT ROUND\n",
            "------------------------------\n",
            "Initial ratings:\n",
            "   hyp_epigenetic_1: 1234\n",
            "   hyp_ai_personalized: 1224\n",
            "   hyp_combination_therapy: 1228\n",
            "   hyp_single_cell: 1224\n",
            "\n",
            "✅ Tournament round completed\n",
            "   Comparisons made: 3\n",
            "   Updated ratings:\n",
            "   hyp_epigenetic_1: 1234 (+0)\n",
            "   hyp_ai_personalized: 1193 (-31)\n",
            "   hyp_combination_therapy: 1227 (-1)\n",
            "   hyp_single_cell: 1256 (+32)\n",
            "\n",
            "4️⃣ TESTING FULL TOURNAMENT\n",
            "------------------------------\n",
            "✅ Full tournament completed\n",
            "   Tournament rounds: 2\n",
            "   Total comparisons: 8\n",
            "   Final rankings generated: 4\n",
            "\n",
            "5️⃣ TESTING RANKINGS ANALYSIS\n",
            "------------------------------\n",
            "📊 Final Rankings:\n",
            "   #1: hyp_single_cell\n",
            "      Elo: 1286 (+62)\n",
            "      Record: 4-0-0 (100.0%)\n",
            "   #2: hyp_epigenetic_1\n",
            "      Elo: 1233 (-1)\n",
            "      Record: 2-2-0 (50.0%)\n",
            "   #3: hyp_combination_therapy\n",
            "      Elo: 1212 (-16)\n",
            "      Record: 2-3-0 (40.0%)\n",
            "   #4: hyp_ai_personalized\n",
            "      Elo: 1178 (-46)\n",
            "      Record: 0-3-0 (0.0%)\n",
            "\n",
            "📈 Tournament Statistics:\n",
            "   total_hypotheses: 4\n",
            "   total_comparisons: 8\n",
            "   decisive_outcomes: 8\n",
            "   draws: 0\n",
            "   draw_rate: 0.00\n",
            "   mean_final_rating: 1227.50\n",
            "   rating_spread: 108.42\n",
            "   mean_rating_change: 0.00\n",
            "   max_rating_change: 62.48\n",
            "   min_rating_change: -45.94\n",
            "   rating_volatility: 45.45\n",
            "   average_confidence: 7.00\n",
            "   reflection_influence: 5.00\n",
            "   tournament_rounds: 2\n",
            "\n",
            "6️⃣ TESTING INTEGRATION\n",
            "------------------------------\n",
            "✅ Integration successful\n",
            "   State status: hypotheses_ranked\n",
            "   Has ranking results: True\n",
            "\n",
            "7️⃣ TOURNAMENT SUMMARY\n",
            "------------------------------\n",
            "RANKING AGENT TOURNAMENT SUMMARY\n",
            "\n",
            "🏆 TOURNAMENT RESULTS:\n",
            "• Total hypotheses: 4\n",
            "• Tournament rounds: 2\n",
            "• Total comparisons: 8\n",
            "• Decisive outcomes: 8 (100.0%)\n",
            "• Draws: 0 (0.0%)\n",
            "\n",
            "📊 RATING ANALYSIS:\n",
            "• Final rating range: 1205 - 1250\n",
            "• Rating spread: 108 points\n",
            "• Average rating change: +0.0 points\n",
            "• Biggest winner: +62.5 points\n",
            "• Biggest loser: -45.9 points\n",
            "\n",
            "🎯 DECISION QUALITY:\n",
            "• Average confidence: 7.0/10\n",
            "• Reflection influence: 5.0/10\n",
            "• Tournament convergence: Yes\n",
            "\n",
            "🥇 TOP 3 RANKINGS:\n",
            "\n",
            "• #1: hyp_single_cell\n",
            "  Elo: 1286 (+62)\n",
            "  Record: 4-0-0 (100.0%)\n",
            "• #2: hyp_epigenetic_1\n",
            "  Elo: 1233 (-1)\n",
            "  Record: 2-2-0 (50.0%)\n",
            "• #3: hyp_combination_therapy\n",
            "  Elo: 1212 (-16)\n",
            "  Record: 2-3-0 (40.0%)\n",
            "\n",
            "📈 CONVERGENCE ANALYSIS:\n",
            "• Rating stability: 19.3 points\n",
            "• Winner concentration: 50.0%\n",
            "• Tournament quality: Good\n",
            "\n",
            "💡 RECOMMENDATIONS:\n",
            "• STRONG CANDIDATE: hyp_single_cell shows consistent performance\\n• STABLE: Rankings have converged - results are reliable\\n• CLEAR DIFFERENCES: Low draw rate shows distinct quality levels\\n• NEXT: Clear winner identified - focus evolution efforts\n",
            "\n",
            "==================================================\n",
            "🎉 ALL RANKING AGENT TESTS COMPLETED SUCCESSFULLY!\n",
            "🏆 The Ranking Agent is ready for pairwise hypothesis comparison\n"
          ]
        }
      ],
      "source": [
        "#@title Ranking Agent - Elo Tournament & Pairwise Comparison System\n",
        "# ===========================================================================\n",
        "# RANKING AGENT IMPLEMENTATION\n",
        "# Elo-Based Tournament System for Hypothesis Ranking\n",
        "# ===========================================================================\n",
        "\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import statistics\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, asdict\n",
        "import datetime # FIX: Added the missing datetime import\n",
        "import logging\n",
        "from itertools import combinations\n",
        "import re # Added missing re import for parsing\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ===========================================================================\n",
        "# RANKING AGENT DATA STRUCTURES\n",
        "# ===========================================================================\n",
        "\n",
        "@dataclass\n",
        "class EloRating:\n",
        "    \"\"\"Elo rating for a hypothesis\"\"\"\n",
        "    hypothesis_id: str\n",
        "    current_rating: float\n",
        "    initial_rating: float\n",
        "    games_played: int\n",
        "    wins: int\n",
        "    losses: int\n",
        "    draws: int\n",
        "    rating_history: List[float]\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.rating_history is None:\n",
        "            self.rating_history = [self.initial_rating]\n",
        "\n",
        "    @property\n",
        "    def win_rate(self) -> float:\n",
        "        \"\"\"Calculate win rate percentage\"\"\"\n",
        "        total_games = self.wins + self.losses + self.draws\n",
        "        return (self.wins / total_games * 100) if total_games > 0 else 0.0\n",
        "\n",
        "    def update_rating(self, new_rating: float, result: str):\n",
        "        \"\"\"Update rating and statistics\"\"\"\n",
        "        self.current_rating = new_rating\n",
        "        self.rating_history.append(new_rating)\n",
        "        self.games_played += 1\n",
        "\n",
        "        if result == \"win\":\n",
        "            self.wins += 1\n",
        "        elif result == \"loss\":\n",
        "            self.losses += 1\n",
        "        else:\n",
        "            self.draws += 1\n",
        "\n",
        "@dataclass\n",
        "class PairwiseComparison:\n",
        "    \"\"\"Record of a single pairwise comparison\"\"\"\n",
        "    comparison_id: str\n",
        "    hypothesis_a_id: str\n",
        "    hypothesis_b_id: str\n",
        "    winner_id: Optional[str]  # None for draw\n",
        "    confidence: float  # 1-10 scale\n",
        "    reasoning: str\n",
        "    debate_transcript: str\n",
        "    reflection_influence: float  # How much reflection scores influenced decision\n",
        "    timestamp: str\n",
        "    comparison_round: int\n",
        "\n",
        "@dataclass\n",
        "class RankingState:\n",
        "    \"\"\"Complete state of the ranking tournament\"\"\"\n",
        "    elo_ratings: Dict[str, EloRating] = None\n",
        "    pairwise_comparisons: List[PairwiseComparison] = None\n",
        "    tournament_rounds: int = 0\n",
        "    final_rankings: List[Dict[str, Any]] = None\n",
        "    ranking_statistics: Dict[str, float] = None\n",
        "    convergence_metrics: Dict[str, float] = None\n",
        "    tournament_summary: str = \"\"\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.elo_ratings is None:\n",
        "            self.elo_ratings = {}\n",
        "        if self.pairwise_comparisons is None:\n",
        "            self.pairwise_comparisons = []\n",
        "        if self.final_rankings is None:\n",
        "            self.final_rankings = []\n",
        "        if self.ranking_statistics is None:\n",
        "            self.ranking_statistics = {}\n",
        "        if self.convergence_metrics is None:\n",
        "            self.convergence_metrics = {}\n",
        "\n",
        "# ===========================================================================\n",
        "# ELO RATING SYSTEM\n",
        "# ===========================================================================\n",
        "\n",
        "class EloSystem:\n",
        "    \"\"\"Elo rating system for hypothesis tournaments\"\"\"\n",
        "\n",
        "    def __init__(self, k_factor: float = 32, initial_rating: float = 1200):\n",
        "        self.k_factor = k_factor  # Rating volatility\n",
        "        self.initial_rating = initial_rating\n",
        "\n",
        "    def expected_score(self, rating_a: float, rating_b: float) -> float:\n",
        "        \"\"\"Calculate expected score for player A against player B\"\"\"\n",
        "        return 1 / (1 + 10**((rating_b - rating_a) / 400))\n",
        "\n",
        "    def update_ratings(self, rating_a: float, rating_b: float,\n",
        "                      actual_score_a: float) -> Tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Update Elo ratings based on match result\n",
        "        actual_score_a: 1 for A wins, 0 for A loses, 0.5 for draw\n",
        "        \"\"\"\n",
        "        expected_a = self.expected_score(rating_a, rating_b)\n",
        "        expected_b = 1 - expected_a\n",
        "        actual_score_b = 1 - actual_score_a\n",
        "\n",
        "        new_rating_a = rating_a + self.k_factor * (actual_score_a - expected_a)\n",
        "        new_rating_b = rating_b + self.k_factor * (actual_score_b - expected_b)\n",
        "\n",
        "        return new_rating_a, new_rating_b\n",
        "\n",
        "    def rating_difference_to_win_probability(self, rating_diff: float) -> float:\n",
        "        \"\"\"Convert rating difference to win probability\"\"\"\n",
        "        return 1 / (1 + 10**(-rating_diff / 400))\n",
        "\n",
        "# ===========================================================================\n",
        "# RANKING AGENT IMPLEMENTATION\n",
        "# ===========================================================================\n",
        "\n",
        "class RankingAgent:\n",
        "    \"\"\"\n",
        "    Ranking Agent that implements Elo-based tournament system with self-debate\n",
        "    for pairwise hypothesis comparison and ranking\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm, k_factor: float = 32, initial_rating: float = 1200):\n",
        "        self.llm = llm\n",
        "        self.elo_system = EloSystem(k_factor, initial_rating)\n",
        "        self.initial_rating = initial_rating\n",
        "\n",
        "    def initialize_ratings(self, hypothesis_reviews: List[HypothesisReview]) -> Dict[str, EloRating]:\n",
        "        \"\"\"Initialize Elo ratings for all hypotheses\"\"\"\n",
        "\n",
        "        ratings = {}\n",
        "\n",
        "        for review in hypothesis_reviews:\n",
        "            # Use reflection score to adjust initial rating slightly\n",
        "            reflection_bonus = (review.overall_score - 6.0) * 20  # ±40 points max\n",
        "            initial_rating = self.initial_rating + reflection_bonus\n",
        "\n",
        "            ratings[review.hypothesis_id] = EloRating(\n",
        "                hypothesis_id=review.hypothesis_id,\n",
        "                current_rating=initial_rating,\n",
        "                initial_rating=initial_rating,\n",
        "                games_played=0,\n",
        "                wins=0,\n",
        "                losses=0,\n",
        "                draws=0,\n",
        "                rating_history=[initial_rating]\n",
        "            )\n",
        "\n",
        "        logger.info(f\"Initialized {len(ratings)} hypothesis ratings\")\n",
        "        return ratings\n",
        "\n",
        "    def pairwise_debate(self, hypothesis_a: HypothesisReview, hypothesis_b: HypothesisReview,\n",
        "                       comparison_round: int) -> PairwiseComparison:\n",
        "        \"\"\"Conduct self-debate to compare two hypotheses\"\"\"\n",
        "\n",
        "        logger.info(f\"Starting pairwise debate: {hypothesis_a.hypothesis_id} vs {hypothesis_b.hypothesis_id}\")\n",
        "\n",
        "        debate_prompt = f\"\"\"You are a senior scientific review panel conducting a rigorous comparison between two research hypotheses. You must engage in self-debate to determine which hypothesis is superior.\n",
        "\n",
        "HYPOTHESIS A ({hypothesis_a.hypothesis_id}):\n",
        "Text: {hypothesis_a.hypothesis_text}\n",
        "Reflection Score: {hypothesis_a.overall_score:.2f}/10\n",
        "Key Strengths: {'; '.join(hypothesis_a.strengths[:3])}\n",
        "Key Weaknesses: {'; '.join(hypothesis_a.weaknesses[:3])}\n",
        "\n",
        "HYPOTHESIS B ({hypothesis_b.hypothesis_id}):\n",
        "Text: {hypothesis_b.hypothesis_text}\n",
        "Reflection Score: {hypothesis_b.overall_score:.2f}/10\n",
        "Key Strengths: {'; '.join(hypothesis_b.strengths[:3])}\n",
        "Key Weaknesses: {'; '.join(hypothesis_b.weaknesses[:3])}\n",
        "\n",
        "CONDUCT A STRUCTURED DEBATE:\n",
        "\n",
        "**ROUND 1 - Advocate for Hypothesis A:**\n",
        "Present the strongest case for why Hypothesis A is superior. Consider novelty, feasibility, impact potential, and scientific rigor.\n",
        "\n",
        "**ROUND 2 - Advocate for Hypothesis B:**\n",
        "Present the strongest case for why Hypothesis B is superior. Consider the same criteria and directly address A's advantages.\n",
        "\n",
        "**ROUND 3 - Critical Analysis:**\n",
        "Critically examine both hypotheses. What are the decisive factors? Which hypothesis has the strongest foundation and highest potential for breakthrough science?\n",
        "\n",
        "**FINAL JUDGMENT:**\n",
        "Winner: [A or B or DRAW]\n",
        "Confidence: [1-10 scale]\n",
        "Reflection Influence: [How much did the reflection scores influence your decision? 1-10 scale]\n",
        "\n",
        "**Reasoning:** [2-3 sentences explaining the decisive factors in your decision]\n",
        "\n",
        "Conduct this debate rigorously and choose the hypothesis that would most likely lead to significant scientific advancement.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke([(\"human\", debate_prompt)])\n",
        "            return self._parse_debate_result(response.content, hypothesis_a, hypothesis_b, comparison_round)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Debate failed for {hypothesis_a.hypothesis_id} vs {hypothesis_b.hypothesis_id}: {str(e)}\")\n",
        "            return self._create_fallback_comparison(hypothesis_a, hypothesis_b, comparison_round)\n",
        "\n",
        "    def _parse_debate_result(self, debate_text: str, hypothesis_a: HypothesisReview,\n",
        "                           hypothesis_b: HypothesisReview, comparison_round: int) -> PairwiseComparison:\n",
        "        \"\"\"Parse the debate result to determine winner\"\"\"\n",
        "\n",
        "        # Extract winner using multiple patterns\n",
        "        winner_patterns = [\n",
        "            r'winner.*?[:\\\\s]*([ABab]|draw|tie)',\n",
        "            r'final.*?judgment.*?[:\\\\s]*([ABab]|draw|tie)',\n",
        "            r'decision.*?[:\\\\s]*([ABab]|draw|tie)',\n",
        "            r'superior.*?hypothesis.*?([ABab])'\n",
        "        ]\n",
        "\n",
        "        winner_id = None\n",
        "        for pattern in winner_patterns:\n",
        "            match = re.search(pattern, debate_text, re.IGNORECASE)\n",
        "            if match:\n",
        "                result = match.group(1).lower()\n",
        "                if result in ['a', 'hypothesis a']:\n",
        "                    winner_id = hypothesis_a.hypothesis_id\n",
        "                elif result in ['b', 'hypothesis b']:\n",
        "                    winner_id = hypothesis_b.hypothesis_id\n",
        "                elif result in ['draw', 'tie']:\n",
        "                    winner_id = None\n",
        "                break\n",
        "\n",
        "        # If no clear winner found, use reflection scores as fallback\n",
        "        if winner_id is None:\n",
        "            if hypothesis_a.overall_score > hypothesis_b.overall_score + 0.5:\n",
        "                winner_id = hypothesis_a.hypothesis_id\n",
        "            elif hypothesis_b.overall_score > hypothesis_a.overall_score + 0.5:\n",
        "                winner_id = hypothesis_b.hypothesis_id\n",
        "            # Otherwise remains None (draw)\n",
        "\n",
        "        # Extract confidence\n",
        "        confidence_match = re.search(r'confidence.*?(\\\\d+(?:\\\\.\\\\d+)?)', debate_text, re.IGNORECASE)\n",
        "        confidence = float(confidence_match.group(1)) if confidence_match else 7.0\n",
        "\n",
        "        # Extract reflection influence\n",
        "        influence_match = re.search(r'reflection.*?influence.*?(\\\\d+(?:\\\\.\\\\d+)?)', debate_text, re.IGNORECASE)\n",
        "        reflection_influence = float(influence_match.group(1)) if influence_match else 5.0\n",
        "\n",
        "        # Extract reasoning\n",
        "        reasoning_patterns = [\n",
        "            r'reasoning.*?:(.*?)(?:\\\\n\\\\n|$)',\n",
        "            r'decisive.*?factors.*?:(.*?)(?:\\\\n\\\\n|$)',\n",
        "            r'explanation.*?:(.*?)(?:\\\\n\\\\n|$)'\n",
        "        ]\n",
        "\n",
        "        reasoning = \"Reasoning could not be extracted from debate.\"\n",
        "        for pattern in reasoning_patterns:\n",
        "            match = re.search(pattern, debate_text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                reasoning = match.group(1).strip()[:300]\n",
        "                break\n",
        "\n",
        "        comparison_id = f\"{hypothesis_a.hypothesis_id}_vs_{hypothesis_b.hypothesis_id}_r{comparison_round}\"\n",
        "\n",
        "        return PairwiseComparison(\n",
        "            comparison_id=comparison_id,\n",
        "            hypothesis_a_id=hypothesis_a.hypothesis_id,\n",
        "            hypothesis_b_id=hypothesis_b.hypothesis_id,\n",
        "            winner_id=winner_id,\n",
        "            confidence=confidence,\n",
        "            reasoning=reasoning,\n",
        "            debate_transcript=debate_text[:1000] + \"...\" if len(debate_text) > 1000 else debate_text,\n",
        "            reflection_influence=reflection_influence,\n",
        "            # FIX: Corrected datetime call\n",
        "            timestamp=datetime.datetime.now().isoformat(),\n",
        "            comparison_round=comparison_round\n",
        "        )\n",
        "\n",
        "    def _create_fallback_comparison(self, hypothesis_a: HypothesisReview,\n",
        "                                  hypothesis_b: HypothesisReview, comparison_round: int) -> PairwiseComparison:\n",
        "        \"\"\"Create fallback comparison based on reflection scores\"\"\"\n",
        "\n",
        "        score_diff = hypothesis_a.overall_score - hypothesis_b.overall_score\n",
        "\n",
        "        if abs(score_diff) < 0.3:\n",
        "            winner_id = None  # Draw for very close scores\n",
        "        elif score_diff > 0:\n",
        "            winner_id = hypothesis_a.hypothesis_id\n",
        "        else:\n",
        "            winner_id = hypothesis_b.hypothesis_id\n",
        "\n",
        "        comparison_id = f\"{hypothesis_a.hypothesis_id}_vs_{hypothesis_b.hypothesis_id}_r{comparison_round}_fallback\"\n",
        "\n",
        "        return PairwiseComparison(\n",
        "            comparison_id=comparison_id,\n",
        "            hypothesis_a_id=hypothesis_a.hypothesis_id,\n",
        "            hypothesis_b_id=hypothesis_b.hypothesis_id,\n",
        "            winner_id=winner_id,\n",
        "            confidence=5.0,  # Lower confidence for fallback\n",
        "            reasoning=f\"Fallback comparison based on reflection scores: {hypothesis_a.overall_score:.2f} vs {hypothesis_b.overall_score:.2f}\",\n",
        "            debate_transcript=\"Fallback comparison - no debate transcript available\",\n",
        "            reflection_influence=10.0,  # Fully based on reflection\n",
        "            # FIX: Corrected datetime call\n",
        "            timestamp=datetime.datetime.now().isoformat(),\n",
        "            comparison_round=comparison_round\n",
        "        )\n",
        "\n",
        "    def run_tournament_round(self, hypothesis_reviews: List[HypothesisReview],\n",
        "                           elo_ratings: Dict[str, EloRating], round_number: int,\n",
        "                           max_comparisons: int = None) -> List[PairwiseComparison]:\n",
        "        \"\"\"Run a single round of the tournament with pairwise comparisons\"\"\"\n",
        "\n",
        "        logger.info(f\"Starting tournament round {round_number}\")\n",
        "\n",
        "        # Create review lookup for easy access\n",
        "        review_lookup = {review.hypothesis_id: review for review in hypothesis_reviews}\n",
        "\n",
        "        # Generate all possible pairs\n",
        "        hypothesis_ids = list(elo_ratings.keys())\n",
        "        all_pairs = list(combinations(hypothesis_ids, 2))\n",
        "\n",
        "        # Limit comparisons if specified\n",
        "        if max_comparisons and len(all_pairs) > max_comparisons:\n",
        "            # Prioritize pairs with similar ratings for more informative comparisons\n",
        "            def rating_similarity(pair):\n",
        "                rating_a = elo_ratings[pair[0]].current_rating\n",
        "                rating_b = elo_ratings[pair[1]].current_rating\n",
        "                return abs(rating_a - rating_b)\n",
        "\n",
        "            all_pairs.sort(key=rating_similarity)\n",
        "            selected_pairs = all_pairs[:max_comparisons]\n",
        "        else:\n",
        "            selected_pairs = all_pairs\n",
        "\n",
        "        round_comparisons = []\n",
        "\n",
        "        for hypothesis_a_id, hypothesis_b_id in selected_pairs:\n",
        "            # Get reviews\n",
        "            review_a = review_lookup.get(hypothesis_a_id)\n",
        "            review_b = review_lookup.get(hypothesis_b_id)\n",
        "\n",
        "            if not review_a or not review_b:\n",
        "                logger.warning(f\"Could not find reviews for pair: {hypothesis_a_id}, {hypothesis_b_id}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Conduct debate\n",
        "            comparison = self.pairwise_debate(review_a, review_b, round_number)\n",
        "            round_comparisons.append(comparison)\n",
        "\n",
        "            # Update Elo ratings\n",
        "            rating_a = elo_ratings[hypothesis_a_id].current_rating\n",
        "            rating_b = elo_ratings[hypothesis_b_id].current_rating\n",
        "\n",
        "            # Convert result to scores\n",
        "            if comparison.winner_id == hypothesis_a_id:\n",
        "                score_a, score_b = 1.0, 0.0\n",
        "                result_a, result_b = \"win\", \"loss\"\n",
        "            elif comparison.winner_id == hypothesis_b_id:\n",
        "                score_a, score_b = 0.0, 1.0\n",
        "                result_a, result_b = \"loss\", \"win\"\n",
        "            else:\n",
        "                score_a, score_b = 0.5, 0.5\n",
        "                result_a, result_b = \"draw\", \"draw\"\n",
        "\n",
        "            # Update ratings\n",
        "            new_rating_a, new_rating_b = self.elo_system.update_ratings(rating_a, rating_b, score_a)\n",
        "\n",
        "            elo_ratings[hypothesis_a_id].update_rating(new_rating_a, result_a)\n",
        "            elo_ratings[hypothesis_b_id].update_rating(new_rating_b, result_b)\n",
        "\n",
        "        logger.info(f\"Completed round {round_number}: {len(round_comparisons)} comparisons\")\n",
        "        return round_comparisons\n",
        "\n",
        "    def run_full_tournament(self, reflection_state: ReflectionState,\n",
        "                          num_rounds: int = 3, max_comparisons_per_round: int = None) -> RankingState:\n",
        "        \"\"\"Run complete tournament with multiple rounds\"\"\"\n",
        "\n",
        "        logger.info(f\"Starting full tournament with {num_rounds} rounds\")\n",
        "\n",
        "        # Initialize ranking state\n",
        "        ranking_state = RankingState()\n",
        "\n",
        "        # Initialize Elo ratings\n",
        "        ranking_state.elo_ratings = self.initialize_ratings(reflection_state.hypothesis_reviews)\n",
        "\n",
        "        # Run tournament rounds\n",
        "        for round_num in range(1, num_rounds + 1):\n",
        "            round_comparisons = self.run_tournament_round(\n",
        "                reflection_state.hypothesis_reviews,\n",
        "                ranking_state.elo_ratings,\n",
        "                round_num,\n",
        "                max_comparisons_per_round\n",
        "            )\n",
        "            ranking_state.pairwise_comparisons.extend(round_comparisons)\n",
        "\n",
        "        ranking_state.tournament_rounds = num_rounds\n",
        "\n",
        "        # Generate final rankings and statistics\n",
        "        ranking_state.final_rankings = self._generate_final_rankings(ranking_state)\n",
        "        ranking_state.ranking_statistics = self._compute_ranking_statistics(ranking_state)\n",
        "        ranking_state.convergence_metrics = self._compute_convergence_metrics(ranking_state)\n",
        "        ranking_state.tournament_summary = self._generate_tournament_summary(ranking_state)\n",
        "\n",
        "        logger.info(f\"Tournament completed: {len(ranking_state.pairwise_comparisons)} total comparisons\")\n",
        "        return ranking_state\n",
        "\n",
        "    def _generate_final_rankings(self, ranking_state: RankingState) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Generate final hypothesis rankings\"\"\"\n",
        "\n",
        "        rankings = []\n",
        "\n",
        "        # Sort hypotheses by final Elo rating\n",
        "        sorted_hypotheses = sorted(\n",
        "            ranking_state.elo_ratings.items(),\n",
        "            key=lambda x: x[1].current_rating,\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        for rank, (hypothesis_id, elo_rating) in enumerate(sorted_hypotheses, 1):\n",
        "            ranking_entry = {\n",
        "                'rank': rank,\n",
        "                'hypothesis_id': hypothesis_id,\n",
        "                'final_elo_rating': elo_rating.current_rating,\n",
        "                'initial_elo_rating': elo_rating.initial_rating,\n",
        "                'rating_change': elo_rating.current_rating - elo_rating.initial_rating,\n",
        "                'games_played': elo_rating.games_played,\n",
        "                'wins': elo_rating.wins,\n",
        "                'losses': elo_rating.losses,\n",
        "                'draws': elo_rating.draws,\n",
        "                'win_rate': elo_rating.win_rate,\n",
        "                'rating_volatility': statistics.stdev(elo_rating.rating_history) if len(elo_rating.rating_history) > 1 else 0\n",
        "            }\n",
        "            rankings.append(ranking_entry)\n",
        "\n",
        "        return rankings\n",
        "\n",
        "    def _compute_ranking_statistics(self, ranking_state: RankingState) -> Dict[str, float]:\n",
        "        \"\"\"Compute comprehensive ranking statistics\"\"\"\n",
        "\n",
        "        if not ranking_state.elo_ratings:\n",
        "            return {}\n",
        "\n",
        "        if not ranking_state.pairwise_comparisons:\n",
        "             return { 'total_hypotheses': len(ranking_state.elo_ratings), 'total_comparisons': 0 }\n",
        "\n",
        "        current_ratings = [rating.current_rating for rating in ranking_state.elo_ratings.values()]\n",
        "        initial_ratings = [rating.initial_rating for rating in ranking_state.elo_ratings.values()]\n",
        "        rating_changes = [current - initial for current, initial in zip(current_ratings, initial_ratings)]\n",
        "\n",
        "        total_comparisons = len(ranking_state.pairwise_comparisons)\n",
        "        decisive_wins = len([c for c in ranking_state.pairwise_comparisons if c.winner_id is not None])\n",
        "        draws = total_comparisons - decisive_wins\n",
        "\n",
        "        avg_confidence = statistics.mean([c.confidence for c in ranking_state.pairwise_comparisons])\n",
        "        avg_reflection_influence = statistics.mean([c.reflection_influence for c in ranking_state.pairwise_comparisons])\n",
        "\n",
        "        return {\n",
        "            'total_hypotheses': len(ranking_state.elo_ratings),\n",
        "            'total_comparisons': total_comparisons,\n",
        "            'decisive_outcomes': decisive_wins,\n",
        "            'draws': draws,\n",
        "            'draw_rate': (draws / total_comparisons * 100) if total_comparisons > 0 else 0,\n",
        "            'mean_final_rating': statistics.mean(current_ratings),\n",
        "            'rating_spread': max(current_ratings) - min(current_ratings),\n",
        "            'mean_rating_change': statistics.mean(rating_changes),\n",
        "            'max_rating_change': max(rating_changes),\n",
        "            'min_rating_change': min(rating_changes),\n",
        "            'rating_volatility': statistics.stdev(current_ratings) if len(current_ratings) > 1 else 0,\n",
        "            'average_confidence': avg_confidence,\n",
        "            'reflection_influence': avg_reflection_influence,\n",
        "            'tournament_rounds': ranking_state.tournament_rounds\n",
        "        }\n",
        "\n",
        "    def _compute_convergence_metrics(self, ranking_state: RankingState) -> Dict[str, float]:\n",
        "        \"\"\"Compute metrics indicating ranking convergence\"\"\"\n",
        "\n",
        "        convergence = {}\n",
        "\n",
        "        # Rating stability (how much ratings changed in later rounds)\n",
        "        if ranking_state.tournament_rounds >= 2:\n",
        "            early_ratings = {}\n",
        "            late_ratings = {}\n",
        "\n",
        "            for hypothesis_id, elo_rating in ranking_state.elo_ratings.items():\n",
        "                history = elo_rating.rating_history\n",
        "                if len(history) >= 3:\n",
        "                    early_ratings[hypothesis_id] = history[len(history)//2]\n",
        "                    late_ratings[hypothesis_id] = history[-1]\n",
        "\n",
        "            if early_ratings and late_ratings:\n",
        "                rating_stability = statistics.mean([\n",
        "                    abs(late_ratings[h_id] - early_ratings[h_id])\n",
        "                    for h_id in early_ratings.keys()\n",
        "                ])\n",
        "                convergence['rating_stability'] = rating_stability\n",
        "                convergence['converged'] = rating_stability < 50  # Threshold for convergence\n",
        "\n",
        "        # Ranking consistency (how often the same hypothesis wins)\n",
        "        if ranking_state.pairwise_comparisons:\n",
        "            winner_counts = {}\n",
        "            for comparison in ranking_state.pairwise_comparisons:\n",
        "                if comparison.winner_id:\n",
        "                    winner_counts[comparison.winner_id] = winner_counts.get(comparison.winner_id, 0) + 1\n",
        "\n",
        "            if winner_counts:\n",
        "                total_wins = sum(winner_counts.values())\n",
        "                max_wins = max(winner_counts.values())\n",
        "                convergence['winner_concentration'] = max_wins / total_wins if total_wins > 0 else 0\n",
        "\n",
        "        return convergence\n",
        "\n",
        "    def _generate_tournament_summary(self, ranking_state: RankingState) -> str:\n",
        "        \"\"\"Generate comprehensive tournament summary\"\"\"\n",
        "\n",
        "        stats = ranking_state.ranking_statistics\n",
        "        convergence = ranking_state.convergence_metrics\n",
        "        rankings = ranking_state.final_rankings\n",
        "\n",
        "        if not stats or not rankings: return \"Summary not available.\"\n",
        "\n",
        "        # Get top 3 hypotheses\n",
        "        top_3 = rankings[:3] if len(rankings) >= 3 else rankings\n",
        "\n",
        "        summary = f\"\"\"\n",
        "RANKING AGENT TOURNAMENT SUMMARY\n",
        "\n",
        "🏆 TOURNAMENT RESULTS:\n",
        "• Total hypotheses: {stats.get('total_hypotheses', 0)}\n",
        "• Tournament rounds: {stats.get('tournament_rounds', 0)}\n",
        "• Total comparisons: {stats.get('total_comparisons', 0)}\n",
        "• Decisive outcomes: {stats.get('decisive_outcomes', 0)} ({100-stats.get('draw_rate', 0):.1f}%)\n",
        "• Draws: {stats.get('draws', 0)} ({stats.get('draw_rate', 0):.1f}%)\n",
        "\n",
        "📊 RATING ANALYSIS:\n",
        "• Final rating range: {stats.get('mean_final_rating', 0) - stats.get('rating_volatility', 0)/2:.0f} - {stats.get('mean_final_rating', 0) + stats.get('rating_volatility', 0)/2:.0f}\n",
        "• Rating spread: {stats.get('rating_spread', 0):.0f} points\n",
        "• Average rating change: {stats.get('mean_rating_change', 0):+.1f} points\n",
        "• Biggest winner: {stats.get('max_rating_change', 0):+.1f} points\n",
        "• Biggest loser: {stats.get('min_rating_change', 0):+.1f} points\n",
        "\n",
        "🎯 DECISION QUALITY:\n",
        "• Average confidence: {stats.get('average_confidence', 0):.1f}/10\n",
        "• Reflection influence: {stats.get('reflection_influence', 0):.1f}/10\n",
        "• Tournament convergence: {'Yes' if convergence.get('converged', False) else 'Partial'}\n",
        "\n",
        "🥇 TOP 3 RANKINGS:\n",
        "{self._format_top_rankings(top_3)}\n",
        "\n",
        "📈 CONVERGENCE ANALYSIS:\n",
        "• Rating stability: {convergence.get('rating_stability', 0):.1f} points\n",
        "• Winner concentration: {convergence.get('winner_concentration', 0)*100:.1f}%\n",
        "• Tournament quality: {'Excellent' if stats.get('average_confidence', 0) > 7.5 else 'Good' if stats.get('average_confidence', 0) > 6.0 else 'Fair'}\n",
        "\n",
        "💡 RECOMMENDATIONS:\n",
        "{self._generate_ranking_recommendations(stats, convergence, rankings)}\n",
        "\"\"\"\n",
        "\n",
        "        return summary.strip()\n",
        "\n",
        "    def _format_top_rankings(self, top_rankings: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Format top rankings for display\"\"\"\n",
        "\n",
        "        formatted = \"\"\n",
        "        for ranking in top_rankings:\n",
        "            formatted += f\"\"\"\n",
        "• #{ranking['rank']}: {ranking['hypothesis_id']}\n",
        "  Elo: {ranking['final_elo_rating']:.0f} ({ranking['rating_change']:+.0f})\n",
        "  Record: {ranking['wins']}-{ranking['losses']}-{ranking['draws']} ({ranking['win_rate']:.1f}%)\"\"\"\n",
        "\n",
        "        return formatted\n",
        "\n",
        "    def _generate_ranking_recommendations(self, stats: Dict[str, float],\n",
        "                                        convergence: Dict[str, float],\n",
        "                                        rankings: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Generate strategic recommendations based on tournament results\"\"\"\n",
        "\n",
        "        recommendations = []\n",
        "\n",
        "        if not stats or not rankings: return \"No recommendations available.\"\n",
        "\n",
        "        # Top hypothesis recommendations\n",
        "        if rankings:\n",
        "            top_hypothesis = rankings[0]\n",
        "            if top_hypothesis['final_elo_rating'] > stats.get('mean_final_rating', 1200) + 100:\n",
        "                recommendations.append(f\"• PRIORITY: {top_hypothesis['hypothesis_id']} is clearly superior - proceed to Evolution Agent\")\n",
        "            elif top_hypothesis['win_rate'] > 70:\n",
        "                recommendations.append(f\"• STRONG CANDIDATE: {top_hypothesis['hypothesis_id']} shows consistent performance\")\n",
        "\n",
        "        # Convergence recommendations\n",
        "        if convergence.get('converged', False):\n",
        "            recommendations.append(\"• STABLE: Rankings have converged - results are reliable\")\n",
        "        else:\n",
        "            recommendations.append(\"• CONTINUE: Consider additional tournament rounds for stability\")\n",
        "\n",
        "        # Quality recommendations\n",
        "        avg_confidence = stats.get('average_confidence', 0)\n",
        "        if avg_confidence < 6.0:\n",
        "            recommendations.append(\"• REVIEW: Low confidence suggests difficult comparisons - validate manually\")\n",
        "        elif avg_confidence > 8.0:\n",
        "            recommendations.append(\"• CONFIDENT: High confidence in ranking decisions\")\n",
        "\n",
        "        # Draw rate recommendations\n",
        "        draw_rate = stats.get('draw_rate', 0)\n",
        "        if draw_rate > 30:\n",
        "            recommendations.append(\"• SIMILAR QUALITY: High draw rate indicates comparable hypotheses\")\n",
        "        elif draw_rate < 10:\n",
        "            recommendations.append(\"• CLEAR DIFFERENCES: Low draw rate shows distinct quality levels\")\n",
        "\n",
        "        # Next steps\n",
        "        if len(rankings) >= 3:\n",
        "            top_3_ratings = [r['final_elo_rating'] for r in rankings[:3]]\n",
        "            if max(top_3_ratings) - min(top_3_ratings) < 50:\n",
        "                recommendations.append(\"• NEXT: Top hypotheses are close - consider parallel development\")\n",
        "            else:\n",
        "                recommendations.append(\"• NEXT: Clear winner identified - focus evolution efforts\")\n",
        "\n",
        "        return '\\\\n'.join(recommendations) if recommendations else \"• Proceed with evolved hypotheses as planned\"\n",
        "\n",
        "# ===========================================================================\n",
        "# INTEGRATION WITH REFLECTION AGENT\n",
        "# ===========================================================================\n",
        "\n",
        "def run_ranking_tournament(reflection_state: ReflectionState, tournament_rounds: int = 3,\n",
        "                          max_comparisons_per_round: int = None) -> RankingState:\n",
        "    \"\"\"Run complete ranking tournament using reflection results\"\"\"\n",
        "\n",
        "    if not reflection_state.hypothesis_reviews:\n",
        "        raise ValueError(\"No hypothesis reviews found in reflection state\")\n",
        "\n",
        "    # Ensure we have the LLM available\n",
        "    try:\n",
        "        ranking_agent = RankingAgent(llm)\n",
        "    except NameError:\n",
        "        raise RuntimeError(\"LLM not available. Run setup cell first.\")\n",
        "\n",
        "    # Run tournament\n",
        "    ranking_state = ranking_agent.run_full_tournament(\n",
        "        reflection_state,\n",
        "        tournament_rounds,\n",
        "        max_comparisons_per_round\n",
        "    )\n",
        "\n",
        "    return ranking_state\n",
        "\n",
        "def integrate_ranking_with_generation_state(generation_state: GenerationState,\n",
        "                                          ranking_state: RankingState) -> GenerationState:\n",
        "    \"\"\"Integrate ranking results with generation state\"\"\"\n",
        "\n",
        "    # Add ranking results to generation state\n",
        "    if hasattr(generation_state, 'ranking_results'):\n",
        "        generation_state.ranking_results = ranking_state\n",
        "    else:\n",
        "        setattr(generation_state, 'ranking_results', ranking_state)\n",
        "\n",
        "    # Update status\n",
        "    generation_state.status = \"hypotheses_ranked\"\n",
        "\n",
        "    return generation_state\n",
        "\n",
        "# ===========================================================================\n",
        "# TESTING CODE\n",
        "# ===========================================================================\n",
        "\n",
        "def test_ranking_agent():\n",
        "    \"\"\"Comprehensive test suite for Ranking Agent\"\"\"\n",
        "\n",
        "    print(\"🏆 TESTING RANKING AGENT\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # First, we need some reflection results to work with\n",
        "    print(\"📋 Setting up test data...\")\n",
        "\n",
        "    # Create mock reflection state with multiple hypotheses\n",
        "    mock_reviews = [\n",
        "        HypothesisReview(\n",
        "            hypothesis_id=\"hyp_epigenetic_1\",\n",
        "            hypothesis_text=\"Epigenetic reprogramming approach targeting DNMT3A for liver fibrosis treatment...\",\n",
        "            criteria=ReviewCriteria(8.0, 7.5, 7.0, 8.5, 7.5, \"Novel\", \"Feasible\", \"Well-grounded\", \"High impact\", \"Testable\"),\n",
        "            overall_score=7.7,\n",
        "            overall_assessment=\"Strong hypothesis with novel approach\",\n",
        "            strengths=[\"Novel epigenetic target\", \"Strong literature support\", \"Clear therapeutic path\"],\n",
        "            weaknesses=[\"Requires specialized expertise\", \"Complex delivery challenges\"],\n",
        "            recommendations=[\"Validate in primary cells\", \"Develop delivery system\"],\n",
        "            confidence_level=8.5,\n",
        "            review_timestamp=datetime.datetime.now().isoformat(),\n",
        "            reviewer_type=\"detailed\"\n",
        "        ),\n",
        "        HypothesisReview(\n",
        "            hypothesis_id=\"hyp_ai_personalized\",\n",
        "            hypothesis_text=\"AI-driven personalized medicine platform for optimized combination therapies...\",\n",
        "            criteria=ReviewCriteria(9.0, 6.0, 6.5, 9.0, 6.0, \"Very novel\", \"Challenging\", \"Moderate support\", \"Transformative\", \"Complex validation\"),\n",
        "            overall_score=7.2,\n",
        "            overall_assessment=\"Highly innovative but technically challenging\",\n",
        "            strengths=[\"Breakthrough AI application\", \"Personalized approach\", \"High impact potential\"],\n",
        "            weaknesses=[\"Technical complexity\", \"Data requirements\", \"Validation challenges\"],\n",
        "            recommendations=[\"Develop proof-of-concept\", \"Establish data partnerships\"],\n",
        "            confidence_level=7.0,\n",
        "            review_timestamp=datetime.datetime.now().isoformat(),\n",
        "            reviewer_type=\"detailed\"\n",
        "        ),\n",
        "        HypothesisReview(\n",
        "            hypothesis_id=\"hyp_combination_therapy\",\n",
        "            hypothesis_text=\"Multi-target combination therapy using BRD4 and HDAC inhibitors...\",\n",
        "            criteria=ReviewCriteria(6.5, 8.0, 7.5, 7.0, 8.0, \"Moderate novelty\", \"Very feasible\", \"Good support\", \"Good impact\", \"Easy to test\"),\n",
        "            overall_score=7.4,\n",
        "            overall_assessment=\"Solid, feasible approach with good scientific foundation\",\n",
        "            strengths=[\"Established targets\", \"Proven drug classes\", \"Clear experimental path\"],\n",
        "            weaknesses=[\"Limited novelty\", \"Incremental advance\", \"Competition exists\"],\n",
        "            recommendations=[\"Focus on unique combinations\", \"Optimize dosing\"],\n",
        "            confidence_level=8.0,\n",
        "            review_timestamp=datetime.datetime.now().isoformat(),\n",
        "            reviewer_type=\"detailed\"\n",
        "        ),\n",
        "        HypothesisReview(\n",
        "            hypothesis_id=\"hyp_single_cell\",\n",
        "            hypothesis_text=\"Single-cell epigenetic mapping to identify therapeutic vulnerabilities...\",\n",
        "            criteria=ReviewCriteria(7.5, 7.0, 8.0, 6.5, 7.0, \"Good novelty\", \"Achievable\", \"Strong evidence\", \"Moderate impact\", \"Good testability\"),\n",
        "            overall_score=7.2,\n",
        "            overall_assessment=\"Well-designed discovery approach with strong methodology\",\n",
        "            strengths=[\"Cutting-edge technology\", \"Discovery potential\", \"Strong methodology\"],\n",
        "            weaknesses=[\"Discovery rather than therapy\", \"Resource intensive\", \"Complex analysis\"],\n",
        "            recommendations=[\"Focus on key cell types\", \"Develop analysis pipeline\"],\n",
        "            confidence_level=7.5,\n",
        "            review_timestamp=datetime.datetime.now().isoformat(),\n",
        "            reviewer_type=\"detailed\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    mock_reflection_state = ReflectionState(\n",
        "        hypothesis_reviews=mock_reviews,\n",
        "        review_statistics={'mean_overall_score': 7.375},\n",
        "        quality_flags=[],\n",
        "        batch_summary=\"Mock reflection state for testing\"\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Created mock reflection state with {len(mock_reviews)} hypotheses\")\n",
        "\n",
        "    # Test 1: Elo System\n",
        "    print(\"\\n1️⃣ TESTING ELO RATING SYSTEM\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    elo_system = EloSystem(k_factor=32, initial_rating=1200)\n",
        "\n",
        "    # Test expected score calculation\n",
        "    expected = elo_system.expected_score(1200, 1300)\n",
        "    print(f\"Expected score (1200 vs 1300): {expected:.3f}\")\n",
        "\n",
        "    # Test rating update\n",
        "    new_a, new_b = elo_system.update_ratings(1200, 1300, 1.0)  # A wins\n",
        "    print(f\"Rating update after A wins: {new_a:.1f}, {new_b:.1f}\")\n",
        "\n",
        "    # Test 2: Pairwise Comparison\n",
        "    print(\"\\n2️⃣ TESTING PAIRWISE COMPARISON\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    try:\n",
        "        ranking_agent = RankingAgent(llm)\n",
        "\n",
        "        # Test debate between two hypotheses\n",
        "        comparison = ranking_agent.pairwise_debate(mock_reviews[0], mock_reviews[1], 1)\n",
        "\n",
        "        print(f\"✅ Pairwise comparison completed\")\n",
        "        print(f\"   Comparison: {comparison.hypothesis_a_id} vs {comparison.hypothesis_b_id}\")\n",
        "        print(f\"   Winner: {comparison.winner_id if comparison.winner_id else 'DRAW'}\")\n",
        "        print(f\"   Confidence: {comparison.confidence:.1f}/10\")\n",
        "        print(f\"   Reflection Influence: {comparison.reflection_influence:.1f}/10\")\n",
        "        print(f\"   Reasoning: {comparison.reasoning[:100]}...\")\n",
        "\n",
        "    except NameError:\n",
        "        print(\"❌ Error: LLM not available. Cannot test pairwise comparison.\")\n",
        "        return\n",
        "\n",
        "    # Test 3: Tournament Round\n",
        "    print(\"\\n3️⃣ TESTING TOURNAMENT ROUND\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Initialize ratings\n",
        "    elo_ratings = ranking_agent.initialize_ratings(mock_reviews)\n",
        "    print(f\"Initial ratings:\")\n",
        "    for hyp_id, rating in elo_ratings.items():\n",
        "        print(f\"   {hyp_id}: {rating.current_rating:.0f}\")\n",
        "\n",
        "    # Run one tournament round\n",
        "    round_comparisons = ranking_agent.run_tournament_round(\n",
        "        mock_reviews, elo_ratings, 1, max_comparisons=3\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✅ Tournament round completed\")\n",
        "    print(f\"   Comparisons made: {len(round_comparisons)}\")\n",
        "    print(f\"   Updated ratings:\")\n",
        "    for hyp_id, rating in elo_ratings.items():\n",
        "        change = rating.current_rating - rating.initial_rating\n",
        "        print(f\"   {hyp_id}: {rating.current_rating:.0f} ({change:+.0f})\")\n",
        "\n",
        "    # Test 4: Full Tournament\n",
        "    print(\"\\n4️⃣ TESTING FULL TOURNAMENT\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    ranking_state = ranking_agent.run_full_tournament(\n",
        "        mock_reflection_state,\n",
        "        num_rounds=2,\n",
        "        max_comparisons_per_round=4\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Full tournament completed\")\n",
        "    print(f\"   Tournament rounds: {ranking_state.tournament_rounds}\")\n",
        "    print(f\"   Total comparisons: {len(ranking_state.pairwise_comparisons)}\")\n",
        "    print(f\"   Final rankings generated: {len(ranking_state.final_rankings)}\")\n",
        "\n",
        "    # Test 5: Rankings Analysis\n",
        "    print(\"\\n5️⃣ TESTING RANKINGS ANALYSIS\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    print(\"📊 Final Rankings:\")\n",
        "    for ranking in ranking_state.final_rankings:\n",
        "        print(f\"   #{ranking['rank']}: {ranking['hypothesis_id']}\")\n",
        "        print(f\"      Elo: {ranking['final_elo_rating']:.0f} ({ranking['rating_change']:+.0f})\")\n",
        "        print(f\"      Record: {ranking['wins']}-{ranking['losses']}-{ranking['draws']} ({ranking['win_rate']:.1f}%)\")\n",
        "\n",
        "    print(f\"\\n📈 Tournament Statistics:\")\n",
        "    stats = ranking_state.ranking_statistics\n",
        "    for key, value in stats.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"   {key}: {value:.2f}\")\n",
        "        else:\n",
        "            print(f\"   {key}: {value}\")\n",
        "\n",
        "    # Test 6: Integration\n",
        "    print(\"\\n6️⃣ TESTING INTEGRATION\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Create mock generation state\n",
        "    mock_generation_state = GenerationState(\n",
        "        research_goal=\"Test ranking integration\",\n",
        "        status=\"hypotheses_reviewed\"\n",
        "    )\n",
        "\n",
        "    # Integrate ranking results\n",
        "    integrated_state = integrate_ranking_with_generation_state(mock_generation_state, ranking_state)\n",
        "\n",
        "    print(f\"✅ Integration successful\")\n",
        "    print(f\"   State status: {integrated_state.status}\")\n",
        "    print(f\"   Has ranking results: {hasattr(integrated_state, 'ranking_results')}\")\n",
        "\n",
        "    # Test 7: Tournament Summary\n",
        "    print(\"\\n7️⃣ TOURNAMENT SUMMARY\")\n",
        "    print(\"-\" * 30)\n",
        "    print(ranking_state.tournament_summary)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"🎉 ALL RANKING AGENT TESTS COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"🏆 The Ranking Agent is ready for pairwise hypothesis comparison\")\n",
        "\n",
        "    return ranking_state\n",
        "\n",
        "# ===========================================================================\n",
        "# RUN TESTS\n",
        "# ===========================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_results = test_ranking_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Dyp7myfTwz5",
        "outputId": "0d9e7425-eaaf-4a3e-a4c4-24041abf91e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧬 TESTING EVOLUTION AGENT\n",
            "==================================================\n",
            "📋 Setting up test data...\n",
            "✅ Created test data with 3 hypotheses\n",
            "\n",
            "1️⃣ TESTING INDIVIDUAL EVOLUTION STRATEGIES\n",
            "----------------------------------------\n",
            "Testing Simplification Strategy...\n",
            "✅ Simplification: simplification\n",
            "   Confidence: 7.0/10\n",
            "   Novelty change: -1.0\n",
            "   Impact change: +2.0\n",
            "\n",
            "Testing Analogical Reasoning Strategy...\n",
            "✅ Analogical Reasoning: analogical_reasoning\n",
            "   Benefits: 1\n",
            "   Risks: 1\n",
            "\n",
            "Testing Combination Strategy...\n",
            "✅ Combination: combination\n",
            "   Parent IDs: ['top_hypothesis_1', 'top_hypothesis_2']\n",
            "   Expected impact: +4.0\n",
            "\n",
            "2️⃣ TESTING FULL EVOLUTION PROCESS\n",
            "----------------------------------------\n",
            "✅ Evolution process completed\n",
            "   Evolved hypotheses: 6\n",
            "   Best evolved: evolved_combination_010933\n",
            "   Generation number: 1\n",
            "\n",
            "3️⃣ TESTING EVOLUTION ANALYSIS\n",
            "----------------------------------------\n",
            "📊 Evolution Statistics:\n",
            "   total_evolved_hypotheses: 6\n",
            "   mean_predicted_score: 8.25\n",
            "   max_predicted_score: 9.27\n",
            "   min_predicted_score: 7.70\n",
            "   mean_novelty_improvement: 2.17\n",
            "   mean_impact_improvement: 3.17\n",
            "   mean_feasibility_change: -0.33\n",
            "   mean_evolution_confidence: 7.00\n",
            "   strategies_applied: 5\n",
            "   generation_number: 1\n",
            "\n",
            "🎯 Strategy Effectiveness:\n",
            "   simplification_avg_novelty_gain: -1.00\n",
            "   simplification_avg_impact_gain: 1.00\n",
            "   simplification_avg_confidence: 7.00\n",
            "   simplification_avg_predicted_score: 8.39\n",
            "   simplification_applications: 2.00\n",
            "   radical_variation_avg_novelty_gain: 4.00\n",
            "   radical_variation_avg_impact_gain: 5.00\n",
            "   radical_variation_avg_confidence: 7.00\n",
            "   radical_variation_avg_predicted_score: 8.05\n",
            "   radical_variation_applications: 1.00\n",
            "   analogical_reasoning_avg_novelty_gain: 4.00\n",
            "   analogical_reasoning_avg_impact_gain: 3.00\n",
            "   analogical_reasoning_avg_confidence: 7.00\n",
            "   analogical_reasoning_avg_predicted_score: 7.70\n",
            "   analogical_reasoning_applications: 1.00\n",
            "   constraint_relaxation_avg_novelty_gain: 3.00\n",
            "   constraint_relaxation_avg_impact_gain: 4.00\n",
            "   constraint_relaxation_avg_confidence: 7.00\n",
            "   constraint_relaxation_avg_predicted_score: 7.70\n",
            "   constraint_relaxation_applications: 1.00\n",
            "   combination_avg_novelty_gain: 4.00\n",
            "   combination_avg_impact_gain: 5.00\n",
            "   combination_avg_confidence: 7.00\n",
            "   combination_avg_predicted_score: 9.27\n",
            "   combination_applications: 1.00\n",
            "\n",
            "4️⃣ TESTING EVOLVED HYPOTHESES\n",
            "----------------------------------------\n",
            "\n",
            "Evolved Hypothesis 1:\n",
            "   ID: evolved_simplification_010917\n",
            "   Strategy: simplification\n",
            "   Predicted Score: 8.60/10\n",
            "   Parents: top_hypothesis_1\n",
            "   Key Advantages: No specific benefit identified\n",
            "   Content Preview: **\n",
            "\n",
            "Selective inhibition of DNMT3A in activated hepatic stellate cells promotes liver fibrosis reversal.\n",
            "\n",
            "**...\n",
            "\n",
            "Evolved Hypothesis 2:\n",
            "   ID: evolved_radical_variation_010917\n",
            "   Strategy: radical_variation\n",
            "   Predicted Score: 7.90/10\n",
            "   Parents: top_hypothesis_1\n",
            "   Key Advantages: No specific benefit identified\n",
            "   Content Preview: **Harnessing endogenous liver regeneration via targeted induction of cellular transdifferentiation and extracellular matrix (ECM) remodeling using foc...\n",
            "\n",
            "Evolved Hypothesis 3:\n",
            "   ID: evolved_analogical_reasoning_010927\n",
            "   Strategy: analogical_reasoning\n",
            "   Predicted Score: 7.50/10\n",
            "   Parents: top_hypothesis_2\n",
            "   Key Advantages: No specific benefit identified\n",
            "   Content Preview: **\n",
            "\n",
            "AI-powered personalized medicine platform that analyzes patient epigenetic profiles *and gut microbiome composition* to optimize *spatiotemporally...\n",
            "\n",
            "5️⃣ TESTING INTEGRATION\n",
            "----------------------------------------\n",
            "✅ Integration successful\n",
            "   State status: hypotheses_evolved\n",
            "   Has evolution results: True\n",
            "\n",
            "6️⃣ EVOLUTION SUMMARY\n",
            "----------------------------------------\n",
            "EVOLUTION AGENT GENERATION SUMMARY\n",
            "\n",
            "🧬 EVOLUTION OVERVIEW:\n",
            "• Generation Number: 1\n",
            "• Evolved Hypotheses: 6\n",
            "• Strategies Applied: 5\n",
            "• Evolution Confidence: 7.0/10\n",
            "\n",
            "📈 IMPROVEMENT METRICS:\n",
            "• Mean Predicted Score: 8.25/10\n",
            "• Score Range: 7.7 - 9.3\n",
            "• Average Novelty Gain: +2.2 points\n",
            "• Average Impact Gain: +3.2 points\n",
            "• Feasibility Change: -0.3 points\n",
            "\n",
            "🎯 STRATEGY EFFECTIVENESS:\n",
            "\n",
            "• Analogical_Reasoning: Score 7.7 (+4.0 novelty, +3.0 impact) [1 applications]\n",
            "• Combination: Score 9.3 (+4.0 novelty, +5.0 impact) [1 applications]\n",
            "• Constraint_Relaxation: Score 7.7 (+3.0 novelty, +4.0 impact) [1 applications]\n",
            "• Radical_Variation: Score 8.1 (+4.0 novelty, +5.0 impact) [1 applications]\n",
            "• Simplification: Score 8.4 (+-1.0 novelty, +1.0 impact) [2 applications]\n",
            "\n",
            "🏆 BEST EVOLVED HYPOTHESIS:\n",
            "\n",
            "• ID: evolved_combination_010933\n",
            "• Strategy Used: Combination\n",
            "• Predicted Score: 9.00/10\n",
            "• Key Advantages: No specific benefit identified\n",
            "• Parent(s): top_hypothesis_1, top_hypothesis_2\n",
            "\n",
            "🚀 IMPLEMENTATION RECOMMENDATIONS:\n",
            "• PRIORITY: Focus on evolved_combination_010933 for detailed development\\n• EXCELLENT: High-quality evolved hypotheses - proceed to validation\\n• BREAKTHROUGH: Significant novelty increases - potential for high impact\\n• NEXT: Run reflection and ranking on evolved hypotheses for comparison\n",
            "\n",
            "==================================================\n",
            "🎉 ALL EVOLUTION AGENT TESTS COMPLETED SUCCESSFULLY!\n",
            "🧬 The Evolution Agent is ready to improve top hypotheses\n"
          ]
        }
      ],
      "source": [
        "#@title Evolution Agent - Multi-Strategy Hypothesis Evolution\n",
        "# ===========================================================================\n",
        "# EVOLUTION AGENT IMPLEMENTATION\n",
        "# Multi-Strategy Hypothesis Evolution and Improvement System\n",
        "# ===========================================================================\n",
        "\n",
        "import json\n",
        "import random\n",
        "import statistics\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, asdict\n",
        "import datetime # FIX: Added the missing datetime import\n",
        "import logging\n",
        "from enum import Enum\n",
        "import re # Added missing re import for parsing\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ===========================================================================\n",
        "# EVOLUTION AGENT DATA STRUCTURES\n",
        "# ===========================================================================\n",
        "\n",
        "class EvolutionStrategy(Enum):\n",
        "    \"\"\"Types of evolution strategies available\"\"\"\n",
        "    SIMPLIFICATION = \"simplification\"\n",
        "    COMBINATION = \"combination\"\n",
        "    ANALOGICAL_REASONING = \"analogical_reasoning\"\n",
        "    RADICAL_VARIATION = \"radical_variation\"\n",
        "    CONSTRAINT_RELAXATION = \"constraint_relaxation\"\n",
        "    DOMAIN_TRANSFER = \"domain_transfer\"\n",
        "\n",
        "@dataclass\n",
        "class EvolutionStep:\n",
        "    \"\"\"Record of a single evolution step\"\"\"\n",
        "    step_id: str\n",
        "    parent_hypothesis_ids: List[str]\n",
        "    strategy_used: EvolutionStrategy\n",
        "    evolution_prompt: str\n",
        "    evolved_content: str\n",
        "    improvement_rationale: str\n",
        "    expected_benefits: List[str]\n",
        "    potential_risks: List[str]\n",
        "    novelty_increase: float  # -5 to +5 scale\n",
        "    feasibility_change: float  # -5 to +5 scale\n",
        "    impact_increase: float  # -5 to +5 scale\n",
        "    confidence: float  # 1-10 scale\n",
        "    timestamp: str\n",
        "\n",
        "@dataclass\n",
        "class EvolvedHypothesis:\n",
        "    \"\"\"Complete evolved hypothesis with lineage\"\"\"\n",
        "    hypothesis_id: str\n",
        "    evolved_content: str\n",
        "    parent_hypothesis_ids: List[str]\n",
        "    evolution_lineage: List[EvolutionStep]\n",
        "    generation_number: int\n",
        "    predicted_scores: Dict[str, float]  # Predicted reflection scores\n",
        "    improvement_summary: str\n",
        "    competitive_advantages: List[str]\n",
        "    implementation_roadmap: List[str]\n",
        "\n",
        "@dataclass\n",
        "class EvolutionState:\n",
        "    \"\"\"Complete state of the evolution process\"\"\"\n",
        "    evolved_hypotheses: List[EvolvedHypothesis] = None\n",
        "    evolution_statistics: Dict[str, float] = None\n",
        "    strategy_effectiveness: Dict[str, float] = None\n",
        "    evolution_tree: Dict[str, Any] = None\n",
        "    generation_summary: str = \"\"\n",
        "    best_evolved_hypothesis: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.evolved_hypotheses is None:\n",
        "            self.evolved_hypotheses = []\n",
        "        if self.evolution_statistics is None:\n",
        "            self.evolution_statistics = {}\n",
        "        if self.strategy_effectiveness is None:\n",
        "            self.strategy_effectiveness = {}\n",
        "        if self.evolution_tree is None:\n",
        "            self.evolution_tree = {}\n",
        "\n",
        "# ===========================================================================\n",
        "# EVOLUTION AGENT IMPLEMENTATION\n",
        "# ===========================================================================\n",
        "\n",
        "class EvolutionAgent:\n",
        "    \"\"\"\n",
        "    Evolution Agent that applies multiple strategies to improve and evolve hypotheses\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.generation_counter = 0\n",
        "        self.evolution_history = []\n",
        "\n",
        "    def simplify_hypothesis(self, hypothesis_review: HypothesisReview,\n",
        "                          ranking_info: Dict[str, Any]) -> EvolutionStep:\n",
        "        \"\"\"Strategy 1: Simplify and focus the hypothesis\"\"\"\n",
        "\n",
        "        logger.info(f\"Applying simplification strategy to {hypothesis_review.hypothesis_id}\")\n",
        "\n",
        "        simplification_prompt = f\"\"\"You are a strategic scientific advisor specializing in hypothesis refinement. Your task is to SIMPLIFY and FOCUS this research hypothesis to make it more achievable and impactful.\n",
        "\n",
        "ORIGINAL HYPOTHESIS ({hypothesis_review.hypothesis_id}):\n",
        "{hypothesis_review.hypothesis_text}\n",
        "\n",
        "CURRENT PERFORMANCE:\n",
        "- Reflection Score: {hypothesis_review.overall_score:.2f}/10\n",
        "- Tournament Ranking: #{ranking_info.get('rank', 'Unknown')}\n",
        "- Key Strengths: {'; '.join(hypothesis_review.strengths[:3])}\n",
        "- Key Weaknesses: {'; '.join(hypothesis_review.weaknesses[:3])}\n",
        "\n",
        "SIMPLIFICATION STRATEGY:\n",
        "Your goal is to create a more focused, achievable version that:\n",
        "1. Reduces complexity while maintaining core innovation\n",
        "2. Focuses on the most promising aspect\n",
        "3. Makes the approach more feasible and testable\n",
        "4. Maintains or increases potential impact\n",
        "\n",
        "EVOLVED HYPOTHESIS:\n",
        "[Provide the simplified, focused version of the hypothesis]\n",
        "\n",
        "IMPROVEMENT RATIONALE:\n",
        "[Explain specifically how this simplification improves the original - 2-3 sentences]\n",
        "\n",
        "EXPECTED BENEFITS:\n",
        "- [Benefit 1: How feasibility improves]\n",
        "- [Benefit 2: How testability improves]\n",
        "- [Benefit 3: How focus improves impact]\n",
        "\n",
        "POTENTIAL RISKS:\n",
        "- [Risk 1: What might be lost in simplification]\n",
        "- [Risk 2: Any new limitations introduced]\n",
        "\n",
        "PREDICTED SCORE CHANGES (-5 to +5):\n",
        "- Novelty Change: [score change and brief reason]\n",
        "- Feasibility Change: [score change and brief reason]\n",
        "- Impact Change: [score change and brief reason]\n",
        "\n",
        "CONFIDENCE: [1-10 in this improvement]\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke([(\"human\", simplification_prompt)])\n",
        "            return self._parse_evolution_response(\n",
        "                response.content, [hypothesis_review.hypothesis_id],\n",
        "                EvolutionStrategy.SIMPLIFICATION\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Simplification failed for {hypothesis_review.hypothesis_id}: {str(e)}\")\n",
        "            return self._create_fallback_evolution_step(\n",
        "                [hypothesis_review.hypothesis_id], EvolutionStrategy.SIMPLIFICATION\n",
        "            )\n",
        "\n",
        "    def combine_hypotheses(self, hypothesis_reviews: List[HypothesisReview],\n",
        "                          ranking_infos: List[Dict[str, Any]]) -> EvolutionStep:\n",
        "        \"\"\"Strategy 2: Combine multiple top hypotheses\"\"\"\n",
        "\n",
        "        hypothesis_ids = [hr.hypothesis_id for hr in hypothesis_reviews]\n",
        "        logger.info(f\"Applying combination strategy to {', '.join(hypothesis_ids)}\")\n",
        "\n",
        "        # Prepare hypothesis summaries\n",
        "        hypothesis_summaries = []\n",
        "        for i, (review, ranking) in enumerate(zip(hypothesis_reviews, ranking_infos)):\n",
        "            summary = f\"\"\"\n",
        "HYPOTHESIS {i+1} ({review.hypothesis_id}):\n",
        "Content: {review.hypothesis_text[:300]}...\n",
        "Strengths: {'; '.join(review.strengths[:2])}\n",
        "Ranking: #{ranking.get('rank', 'Unknown')} (Score: {review.overall_score:.1f}/10)\"\"\"\n",
        "            hypothesis_summaries.append(summary)\n",
        "\n",
        "        combination_prompt = f\"\"\"You are an innovative scientific researcher expert at synthesizing breakthrough ideas. Your task is to COMBINE the best elements from these top-performing hypotheses into a superior unified approach.\n",
        "\n",
        "TOP HYPOTHESES TO COMBINE:\n",
        "{''.join(hypothesis_summaries)}\n",
        "\n",
        "COMBINATION STRATEGY:\n",
        "Create a novel hybrid hypothesis that:\n",
        "1. Integrates the strongest elements from each hypothesis\n",
        "2. Creates synergistic effects between different approaches\n",
        "3. Eliminates redundancies and weaknesses\n",
        "4. Results in greater impact than individual components\n",
        "5. Maintains feasibility while increasing innovation\n",
        "\n",
        "COMBINED EVOLVED HYPOTHESIS:\n",
        "[Present the unified hypothesis that combines the best elements]\n",
        "\n",
        "INTEGRATION RATIONALE:\n",
        "[Explain how you combined the hypotheses and why this synthesis is superior - 3-4 sentences]\n",
        "\n",
        "SYNERGISTIC BENEFITS:\n",
        "- [Benefit 1: How combination creates new capabilities]\n",
        "- [Benefit 2: How different approaches reinforce each other]\n",
        "- [Benefit 3: How combined approach increases impact]\n",
        "- [Benefit 4: How integration reduces individual limitations]\n",
        "\n",
        "POTENTIAL INTEGRATION RISKS:\n",
        "- [Risk 1: Complexity from combination]\n",
        "- [Risk 2: Potential conflicts between approaches]\n",
        "\n",
        "PREDICTED SCORE CHANGES (-5 to +5):\n",
        "- Novelty Change: [score change - combinations often increase novelty]\n",
        "- Feasibility Change: [score change - may decrease due to complexity]\n",
        "- Impact Change: [score change - typically increases significantly]\n",
        "\n",
        "CONFIDENCE: [1-10 in this combination approach]\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke([(\"human\", combination_prompt)])\n",
        "            return self._parse_evolution_response(\n",
        "                response.content, hypothesis_ids, EvolutionStrategy.COMBINATION\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Combination failed for {hypothesis_ids}: {str(e)}\")\n",
        "            return self._create_fallback_evolution_step(hypothesis_ids, EvolutionStrategy.COMBINATION)\n",
        "\n",
        "    def analogical_reasoning(self, hypothesis_review: HypothesisReview,\n",
        "                           ranking_info: Dict[str, Any]) -> EvolutionStep:\n",
        "        \"\"\"Strategy 3: Apply analogical reasoning from other successful domains\"\"\"\n",
        "\n",
        "        logger.info(f\"Applying analogical reasoning to {hypothesis_review.hypothesis_id}\")\n",
        "\n",
        "        analogical_prompt = f\"\"\"You are a cross-disciplinary innovation expert skilled at applying successful patterns from one domain to create breakthroughs in another. Your task is to evolve this hypothesis using ANALOGICAL REASONING from other successful scientific domains.\n",
        "\n",
        "ORIGINAL HYPOTHESIS ({hypothesis_review.hypothesis_id}):\n",
        "{hypothesis_review.hypothesis_text}\n",
        "\n",
        "CURRENT PERFORMANCE:\n",
        "- Reflection Score: {hypothesis_review.overall_score:.2f}/10\n",
        "- Tournament Ranking: #{ranking_info.get('rank', 'Unknown')}\n",
        "- Strengths: {'; '.join(hypothesis_review.strengths[:3])}\n",
        "- Areas for improvement: {'; '.join(hypothesis_review.weaknesses[:2])}\n",
        "\n",
        "ANALOGICAL REASONING STRATEGY:\n",
        "1. Identify successful patterns from other domains (immunology, engineering, computer science, ecology, etc.)\n",
        "2. Find analogous challenges that have been solved elegantly elsewhere\n",
        "3. Adapt those successful approaches to this biomedical context\n",
        "4. Create novel solutions inspired by cross-domain insights\n",
        "\n",
        "EVOLVED HYPOTHESIS WITH ANALOGICAL INSPIRATION:\n",
        "[Present the evolved hypothesis incorporating cross-domain insights]\n",
        "\n",
        "ANALOGICAL INSPIRATION:\n",
        "[Describe the specific analogy/domain that inspired this evolution - 2-3 sentences]\n",
        "\n",
        "CROSS-DOMAIN BENEFITS:\n",
        "- [Benefit 1: What successful pattern was adapted]\n",
        "- [Benefit 2: How this analogy solves current limitations]\n",
        "- [Benefit 3: What new capabilities this enables]\n",
        "- [Benefit 4: How this increases novelty and innovation]\n",
        "\n",
        "ADAPTATION RISKS:\n",
        "- [Risk 1: Challenges in domain transfer]\n",
        "- [Risk 2: Potential misalignment with biomedical context]\n",
        "\n",
        "PREDICTED SCORE CHANGES (-5 to +5):\n",
        "- Novelty Change: [score change - analogical reasoning typically increases novelty significantly]\n",
        "- Feasibility Change: [score change - depends on analogy complexity]\n",
        "- Impact Change: [score change - cross-domain insights often increase impact]\n",
        "\n",
        "CONFIDENCE: [1-10 in this analogical approach]\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke([(\"human\", analogical_prompt)])\n",
        "            return self._parse_evolution_response(\n",
        "                response.content, [hypothesis_review.hypothesis_id],\n",
        "                EvolutionStrategy.ANALOGICAL_REASONING\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analogical reasoning failed for {hypothesis_review.hypothesis_id}: {str(e)}\")\n",
        "            return self._create_fallback_evolution_step(\n",
        "                [hypothesis_review.hypothesis_id], EvolutionStrategy.ANALOGICAL_REASONING\n",
        "            )\n",
        "\n",
        "    def radical_variation(self, hypothesis_review: HypothesisReview,\n",
        "                         ranking_info: Dict[str, Any]) -> EvolutionStep:\n",
        "        \"\"\"Strategy 4: Create radical variations exploring very different approaches\"\"\"\n",
        "\n",
        "        logger.info(f\"Applying radical variation to {hypothesis_review.hypothesis_id}\")\n",
        "\n",
        "        radical_prompt = f\"\"\"You are a visionary scientific researcher known for paradigm-shifting breakthroughs. Your task is to create a RADICAL VARIATION of this hypothesis that explores a completely different approach while addressing the same fundamental problem.\n",
        "\n",
        "ORIGINAL HYPOTHESIS ({hypothesis_review.hypothesis_id}):\n",
        "{hypothesis_review.hypothesis_text}\n",
        "\n",
        "CURRENT APPROACH ANALYSIS:\n",
        "- Current Score: {hypothesis_review.overall_score:.2f}/10\n",
        "- Current Strengths: {'; '.join(hypothesis_review.strengths[:2])}\n",
        "- Current Limitations: {'; '.join(hypothesis_review.weaknesses[:2])}\n",
        "\n",
        "RADICAL VARIATION STRATEGY:\n",
        "Create a fundamentally different approach that:\n",
        "1. Addresses the same core problem from a completely new angle\n",
        "2. Challenges conventional assumptions in the field\n",
        "3. Explores cutting-edge technologies or methodologies\n",
        "4. Has potential for paradigm-shifting impact\n",
        "5. May be higher risk but also higher reward\n",
        "\n",
        "Think beyond incremental improvements - imagine a breakthrough that would make current approaches obsolete.\n",
        "\n",
        "RADICAL EVOLVED HYPOTHESIS:\n",
        "[Present the radically different approach to the same problem]\n",
        "\n",
        "PARADIGM SHIFT RATIONALE:\n",
        "[Explain what fundamental assumptions you're challenging and why this radical approach could be superior - 3-4 sentences]\n",
        "\n",
        "BREAKTHROUGH POTENTIAL:\n",
        "- [Revolutionary aspect 1: What paradigm this challenges]\n",
        "- [Revolutionary aspect 2: What new possibilities this opens]\n",
        "- [Revolutionary aspect 3: How this could transform the field]\n",
        "- [Revolutionary aspect 4: What conventional limitations this bypasses]\n",
        "\n",
        "HIGH-RISK FACTORS:\n",
        "- [Risk 1: Technical challenges of radical approach]\n",
        "- [Risk 2: Acceptance and validation difficulties]\n",
        "- [Risk 3: Resource and timeline implications]\n",
        "\n",
        "PREDICTED SCORE CHANGES (-5 to +5):\n",
        "- Novelty Change: [score change - should be strongly positive for radical innovation]\n",
        "- Feasibility Change: [score change - may decrease due to cutting-edge nature]\n",
        "- Impact Change: [score change - potential for transformative impact]\n",
        "\n",
        "CONFIDENCE: [1-10 in this radical approach - may be lower due to higher uncertainty]\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke([(\"human\", radical_prompt)])\n",
        "            return self._parse_evolution_response(\n",
        "                response.content, [hypothesis_review.hypothesis_id],\n",
        "                EvolutionStrategy.RADICAL_VARIATION\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Radical variation failed for {hypothesis_review.hypothesis_id}: {str(e)}\")\n",
        "            return self._create_fallback_evolution_step(\n",
        "                [hypothesis_review.hypothesis_id], EvolutionStrategy.RADICAL_VARIATION\n",
        "            )\n",
        "\n",
        "    def constraint_relaxation(self, hypothesis_review: HypothesisReview,\n",
        "                            ranking_info: Dict[str, Any],\n",
        "                            original_constraints: List[str]) -> EvolutionStep:\n",
        "        \"\"\"Strategy 5: Relax constraints to explore new possibilities\"\"\"\n",
        "\n",
        "        logger.info(f\"Applying constraint relaxation to {hypothesis_review.hypothesis_id}\")\n",
        "\n",
        "        constraint_prompt = f\"\"\"You are a strategic research planner exploring expanded possibilities. Your task is to evolve this hypothesis by RELAXING KEY CONSTRAINTS to unlock new potential approaches.\n",
        "\n",
        "ORIGINAL HYPOTHESIS ({hypothesis_review.hypothesis_id}):\n",
        "{hypothesis_review.hypothesis_text}\n",
        "\n",
        "CURRENT CONSTRAINTS:\n",
        "{chr(10).join([f\"- {constraint}\" for constraint in original_constraints])}\n",
        "\n",
        "CURRENT PERFORMANCE:\n",
        "- Score: {hypothesis_review.overall_score:.2f}/10\n",
        "- Main limitations: {'; '.join(hypothesis_review.weaknesses[:3])}\n",
        "\n",
        "CONSTRAINT RELAXATION STRATEGY:\n",
        "1. Identify which constraints are most limiting the hypothesis potential\n",
        "2. Explore what becomes possible if we relax 1-2 key constraints\n",
        "3. Develop evolved approach that leverages this expanded possibility space\n",
        "4. Maintain core innovation while expanding scope\n",
        "\n",
        "EVOLVED HYPOTHESIS WITH RELAXED CONSTRAINTS:\n",
        "[Present the evolved hypothesis with expanded possibilities]\n",
        "\n",
        "CONSTRAINTS RELAXED:\n",
        "[Specify which 1-2 constraints you relaxed and why]\n",
        "\n",
        "EXPANDED POSSIBILITIES:\n",
        "- [Possibility 1: What new approaches become available]\n",
        "- [Possibility 2: What additional impact becomes possible]\n",
        "- [Possibility 3: What technological opportunities open up]\n",
        "- [Possibility 4: How timeline or scope can be optimized]\n",
        "\n",
        "TRADE-OFF ANALYSIS:\n",
        "- [Trade-off 1: What becomes more challenging]\n",
        "- [Trade-off 2: What additional resources might be needed]\n",
        "\n",
        "PREDICTED SCORE CHANGES (-5 to +5):\n",
        "- Novelty Change: [score change and reasoning]\n",
        "- Feasibility Change: [score change - may decrease with relaxed constraints]\n",
        "- Impact Change: [score change - typically increases with expanded scope]\n",
        "\n",
        "CONFIDENCE: [1-10 in this constraint relaxation approach]\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.llm.invoke([(\"human\", constraint_prompt)])\n",
        "            return self._parse_evolution_response(\n",
        "                response.content, [hypothesis_review.hypothesis_id],\n",
        "                EvolutionStrategy.CONSTRAINT_RELAXATION\n",
        "            )\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Constraint relaxation failed for {hypothesis_review.hypothesis_id}: {str(e)}\")\n",
        "            return self._create_fallback_evolution_step(\n",
        "                [hypothesis_review.hypothesis_id], EvolutionStrategy.CONSTRAINT_RELAXATION\n",
        "            )\n",
        "\n",
        "    def _parse_evolution_response(self, response_text: str, parent_ids: List[str],\n",
        "                                strategy: EvolutionStrategy) -> EvolutionStep:\n",
        "        \"\"\"Parse LLM evolution response into structured format\"\"\"\n",
        "\n",
        "        try:\n",
        "            # Extract evolved hypothesis content\n",
        "            content_patterns = [\n",
        "                r'evolved hypothesis.*?:(.*?)(?:improvement rationale|rationale|benefits)',\n",
        "                r'hypothesis.*?:(.*?)(?:improvement|rationale|benefits)',\n",
        "                r'combined.*?hypothesis.*?:(.*?)(?:integration|rationale|benefits)'\n",
        "            ]\n",
        "\n",
        "            evolved_content = \"Evolution content could not be extracted\"\n",
        "            for pattern in content_patterns:\n",
        "                match = re.search(pattern, response_text, re.IGNORECASE | re.DOTALL)\n",
        "                if match:\n",
        "                    evolved_content = match.group(1).strip()[:1000]\n",
        "                    break\n",
        "\n",
        "            # Extract rationale\n",
        "            rationale_patterns = [\n",
        "                r'rationale.*?:(.*?)(?:benefits|expected|predicted)',\n",
        "                r'improvement.*?rationale.*?:(.*?)(?:benefits|expected|predicted)',\n",
        "                r'integration.*?rationale.*?:(.*?)(?:benefits|expected|predicted)'\n",
        "            ]\n",
        "\n",
        "            improvement_rationale = \"Rationale could not be extracted\"\n",
        "            for pattern in rationale_patterns:\n",
        "                match = re.search(pattern, response_text, re.IGNORECASE | re.DOTALL)\n",
        "                if match:\n",
        "                    improvement_rationale = match.group(1).strip()[:500]\n",
        "                    break\n",
        "\n",
        "            # Extract benefits and risks\n",
        "            benefits = self._extract_list_items(response_text, [\"benefit\", \"benefits\", \"advantages\"])\n",
        "            risks = self._extract_list_items(response_text, [\"risk\", \"risks\", \"challenge\", \"challenges\"])\n",
        "\n",
        "            # Extract predicted score changes\n",
        "            score_changes = self._extract_score_changes(response_text)\n",
        "\n",
        "            # Extract confidence\n",
        "            confidence_match = re.search(r'confidence.*?(\\\\d+(?:\\\\.\\\\d+)?)', response_text, re.IGNORECASE)\n",
        "            confidence = float(confidence_match.group(1)) if confidence_match else 7.0\n",
        "\n",
        "            # FIX: Corrected datetime call\n",
        "            step_id = f\"evolution_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{strategy.value}\"\n",
        "\n",
        "            return EvolutionStep(\n",
        "                step_id=step_id,\n",
        "                parent_hypothesis_ids=parent_ids,\n",
        "                strategy_used=strategy,\n",
        "                evolution_prompt=\"Evolution prompt executed\",\n",
        "                evolved_content=evolved_content,\n",
        "                improvement_rationale=improvement_rationale,\n",
        "                expected_benefits=benefits,\n",
        "                potential_risks=risks,\n",
        "                novelty_increase=score_changes.get('novelty', 0.0),\n",
        "                feasibility_change=score_changes.get('feasibility', 0.0),\n",
        "                impact_increase=score_changes.get('impact', 0.0),\n",
        "                confidence=confidence,\n",
        "                # FIX: Corrected datetime call\n",
        "                timestamp=datetime.datetime.now().isoformat()\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to parse evolution response: {str(e)}\")\n",
        "            return self._create_fallback_evolution_step(parent_ids, strategy)\n",
        "\n",
        "    def _extract_list_items(self, text: str, keywords: List[str]) -> List[str]:\n",
        "        \"\"\"Extract bulleted list items from text\"\"\"\n",
        "\n",
        "        items = []\n",
        "        for keyword in keywords:\n",
        "            pattern = f'{keyword}.*?:(.*?)(?:[A-Z]{{2,}}.*?:|$)'\n",
        "            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                section_text = match.group(1)\n",
        "                lines = section_text.split('\\\\n')\n",
        "                for line in lines:\n",
        "                    line = line.strip()\n",
        "                    if re.match(r'^[-•*]\\\\s+', line) or re.match(r'^\\\\d+\\\\.\\\\s+', line):\n",
        "                        cleaned_line = re.sub(r'^[-•*\\\\d.]\\\\s*', '', line)\n",
        "                        if len(cleaned_line) > 10:\n",
        "                            items.append(cleaned_line[:150])\n",
        "                break\n",
        "\n",
        "        return items[:4] if items else [f\"No specific {keywords[0]} identified\"]\n",
        "\n",
        "    def _extract_score_changes(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\"Extract predicted score changes from text\"\"\"\n",
        "\n",
        "        changes = {}\n",
        "        criteria = ['novelty', 'feasibility', 'impact']\n",
        "\n",
        "        for criterion in criteria:\n",
        "            pattern = f'{criterion}.*?change.*?([+-]?\\\\d+(?:\\\\.\\\\d+)?)'\n",
        "            match = re.search(pattern, text, re.IGNORECASE)\n",
        "            if match:\n",
        "                try:\n",
        "                    changes[criterion] = float(match.group(1))\n",
        "                except ValueError:\n",
        "                    changes[criterion] = 0.0\n",
        "            else:\n",
        "                changes[criterion] = 0.0\n",
        "\n",
        "        return changes\n",
        "\n",
        "    def _create_fallback_evolution_step(self, parent_ids: List[str],\n",
        "                                      strategy: EvolutionStrategy) -> EvolutionStep:\n",
        "        \"\"\"Create fallback evolution step when parsing fails\"\"\"\n",
        "\n",
        "        # FIX: Corrected datetime call\n",
        "        step_id = f\"fallback_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}_{strategy.value}\"\n",
        "\n",
        "        return EvolutionStep(\n",
        "            step_id=step_id,\n",
        "            parent_hypothesis_ids=parent_ids,\n",
        "            strategy_used=strategy,\n",
        "            evolution_prompt=\"Fallback evolution step\",\n",
        "            evolved_content=f\"Evolved hypothesis using {strategy.value} strategy (fallback generated)\",\n",
        "            improvement_rationale=f\"Applied {strategy.value} strategy to improve the original hypothesis\",\n",
        "            expected_benefits=[f\"Expected benefits from {strategy.value}\", \"Improved hypothesis quality\"],\n",
        "            potential_risks=[\"Fallback evolution may need manual refinement\"],\n",
        "            novelty_increase=0.5,\n",
        "            feasibility_change=0.0,\n",
        "            impact_increase=0.5,\n",
        "            confidence=5.0,\n",
        "            # FIX: Corrected datetime call\n",
        "            timestamp=datetime.datetime.now().isoformat()\n",
        "        )\n",
        "\n",
        "    def evolve_top_hypotheses(self, ranking_state: RankingState,\n",
        "                            reflection_state: ReflectionState,\n",
        "                            original_constraints: List[str] = None,\n",
        "                            top_n: int = 3) -> EvolutionState:\n",
        "        \"\"\"Evolve the top N hypotheses using multiple strategies\"\"\"\n",
        "\n",
        "        logger.info(f\"Starting evolution of top {top_n} hypotheses\")\n",
        "\n",
        "        if original_constraints is None:\n",
        "            original_constraints = [\"Must be clinically applicable\", \"Focus on translational approaches\"]\n",
        "\n",
        "        evolution_state = EvolutionState()\n",
        "        self.generation_counter += 1\n",
        "\n",
        "        # Get top hypotheses from ranking\n",
        "        top_rankings = ranking_state.final_rankings[:top_n]\n",
        "        review_lookup = {r.hypothesis_id: r for r in reflection_state.hypothesis_reviews}\n",
        "\n",
        "        # Apply different strategies to different hypotheses\n",
        "        for i, ranking in enumerate(top_rankings):\n",
        "            hypothesis_id = ranking['hypothesis_id']\n",
        "            hypothesis_review = review_lookup.get(hypothesis_id)\n",
        "            if not hypothesis_review:\n",
        "                logger.warning(f\"Could not find review for hypothesis {hypothesis_id}, skipping evolution.\")\n",
        "                continue\n",
        "\n",
        "            logger.info(f\"Evolving #{ranking['rank']}: {hypothesis_id}\")\n",
        "\n",
        "            # Strategy selection based on rank and characteristics\n",
        "            evolution_steps = []\n",
        "\n",
        "            # Rank 1: Apply multiple strategies\n",
        "            if ranking['rank'] == 1:\n",
        "                # Simplify the top hypothesis for broader applicability\n",
        "                evolution_steps.append(\n",
        "                    self.simplify_hypothesis(hypothesis_review, ranking)\n",
        "                )\n",
        "                # Also create a radical variation for breakthrough potential\n",
        "                evolution_steps.append(\n",
        "                    self.radical_variation(hypothesis_review, ranking)\n",
        "                )\n",
        "\n",
        "            # Rank 2: Focus on enhancement strategies\n",
        "            elif ranking['rank'] == 2:\n",
        "                # Apply analogical reasoning for novel insights\n",
        "                evolution_steps.append(\n",
        "                    self.analogical_reasoning(hypothesis_review, ranking)\n",
        "                )\n",
        "                # Relax constraints to explore expanded possibilities\n",
        "                evolution_steps.append(\n",
        "                    self.constraint_relaxation(hypothesis_review, ranking, original_constraints)\n",
        "                )\n",
        "\n",
        "            # Rank 3: Apply targeted improvements\n",
        "            else:\n",
        "                # Simplify to increase feasibility\n",
        "                evolution_steps.append(\n",
        "                    self.simplify_hypothesis(hypothesis_review, ranking)\n",
        "                )\n",
        "\n",
        "            # Create evolved hypotheses from each evolution step\n",
        "            for step in evolution_steps:\n",
        "                evolved_hyp = self._create_evolved_hypothesis(step, hypothesis_review)\n",
        "                evolution_state.evolved_hypotheses.append(evolved_hyp)\n",
        "\n",
        "        # Apply combination strategy if we have multiple top hypotheses\n",
        "        if len(top_rankings) >= 2:\n",
        "            top_2_reviews = [review_lookup[ranking['hypothesis_id']] for ranking in top_rankings[:2] if ranking['hypothesis_id'] in review_lookup]\n",
        "            top_2_rankings = top_rankings[:2]\n",
        "\n",
        "            if len(top_2_reviews) >= 2:\n",
        "                combination_step = self.combine_hypotheses(top_2_reviews, top_2_rankings)\n",
        "                combined_hyp = self._create_evolved_hypothesis(combination_step, None)\n",
        "                evolution_state.evolved_hypotheses.append(combined_hyp)\n",
        "\n",
        "        # Generate analytics and summary\n",
        "        evolution_state.evolution_statistics = self._compute_evolution_statistics(evolution_state)\n",
        "        evolution_state.strategy_effectiveness = self._analyze_strategy_effectiveness(evolution_state)\n",
        "        evolution_state.best_evolved_hypothesis = self._identify_best_evolved(evolution_state)\n",
        "        evolution_state.generation_summary = self._generate_evolution_summary(evolution_state)\n",
        "\n",
        "        logger.info(f\"Evolution completed: {len(evolution_state.evolved_hypotheses)} evolved hypotheses generated\")\n",
        "        return evolution_state\n",
        "\n",
        "    def _create_evolved_hypothesis(self, evolution_step: EvolutionStep,\n",
        "                                 original_review: Optional[HypothesisReview]) -> EvolvedHypothesis:\n",
        "        \"\"\"Create evolved hypothesis from evolution step\"\"\"\n",
        "        # FIX: Corrected datetime call\n",
        "        hypothesis_id = f\"evolved_{evolution_step.strategy_used.value}_{datetime.datetime.now().strftime('%H%M%S')}\"\n",
        "\n",
        "        # Predict new scores based on evolution step\n",
        "        if original_review:\n",
        "            predicted_scores = {\n",
        "                'novelty': min(10, max(1, original_review.criteria.novelty_score + evolution_step.novelty_increase)),\n",
        "                'feasibility': min(10, max(1, original_review.criteria.feasibility_score + evolution_step.feasibility_change)),\n",
        "                'scientific_rigor': original_review.criteria.scientific_rigor_score,  # Usually preserved\n",
        "                'impact_potential': min(10, max(1, original_review.criteria.impact_potential_score + evolution_step.impact_increase)),\n",
        "                'testability': original_review.criteria.testability_score  # Usually preserved\n",
        "            }\n",
        "        else:\n",
        "            # For combinations, estimate higher scores\n",
        "            predicted_scores = {\n",
        "                'novelty': 8.0 + evolution_step.novelty_increase,\n",
        "                'feasibility': 7.0 + evolution_step.feasibility_change,\n",
        "                'scientific_rigor': 7.5,\n",
        "                'impact_potential': 8.5 + evolution_step.impact_increase,\n",
        "                'testability': 7.0\n",
        "            }\n",
        "\n",
        "        # Generate implementation roadmap\n",
        "        roadmap = self._generate_implementation_roadmap(evolution_step)\n",
        "        competitive_advantages = evolution_step.expected_benefits[:3]\n",
        "\n",
        "        return EvolvedHypothesis(\n",
        "            hypothesis_id=hypothesis_id,\n",
        "            evolved_content=evolution_step.evolved_content,\n",
        "            parent_hypothesis_ids=evolution_step.parent_hypothesis_ids,\n",
        "            evolution_lineage=[evolution_step],\n",
        "            generation_number=self.generation_counter,\n",
        "            predicted_scores=predicted_scores,\n",
        "            improvement_summary=evolution_step.improvement_rationale,\n",
        "            competitive_advantages=competitive_advantages,\n",
        "            implementation_roadmap=roadmap\n",
        "        )\n",
        "\n",
        "    def _generate_implementation_roadmap(self, evolution_step: EvolutionStep) -> List[str]:\n",
        "        \"\"\"Generate implementation roadmap based on evolution strategy\"\"\"\n",
        "\n",
        "        strategy = evolution_step.strategy_used\n",
        "\n",
        "        roadmap_templates = {\n",
        "            EvolutionStrategy.SIMPLIFICATION: [\n",
        "                \"Phase 1: Validate simplified approach in proof-of-concept studies\",\n",
        "                \"Phase 2: Optimize focused methodology for maximum impact\",\n",
        "                \"Phase 3: Scale up simplified approach for broader application\"\n",
        "            ],\n",
        "            EvolutionStrategy.COMBINATION: [\n",
        "                \"Phase 1: Validate synergistic effects between combined approaches\",\n",
        "                \"Phase 2: Optimize integration protocols and workflows\",\n",
        "                \"Phase 3: Implement unified approach in comprehensive studies\"\n",
        "            ],\n",
        "            EvolutionStrategy.ANALOGICAL_REASONING: [\n",
        "                \"Phase 1: Validate cross-domain adaptation in target context\",\n",
        "                \"Phase 2: Refine approach based on domain-specific requirements\",\n",
        "                \"Phase 3: Demonstrate superior performance vs conventional methods\"\n",
        "            ],\n",
        "            EvolutionStrategy.RADICAL_VARIATION: [\n",
        "                \"Phase 1: Establish proof-of-concept for paradigm-shifting approach\",\n",
        "                \"Phase 2: Address technical challenges and validation requirements\",\n",
        "                \"Phase 3: Develop breakthrough approach for transformative impact\"\n",
        "            ],\n",
        "            EvolutionStrategy.CONSTRAINT_RELAXATION: [\n",
        "                \"Phase 1: Explore expanded possibilities with relaxed constraints\",\n",
        "                \"Phase 2: Optimize approach within new possibility space\",\n",
        "                \"Phase 3: Demonstrate enhanced impact from expanded scope\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        return roadmap_templates.get(strategy, [\n",
        "            \"Phase 1: Validate evolved approach\",\n",
        "            \"Phase 2: Optimize implementation\",\n",
        "            \"Phase 3: Scale for maximum impact\"\n",
        "        ])\n",
        "\n",
        "    def _compute_evolution_statistics(self, evolution_state: EvolutionState) -> Dict[str, float]:\n",
        "        \"\"\"Compute comprehensive evolution statistics\"\"\"\n",
        "\n",
        "        if not evolution_state.evolved_hypotheses:\n",
        "            return {}\n",
        "\n",
        "        predicted_overall_scores = []\n",
        "        novelty_improvements = []\n",
        "        impact_improvements = []\n",
        "        feasibility_changes = []\n",
        "        confidence_levels = []\n",
        "\n",
        "        for evolved_hyp in evolution_state.evolved_hypotheses:\n",
        "            # Calculate predicted overall score\n",
        "            scores = evolved_hyp.predicted_scores\n",
        "            overall = (scores['novelty'] * 0.25 + scores['feasibility'] * 0.20 +\n",
        "                      scores['scientific_rigor'] * 0.25 + scores['impact_potential'] * 0.20 +\n",
        "                      scores['testability'] * 0.10)\n",
        "            predicted_overall_scores.append(overall)\n",
        "\n",
        "            # Get evolution metrics from lineage\n",
        "            if evolved_hyp.evolution_lineage:\n",
        "                step = evolved_hyp.evolution_lineage[0]\n",
        "                novelty_improvements.append(step.novelty_increase)\n",
        "                impact_improvements.append(step.impact_increase)\n",
        "                feasibility_changes.append(step.feasibility_change)\n",
        "                confidence_levels.append(step.confidence)\n",
        "\n",
        "        if not predicted_overall_scores: return {}\n",
        "\n",
        "        return {\n",
        "            'total_evolved_hypotheses': len(evolution_state.evolved_hypotheses),\n",
        "            'mean_predicted_score': statistics.mean(predicted_overall_scores),\n",
        "            'max_predicted_score': max(predicted_overall_scores),\n",
        "            'min_predicted_score': min(predicted_overall_scores),\n",
        "            'mean_novelty_improvement': statistics.mean(novelty_improvements),\n",
        "            'mean_impact_improvement': statistics.mean(impact_improvements),\n",
        "            'mean_feasibility_change': statistics.mean(feasibility_changes),\n",
        "            'mean_evolution_confidence': statistics.mean(confidence_levels),\n",
        "            'strategies_applied': len(set(hyp.evolution_lineage[0].strategy_used for hyp in evolution_state.evolved_hypotheses)),\n",
        "            'generation_number': self.generation_counter\n",
        "        }\n",
        "\n",
        "    def _analyze_strategy_effectiveness(self, evolution_state: EvolutionState) -> Dict[str, float]:\n",
        "        \"\"\"Analyze effectiveness of different evolution strategies\"\"\"\n",
        "\n",
        "        strategy_metrics = {}\n",
        "\n",
        "        for evolved_hyp in evolution_state.evolved_hypotheses:\n",
        "            if evolved_hyp.evolution_lineage:\n",
        "                step = evolved_hyp.evolution_lineage[0]\n",
        "                strategy = step.strategy_used.value\n",
        "\n",
        "                if strategy not in strategy_metrics:\n",
        "                    strategy_metrics[strategy] = {\n",
        "                        'count': 0,\n",
        "                        'total_novelty_gain': 0,\n",
        "                        'total_impact_gain': 0,\n",
        "                        'total_confidence': 0,\n",
        "                        'predicted_scores': []\n",
        "                    }\n",
        "\n",
        "                metrics = strategy_metrics[strategy]\n",
        "                metrics['count'] += 1\n",
        "                metrics['total_novelty_gain'] += step.novelty_increase\n",
        "                metrics['total_impact_gain'] += step.impact_increase\n",
        "                metrics['total_confidence'] += step.confidence\n",
        "\n",
        "                # Calculate predicted overall score\n",
        "                scores = evolved_hyp.predicted_scores\n",
        "                overall = (scores['novelty'] * 0.25 + scores['feasibility'] * 0.20 +\n",
        "                          scores['scientific_rigor'] * 0.25 + scores['impact_potential'] * 0.20 +\n",
        "                          scores['testability'] * 0.10)\n",
        "                metrics['predicted_scores'].append(overall)\n",
        "\n",
        "        # Calculate effectiveness scores\n",
        "        effectiveness = {}\n",
        "        for strategy, metrics in strategy_metrics.items():\n",
        "            if metrics['count'] > 0:\n",
        "                effectiveness[f\"{strategy}_avg_novelty_gain\"] = metrics['total_novelty_gain'] / metrics['count']\n",
        "                effectiveness[f\"{strategy}_avg_impact_gain\"] = metrics['total_impact_gain'] / metrics['count']\n",
        "                effectiveness[f\"{strategy}_avg_confidence\"] = metrics['total_confidence'] / metrics['count']\n",
        "                effectiveness[f\"{strategy}_avg_predicted_score\"] = statistics.mean(metrics['predicted_scores'])\n",
        "                effectiveness[f\"{strategy}_applications\"] = metrics['count']\n",
        "\n",
        "        return effectiveness\n",
        "\n",
        "    def _identify_best_evolved(self, evolution_state: EvolutionState) -> Optional[str]:\n",
        "        \"\"\"Identify the best evolved hypothesis based on predicted scores\"\"\"\n",
        "\n",
        "        if not evolution_state.evolved_hypotheses:\n",
        "            return None\n",
        "\n",
        "        best_hypothesis = max(\n",
        "            evolution_state.evolved_hypotheses,\n",
        "            key=lambda h: statistics.mean(list(h.predicted_scores.values()))\n",
        "        )\n",
        "\n",
        "        return best_hypothesis.hypothesis_id\n",
        "\n",
        "    def _generate_evolution_summary(self, evolution_state: EvolutionState) -> str:\n",
        "        \"\"\"Generate comprehensive evolution summary\"\"\"\n",
        "\n",
        "        stats = evolution_state.evolution_statistics\n",
        "        strategy_eff = evolution_state.strategy_effectiveness\n",
        "        best_id = evolution_state.best_evolved_hypothesis\n",
        "\n",
        "        # Find best hypothesis for details\n",
        "        best_hyp = None\n",
        "        if best_id:\n",
        "            best_hyp = next((h for h in evolution_state.evolved_hypotheses if h.hypothesis_id == best_id), None)\n",
        "\n",
        "        if not stats: return \"Evolution summary could not be generated.\"\n",
        "\n",
        "        summary = f\"\"\"\n",
        "EVOLUTION AGENT GENERATION SUMMARY\n",
        "\n",
        "🧬 EVOLUTION OVERVIEW:\n",
        "• Generation Number: {stats.get('generation_number', 1)}\n",
        "• Evolved Hypotheses: {stats.get('total_evolved_hypotheses', 0)}\n",
        "• Strategies Applied: {stats.get('strategies_applied', 0)}\n",
        "• Evolution Confidence: {stats.get('mean_evolution_confidence', 0):.1f}/10\n",
        "\n",
        "📈 IMPROVEMENT METRICS:\n",
        "• Mean Predicted Score: {stats.get('mean_predicted_score', 0):.2f}/10\n",
        "• Score Range: {stats.get('min_predicted_score', 0):.1f} - {stats.get('max_predicted_score', 0):.1f}\n",
        "• Average Novelty Gain: {stats.get('mean_novelty_improvement', 0):+.1f} points\n",
        "• Average Impact Gain: {stats.get('mean_impact_improvement', 0):+.1f} points\n",
        "• Feasibility Change: {stats.get('mean_feasibility_change', 0):+.1f} points\n",
        "\n",
        "🎯 STRATEGY EFFECTIVENESS:\n",
        "{self._format_strategy_effectiveness(strategy_eff)}\n",
        "\n",
        "🏆 BEST EVOLVED HYPOTHESIS:\n",
        "{self._format_best_hypothesis(best_hyp)}\n",
        "\n",
        "🚀 IMPLEMENTATION RECOMMENDATIONS:\n",
        "{self._generate_implementation_recommendations(evolution_state)}\n",
        "\"\"\"\n",
        "\n",
        "        return summary.strip()\n",
        "\n",
        "    def _format_strategy_effectiveness(self, strategy_effectiveness: Dict[str, float]) -> str:\n",
        "        \"\"\"Format strategy effectiveness for display\"\"\"\n",
        "\n",
        "        strategies = set()\n",
        "        for key in strategy_effectiveness.keys():\n",
        "            if '_avg_predicted_score' in key:\n",
        "                strategies.add(key.replace('_avg_predicted_score', ''))\n",
        "\n",
        "        formatted = \"\"\n",
        "        for strategy in sorted(list(strategies)):\n",
        "            score = strategy_effectiveness.get(f\"{strategy}_avg_predicted_score\", 0)\n",
        "            novelty = strategy_effectiveness.get(f\"{strategy}_avg_novelty_gain\", 0)\n",
        "            impact = strategy_effectiveness.get(f\"{strategy}_avg_impact_gain\", 0)\n",
        "            count = strategy_effectiveness.get(f\"{strategy}_applications\", 0)\n",
        "\n",
        "            formatted += f\"\\n• {strategy.title()}: Score {score:.1f} (+{novelty:.1f} novelty, +{impact:.1f} impact) [{int(count)} applications]\"\n",
        "\n",
        "        return formatted if formatted else \"No strategy effectiveness data available\"\n",
        "\n",
        "    def _format_best_hypothesis(self, best_hyp: Optional[EvolvedHypothesis]) -> str:\n",
        "        \"\"\"Format best hypothesis for display\"\"\"\n",
        "\n",
        "        if not best_hyp:\n",
        "            return \"No best hypothesis identified\"\n",
        "\n",
        "        strategy = best_hyp.evolution_lineage[0].strategy_used.value if best_hyp.evolution_lineage else \"unknown\"\n",
        "        predicted_score = statistics.mean(list(best_hyp.predicted_scores.values()))\n",
        "\n",
        "        return f\"\"\"\n",
        "• ID: {best_hyp.hypothesis_id}\n",
        "• Strategy Used: {strategy.title()}\n",
        "• Predicted Score: {predicted_score:.2f}/10\n",
        "• Key Advantages: {'; '.join(best_hyp.competitive_advantages[:2])}\n",
        "• Parent(s): {', '.join(best_hyp.parent_hypothesis_ids)}\"\"\"\n",
        "\n",
        "    def _generate_implementation_recommendations(self, evolution_state: EvolutionState) -> str:\n",
        "        \"\"\"Generate implementation recommendations\"\"\"\n",
        "\n",
        "        recommendations = []\n",
        "        stats = evolution_state.evolution_statistics\n",
        "\n",
        "        if not stats: return \"No recommendations available.\"\n",
        "\n",
        "        # Best hypothesis recommendations\n",
        "        if evolution_state.best_evolved_hypothesis:\n",
        "            recommendations.append(f\"• PRIORITY: Focus on {evolution_state.best_evolved_hypothesis} for detailed development\")\n",
        "\n",
        "        # Strategy-based recommendations\n",
        "        mean_score = stats.get('mean_predicted_score', 0)\n",
        "        if mean_score > 8.0:\n",
        "            recommendations.append(\"• EXCELLENT: High-quality evolved hypotheses - proceed to validation\")\n",
        "        elif mean_score > 7.0:\n",
        "            recommendations.append(\"• STRONG: Good evolution results - consider further refinement\")\n",
        "        else:\n",
        "            recommendations.append(\"• ITERATE: Consider additional evolution rounds\")\n",
        "\n",
        "        # Novelty vs feasibility trade-offs\n",
        "        novelty_gain = stats.get('mean_novelty_improvement', 0)\n",
        "        feasibility_change = stats.get('mean_feasibility_change', 0)\n",
        "\n",
        "        if novelty_gain > 1.5 and feasibility_change < -1.0:\n",
        "            recommendations.append(\"• BALANCE: High novelty gained but feasibility decreased - validate technical approach\")\n",
        "        elif novelty_gain > 2.0:\n",
        "            recommendations.append(\"• BREAKTHROUGH: Significant novelty increases - potential for high impact\")\n",
        "\n",
        "        # Next steps\n",
        "        if len(evolution_state.evolved_hypotheses) >= 3:\n",
        "            recommendations.append(\"• NEXT: Run reflection and ranking on evolved hypotheses for comparison\")\n",
        "\n",
        "        return '\\\\n'.join(recommendations) if recommendations else \"• Proceed with evolved hypotheses as planned\"\n",
        "\n",
        "# ===========================================================================\n",
        "# INTEGRATION FUNCTIONS\n",
        "# ===========================================================================\n",
        "\n",
        "def run_evolution_process(ranking_state: RankingState, reflection_state: ReflectionState,\n",
        "                         original_constraints: List[str] = None, top_n: int = 3) -> EvolutionState:\n",
        "    \"\"\"Run complete evolution process on top-ranked hypotheses\"\"\"\n",
        "\n",
        "    if not ranking_state.final_rankings:\n",
        "        raise ValueError(\"No rankings found in ranking state\")\n",
        "\n",
        "    # Ensure we have the LLM available\n",
        "    try:\n",
        "        evolution_agent = EvolutionAgent(llm)\n",
        "    except NameError:\n",
        "        raise RuntimeError(\"LLM not available. Run setup cell first.\")\n",
        "\n",
        "    # Run evolution\n",
        "    evolution_state = evolution_agent.evolve_top_hypotheses(\n",
        "        ranking_state, reflection_state, original_constraints, top_n\n",
        "    )\n",
        "\n",
        "    return evolution_state\n",
        "\n",
        "def integrate_evolution_with_generation_state(generation_state: GenerationState,\n",
        "                                            evolution_state: EvolutionState) -> GenerationState:\n",
        "    \"\"\"Integrate evolution results with generation state\"\"\"\n",
        "\n",
        "    # Add evolution results to generation state\n",
        "    if hasattr(generation_state, 'evolution_results'):\n",
        "        generation_state.evolution_results = evolution_state\n",
        "    else:\n",
        "        setattr(generation_state, 'evolution_results', evolution_state)\n",
        "\n",
        "    # Update status\n",
        "    generation_state.status = \"hypotheses_evolved\"\n",
        "\n",
        "    return generation_state\n",
        "\n",
        "# ===========================================================================\n",
        "# TESTING CODE\n",
        "# ===========================================================================\n",
        "\n",
        "def test_evolution_agent():\n",
        "    \"\"\"Comprehensive test suite for Evolution Agent\"\"\"\n",
        "\n",
        "    print(\"🧬 TESTING EVOLUTION AGENT\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create comprehensive test data\n",
        "    print(\"📋 Setting up test data...\")\n",
        "\n",
        "    # Mock reflection and ranking states\n",
        "    mock_reviews = [\n",
        "        HypothesisReview(\n",
        "            hypothesis_id=\"top_hypothesis_1\",\n",
        "            hypothesis_text=\"Advanced epigenetic reprogramming approach targeting DNMT3A and HDAC2 in activated hepatic stellate cells for liver fibrosis reversal. This approach combines selective inhibition with targeted delivery systems.\",\n",
        "            criteria=ReviewCriteria(8.5, 7.0, 8.0, 9.0, 7.5, \"Highly novel\", \"Challenging but feasible\", \"Strong evidence\", \"Transformative impact\", \"Well testable\"),\n",
        "            overall_score=8.2,\n",
        "            overall_assessment=\"Exceptional hypothesis with breakthrough potential\",\n",
        "            strengths=[\"Novel dual-target approach\", \"Strong mechanistic foundation\", \"Clear therapeutic pathway\"],\n",
        "            weaknesses=[\"Complex delivery requirements\", \"Regulatory challenges\"],\n",
        "            recommendations=[\"Develop targeted delivery\", \"Validate in animal models\"],\n",
        "            confidence_level=8.5,\n",
        "            review_timestamp=datetime.datetime.now().isoformat(),\n",
        "            reviewer_type=\"detailed\"\n",
        "        ),\n",
        "        HypothesisReview(\n",
        "            hypothesis_id=\"top_hypothesis_2\",\n",
        "            hypothesis_text=\"AI-powered personalized medicine platform that analyzes patient epigenetic profiles to optimize combination therapies for liver fibrosis treatment.\",\n",
        "            criteria=ReviewCriteria(9.0, 6.0, 7.0, 8.5, 6.5, \"Breakthrough innovation\", \"Technical challenges\", \"Good foundation\", \"High impact\", \"Complex validation\"),\n",
        "            overall_score=7.6,\n",
        "            overall_assessment=\"Innovative AI approach with significant potential\",\n",
        "            strengths=[\"Revolutionary AI application\", \"Personalized approach\", \"Scalable platform\"],\n",
        "            weaknesses=[\"Technical complexity\", \"Data requirements\", \"Validation challenges\"],\n",
        "            recommendations=[\"Develop proof-of-concept\", \"Secure data partnerships\"],\n",
        "            confidence_level=7.5,\n",
        "            review_timestamp=datetime.datetime.now().isoformat(),\n",
        "            reviewer_type=\"detailed\"\n",
        "        ),\n",
        "        HypothesisReview(\n",
        "            hypothesis_id=\"top_hypothesis_3\",\n",
        "            hypothesis_text=\"Single-cell epigenomic mapping combined with machine learning to identify stage-specific therapeutic vulnerabilities in liver fibrosis progression.\",\n",
        "            criteria=ReviewCriteria(7.5, 7.5, 8.5, 7.0, 8.0, \"Good novelty\", \"Very feasible\", \"Excellent methodology\", \"Solid impact\", \"Highly testable\"),\n",
        "            overall_score=7.7,\n",
        "            overall_assessment=\"Well-designed discovery approach with strong scientific foundation\",\n",
        "            strengths=[\"Cutting-edge technology\", \"Discovery potential\", \"Rigorous methodology\"],\n",
        "            weaknesses=[\"Discovery focus\", \"Resource intensive\", \"Timeline uncertainty\"],\n",
        "            recommendations=[\"Focus on key stages\", \"Develop analysis pipeline\"],\n",
        "            confidence_level=8.0,\n",
        "            review_timestamp=datetime.datetime.now().isoformat(),\n",
        "            reviewer_type=\"detailed\"\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    mock_reflection_state = ReflectionState(\n",
        "        hypothesis_reviews=mock_reviews,\n",
        "        review_statistics={'mean_overall_score': 7.83},\n",
        "        quality_flags=[],\n",
        "        batch_summary=\"Mock reflection state for evolution testing\"\n",
        "    )\n",
        "\n",
        "    mock_rankings = [\n",
        "        {'rank': 1, 'hypothesis_id': 'top_hypothesis_1', 'final_elo_rating': 1350, 'win_rate': 75.0},\n",
        "        {'rank': 2, 'hypothesis_id': 'top_hypothesis_2', 'final_elo_rating': 1280, 'win_rate': 60.0},\n",
        "        {'rank': 3, 'hypothesis_id': 'top_hypothesis_3', 'final_elo_rating': 1220, 'win_rate': 50.0}\n",
        "    ]\n",
        "\n",
        "    mock_ranking_state = RankingState(\n",
        "        final_rankings=mock_rankings,\n",
        "        ranking_statistics={'mean_final_rating': 1283}\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Created test data with {len(mock_reviews)} hypotheses\")\n",
        "\n",
        "    # Test individual evolution strategies\n",
        "    try:\n",
        "        evolution_agent = EvolutionAgent(llm)\n",
        "    except NameError:\n",
        "        print(\"❌ Error: LLM not available. Cannot test evolution strategies.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n1️⃣ TESTING INDIVIDUAL EVOLUTION STRATEGIES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Test simplification\n",
        "    print(\"Testing Simplification Strategy...\")\n",
        "    simplification_step = evolution_agent.simplify_hypothesis(mock_reviews[0], mock_rankings[0])\n",
        "    print(f\"✅ Simplification: {simplification_step.strategy_used.value}\")\n",
        "    print(f\"   Confidence: {simplification_step.confidence:.1f}/10\")\n",
        "    print(f\"   Novelty change: {simplification_step.novelty_increase:+.1f}\")\n",
        "    print(f\"   Impact change: {simplification_step.impact_increase:+.1f}\")\n",
        "\n",
        "    # Test analogical reasoning\n",
        "    print(\"\\nTesting Analogical Reasoning Strategy...\")\n",
        "    analogical_step = evolution_agent.analogical_reasoning(mock_reviews[1], mock_rankings[1])\n",
        "    print(f\"✅ Analogical Reasoning: {analogical_step.strategy_used.value}\")\n",
        "    print(f\"   Benefits: {len(analogical_step.expected_benefits)}\")\n",
        "    print(f\"   Risks: {len(analogical_step.potential_risks)}\")\n",
        "\n",
        "    # Test combination\n",
        "    print(\"\\nTesting Combination Strategy...\")\n",
        "    combination_step = evolution_agent.combine_hypotheses(mock_reviews[:2], mock_rankings[:2])\n",
        "    print(f\"✅ Combination: {combination_step.strategy_used.value}\")\n",
        "    print(f\"   Parent IDs: {combination_step.parent_hypothesis_ids}\")\n",
        "    print(f\"   Expected impact: {combination_step.impact_increase:+.1f}\")\n",
        "\n",
        "    print(\"\\n2️⃣ TESTING FULL EVOLUTION PROCESS\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Test complete evolution process\n",
        "    original_constraints = [\n",
        "        \"Must be clinically applicable\",\n",
        "        \"Focus on translational approaches\",\n",
        "        \"Consider regulatory pathways\"\n",
        "    ]\n",
        "\n",
        "    evolution_state = evolution_agent.evolve_top_hypotheses(\n",
        "        mock_ranking_state,\n",
        "        mock_reflection_state,\n",
        "        original_constraints,\n",
        "        top_n=3\n",
        "    )\n",
        "\n",
        "    print(f\"✅ Evolution process completed\")\n",
        "    print(f\"   Evolved hypotheses: {len(evolution_state.evolved_hypotheses)}\")\n",
        "    print(f\"   Best evolved: {evolution_state.best_evolved_hypothesis}\")\n",
        "    print(f\"   Generation number: {evolution_state.evolution_statistics.get('generation_number', 1)}\")\n",
        "\n",
        "    print(\"\\n3️⃣ TESTING EVOLUTION ANALYSIS\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    print(\"📊 Evolution Statistics:\")\n",
        "    stats = evolution_state.evolution_statistics\n",
        "    for key, value in stats.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"   {key}: {value:.2f}\")\n",
        "        else:\n",
        "            print(f\"   {key}: {value}\")\n",
        "\n",
        "    print(f\"\\n🎯 Strategy Effectiveness:\")\n",
        "    strategy_eff = evolution_state.strategy_effectiveness\n",
        "    for key, value in strategy_eff.items():\n",
        "        print(f\"   {key}: {value:.2f}\")\n",
        "\n",
        "    print(\"\\n4️⃣ TESTING EVOLVED HYPOTHESES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for i, evolved_hyp in enumerate(evolution_state.evolved_hypotheses[:3], 1):\n",
        "        strategy = evolved_hyp.evolution_lineage[0].strategy_used.value if evolved_hyp.evolution_lineage else \"unknown\"\n",
        "        predicted_score = statistics.mean(list(evolved_hyp.predicted_scores.values()))\n",
        "\n",
        "        print(f\"\\nEvolved Hypothesis {i}:\")\n",
        "        print(f\"   ID: {evolved_hyp.hypothesis_id}\")\n",
        "        print(f\"   Strategy: {strategy}\")\n",
        "        print(f\"   Predicted Score: {predicted_score:.2f}/10\")\n",
        "        print(f\"   Parents: {', '.join(evolved_hyp.parent_hypothesis_ids)}\")\n",
        "        print(f\"   Key Advantages: {'; '.join(evolved_hyp.competitive_advantages[:2])}\")\n",
        "        print(f\"   Content Preview: {evolved_hyp.evolved_content[:150]}...\")\n",
        "\n",
        "    print(\"\\n5️⃣ TESTING INTEGRATION\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Test integration with generation state\n",
        "    mock_generation_state = GenerationState(\n",
        "        research_goal=\"Test evolution integration\",\n",
        "        status=\"hypotheses_ranked\"\n",
        "    )\n",
        "\n",
        "    integrated_state = integrate_evolution_with_generation_state(mock_generation_state, evolution_state)\n",
        "\n",
        "    print(f\"✅ Integration successful\")\n",
        "    print(f\"   State status: {integrated_state.status}\")\n",
        "    print(f\"   Has evolution results: {hasattr(integrated_state, 'evolution_results')}\")\n",
        "\n",
        "    print(\"\\n6️⃣ EVOLUTION SUMMARY\")\n",
        "    print(\"-\" * 40)\n",
        "    print(evolution_state.generation_summary)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"🎉 ALL EVOLUTION AGENT TESTS COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"🧬 The Evolution Agent is ready to improve top hypotheses\")\n",
        "\n",
        "    return evolution_state\n",
        "\n",
        "# ===========================================================================\n",
        "# RUN TESTS\n",
        "# ===========================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_results = test_evolution_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQYn31TNBvlC",
        "outputId": "46e709c2-7472-4960-c776-f854b494751f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 TESTING ENHANCED META-REVIEW AGENT (v2 - FIXED)\n",
            "=======================================================\n",
            "✅ Analysis Quality: high\n",
            "📊 Patterns Found: 3\n",
            "🔗 Correlations: 5\n",
            "🏷️ Clustering Method: sklearn_agglomerative_improved\n",
            "📈 Sample Correlations:\n",
            "   • novelty_score: 0.000\n",
            "   • feasibility_score: 0.000\n",
            "   • scientific_rigor_score: 0.000\n",
            "📊 Diversity Score: 0.571\n",
            "🤝 Consensus Score: 0.000\n",
            "\n",
            "=======================================================\n",
            "🎉 ENHANCED META-REVIEW AGENT (v2) TEST COMPLETED!\n",
            "🛠️  Issues fixed: TF-IDF parameters, correlation calculation, clustering robustness\n",
            "\n",
            "📋 Sample Pattern: Cluster 1: Themes - hypothesis, validation, good, focus, creative\n",
            "\n",
            "📊 Report Summary:\n",
            "   Clustering: sklearn_agglomerative_improved\n",
            "   Quality: high\n"
          ]
        }
      ],
      "source": [
        "#@title 🚀 Meta-Review Agent Enhancements (Fixed - v2)\n",
        "# ===========================================================================\n",
        "# FIXES:\n",
        "# 1. More robust TF-IDF parameters for small datasets\n",
        "# 2. Better correlation calculation with variance checking\n",
        "# 3. Enhanced text preprocessing\n",
        "# 4. Improved error handling for edge cases\n",
        "# 5. Corrected deprecated datetime.utcnow() calls\n",
        "# 6. Corrected a SyntaxError from an invalid escape character\n",
        "# ===========================================================================\n",
        "\n",
        "import asyncio\n",
        "import json\n",
        "import pickle\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import logging\n",
        "import warnings\n",
        "from typing import Union, Protocol, List, Dict, Any, Optional, Tuple\n",
        "from dataclasses import dataclass, field, asdict\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "# Suppress numpy warnings for cleaner output\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ===========================================================================\n",
        "# 1️⃣ CORE DATACLASSES (Same as before)\n",
        "# ===========================================================================\n",
        "\n",
        "@dataclass\n",
        "class PatternInsight:\n",
        "    \"\"\"Represents a discovered pattern in hypothesis reviews.\"\"\"\n",
        "    description: str\n",
        "    frequency: int\n",
        "    sample_hypotheses: List[str]\n",
        "    confidence: float = 0.0\n",
        "\n",
        "@dataclass\n",
        "class ClusteringMetrics:\n",
        "    \"\"\"Metrics about the clustering process.\"\"\"\n",
        "    n_samples: int\n",
        "    n_clusters_requested: int\n",
        "    n_clusters_actual: int\n",
        "    clustering_method: str\n",
        "    silhouette_score: Optional[float] = None\n",
        "\n",
        "@dataclass\n",
        "class AdvancedMetrics:\n",
        "    \"\"\"Extended metrics for deeper analysis.\"\"\"\n",
        "    diversity_score: float\n",
        "    consensus_score: float\n",
        "    trend_analysis: Dict[str, float]\n",
        "    outlier_detection: List[str]\n",
        "    confidence_intervals: Dict[str, Tuple[float, float]]\n",
        "\n",
        "@dataclass\n",
        "class MetaReviewState:\n",
        "    \"\"\"Complete state of meta-review analysis.\"\"\"\n",
        "    pattern_insights: List[PatternInsight]\n",
        "    criterion_correlations: Dict[str, float]\n",
        "    actionable_for_generation: str\n",
        "    actionable_for_reflection: str\n",
        "    clustering_metrics: ClusteringMetrics\n",
        "    created_at: str\n",
        "    analysis_quality: str = \"high\"\n",
        "    advanced_metrics: Optional[AdvancedMetrics] = None\n",
        "\n",
        "@dataclass\n",
        "class MetaReviewConfig:\n",
        "    \"\"\"Configuration for meta-review agent.\"\"\"\n",
        "    max_clusters: int = 6\n",
        "    min_cluster_size: int = 2\n",
        "    cache_enabled: bool = True\n",
        "    async_processing: bool = False\n",
        "    enable_advanced_metrics: bool = True\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, config_dict: dict) -> 'MetaReviewConfig':\n",
        "        return cls(**{k: v for k, v in config_dict.items() if k in cls.__annotations__})\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return asdict(self)\n",
        "\n",
        "# ===========================================================================\n",
        "# 2️⃣ ENHANCED TEXT PREPROCESSING\n",
        "# ===========================================================================\n",
        "\n",
        "def _preprocess_text_robust(text: str) -> str:\n",
        "    \"\"\"Enhanced text preprocessing for better clustering.\"\"\"\n",
        "    import re\n",
        "\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Remove special characters but keep spaces\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove very short words (less than 3 characters)\n",
        "    words = [word for word in text.split() if len(word) >= 3]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def _extract_text_features(corpus: List[str]) -> List[str]:\n",
        "    \"\"\"Extract meaningful features from text corpus.\"\"\"\n",
        "    if not corpus:\n",
        "        return []\n",
        "\n",
        "    # Preprocess all texts\n",
        "    processed_corpus = [_preprocess_text_robust(text) for text in corpus]\n",
        "\n",
        "    # Remove empty texts\n",
        "    processed_corpus = [text for text in processed_corpus if text.strip()]\n",
        "\n",
        "    if not processed_corpus:\n",
        "        return []\n",
        "\n",
        "    # Extract all unique words\n",
        "    all_words = set()\n",
        "    for text in processed_corpus:\n",
        "        all_words.update(text.split())\n",
        "\n",
        "    # Filter out very common and very rare words manually\n",
        "    word_counts = Counter()\n",
        "    for text in processed_corpus:\n",
        "        word_counts.update(text.split())\n",
        "\n",
        "    # Keep words that appear in at least 1 document but not in all documents\n",
        "    n_docs = len(processed_corpus)\n",
        "    meaningful_words = [\n",
        "        word for word, count in word_counts.items()\n",
        "        if 1 <= count <= max(1, n_docs - 1) and len(word) >= 3\n",
        "    ]\n",
        "\n",
        "    return processed_corpus if meaningful_words else processed_corpus\n",
        "\n",
        "# ===========================================================================\n",
        "# 3️⃣ ROBUST CLUSTERING IMPLEMENTATION\n",
        "# ===========================================================================\n",
        "\n",
        "def _improved_text_clustering(corpus: List[str], n_clusters: int) -> Tuple[Dict[int, List[int]], ClusteringMetrics]:\n",
        "    \"\"\"Improved text clustering with better parameter handling.\"\"\"\n",
        "\n",
        "    if not corpus:\n",
        "        return {}, ClusteringMetrics(0, 0, 0, \"no_data\")\n",
        "\n",
        "    if len(corpus) == 1:\n",
        "        return {0: [0]}, ClusteringMetrics(1, 1, 1, \"single_sample\")\n",
        "\n",
        "    # Preprocess texts\n",
        "    processed_texts = _extract_text_features(corpus)\n",
        "\n",
        "    if not processed_texts or all(not text.strip() for text in processed_texts):\n",
        "        logger.warning(\"No meaningful text content found for clustering\")\n",
        "        return _simple_grouping_fallback(len(corpus), n_clusters)\n",
        "\n",
        "    try:\n",
        "        # Try sklearn clustering with improved parameters\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.cluster import AgglomerativeClustering\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "        # FIXED: More robust TF-IDF parameters\n",
        "        vectorizer = TfidfVectorizer(\n",
        "            max_features=min(100, len(processed_texts) * 5),  # Reduced max features\n",
        "            stop_words=\"english\",\n",
        "            min_df=1,  # Keep all terms that appear at least once\n",
        "            max_df=1.0,  # Keep all terms (no upper limit)\n",
        "            ngram_range=(1, 2),  # Include bigrams for better features\n",
        "            lowercase=True,\n",
        "            token_pattern=r'\\b[a-zA-Z]{3,}\\b'  # Only words with 3+ letters\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            X = vectorizer.fit_transform(processed_texts)\n",
        "            logger.info(f\"TF-IDF vectorization successful: {X.shape[0]} docs, {X.shape[1]} features\")\n",
        "\n",
        "            if X.shape[1] == 0:\n",
        "                raise ValueError(\"No features extracted after vectorization\")\n",
        "\n",
        "            # Adjust cluster count for small datasets\n",
        "            actual_n_clusters = min(n_clusters, len(processed_texts) - 1, max(1, len(processed_texts) // 2))\n",
        "\n",
        "            if actual_n_clusters < 2:\n",
        "                logger.info(\"Too few samples for meaningful clustering, using simple grouping\")\n",
        "                return _simple_grouping_fallback(len(corpus), n_clusters)\n",
        "\n",
        "            # Perform clustering\n",
        "            clustering = AgglomerativeClustering(\n",
        "                n_clusters=actual_n_clusters,\n",
        "                metric='cosine',\n",
        "                linkage='average'\n",
        "            )\n",
        "\n",
        "            X_dense = X.toarray()\n",
        "            labels = clustering.fit_predict(X_dense)\n",
        "\n",
        "            # Organize results\n",
        "            clusters = {}\n",
        "            for idx, label in enumerate(labels):\n",
        "                clusters.setdefault(int(label), []).append(idx)\n",
        "\n",
        "            # Calculate silhouette score safely\n",
        "            silhouette_score = None\n",
        "            try:\n",
        "                if len(set(labels)) > 1 and len(processed_texts) > 2:\n",
        "                    from sklearn.metrics import silhouette_score as calc_silhouette\n",
        "                    silhouette_score = float(calc_silhouette(X_dense, labels, metric='cosine'))\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"Silhouette score calculation failed: {e}\")\n",
        "\n",
        "            metrics = ClusteringMetrics(\n",
        "                n_samples=len(corpus),\n",
        "                n_clusters_requested=n_clusters,\n",
        "                n_clusters_actual=len(clusters),\n",
        "                clustering_method=\"sklearn_agglomerative_improved\",\n",
        "                silhouette_score=silhouette_score\n",
        "            )\n",
        "\n",
        "            logger.info(f\"Sklearn clustering successful: {len(clusters)} clusters created\")\n",
        "            return clusters, metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"TF-IDF vectorization failed: {e}\")\n",
        "            raise e\n",
        "\n",
        "    except ImportError:\n",
        "        logger.info(\"Sklearn not available, using simple clustering\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Advanced clustering failed: {e}, using simple clustering\")\n",
        "\n",
        "    # Enhanced fallback clustering\n",
        "    return _enhanced_simple_clustering(processed_texts, n_clusters)\n",
        "\n",
        "def _enhanced_simple_clustering(texts: List[str], n_clusters: int) -> Tuple[Dict[int, List[int]], ClusteringMetrics]:\n",
        "    \"\"\"Enhanced simple clustering based on text similarity.\"\"\"\n",
        "\n",
        "    if not texts:\n",
        "        return {}, ClusteringMetrics(0, 0, 0, \"no_data\")\n",
        "\n",
        "    # Simple word-based similarity clustering\n",
        "    clusters = {}\n",
        "    text_features = []\n",
        "\n",
        "    # Extract word sets for each text\n",
        "    for text in texts:\n",
        "        words = set(text.lower().split()) if text else set()\n",
        "        text_features.append(words)\n",
        "\n",
        "    # Group texts with similar word overlap\n",
        "    assigned = [False] * len(texts)\n",
        "    cluster_id = 0\n",
        "\n",
        "    for i in range(len(texts)):\n",
        "        if assigned[i]:\n",
        "            continue\n",
        "\n",
        "        current_cluster = [i]\n",
        "        assigned[i] = True\n",
        "\n",
        "        # Find similar texts\n",
        "        for j in range(i + 1, len(texts)):\n",
        "            if assigned[j]:\n",
        "                continue\n",
        "\n",
        "            # Calculate Jaccard similarity\n",
        "            intersection = len(text_features[i].intersection(text_features[j]))\n",
        "            union = len(text_features[i].union(text_features[j]))\n",
        "\n",
        "            if union > 0:\n",
        "                similarity = intersection / union\n",
        "                if similarity > 0.2:  # Threshold for grouping\n",
        "                    current_cluster.append(j)\n",
        "                    assigned[j] = True\n",
        "\n",
        "        clusters[cluster_id] = current_cluster\n",
        "        cluster_id += 1\n",
        "\n",
        "        # Limit number of clusters\n",
        "        if cluster_id >= n_clusters:\n",
        "            # Assign remaining texts to existing clusters\n",
        "            for k in range(len(texts)):\n",
        "                if not assigned[k]:\n",
        "                    # Assign to closest cluster (first one for simplicity)\n",
        "                    clusters[0].append(k)\n",
        "                    assigned[k] = True\n",
        "            break\n",
        "\n",
        "    metrics = ClusteringMetrics(\n",
        "        n_samples=len(texts),\n",
        "        n_clusters_requested=n_clusters,\n",
        "        n_clusters_actual=len(clusters),\n",
        "        clustering_method=\"enhanced_simple_similarity\"\n",
        "    )\n",
        "\n",
        "    return clusters, metrics\n",
        "\n",
        "def _simple_grouping_fallback(n_samples: int, n_groups: int) -> Tuple[Dict[int, List[int]], ClusteringMetrics]:\n",
        "    \"\"\"Simple fallback grouping when clustering fails.\"\"\"\n",
        "    groups = {}\n",
        "    group_size = max(1, n_samples // n_groups)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        group_id = min(i // group_size, n_groups - 1)\n",
        "        groups.setdefault(group_id, []).append(i)\n",
        "\n",
        "    metrics = ClusteringMetrics(\n",
        "        n_samples=n_samples,\n",
        "        n_clusters_requested=n_groups,\n",
        "        n_clusters_actual=len(groups),\n",
        "        clustering_method=\"simple_fallback\"\n",
        "    )\n",
        "\n",
        "    return groups, metrics\n",
        "\n",
        "# ===========================================================================\n",
        "# 4️⃣ ROBUST CORRELATION CALCULATION\n",
        "# ===========================================================================\n",
        "\n",
        "def _compute_correlations_robust(reflection_state, ranking_state) -> Dict[str, float]:\n",
        "    \"\"\"FIXED: Robust correlation computation with proper variance handling.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Get ranking information\n",
        "        rank_map = {}\n",
        "\n",
        "        for attr_name in ['final_rankings', 'leaderboard', 'rankings']:\n",
        "            rankings_data = getattr(ranking_state, attr_name, None)\n",
        "            if rankings_data:\n",
        "                if isinstance(rankings_data, list):\n",
        "                    for i, item in enumerate(rankings_data):\n",
        "                        if isinstance(item, dict):\n",
        "                            hyp_id = item.get('hypothesis_id', f'hyp_{i}')\n",
        "                        else:\n",
        "                            hyp_id = getattr(item, 'hypothesis_id', f'hyp_{i}')\n",
        "                        rank_map[hyp_id] = i + 1\n",
        "                break\n",
        "\n",
        "        if not rank_map:\n",
        "            logger.warning(\"No ranking data available\")\n",
        "            return {}\n",
        "\n",
        "        # Get review data\n",
        "        reviews = getattr(reflection_state, 'hypothesis_reviews', [])\n",
        "        if not reviews:\n",
        "            return {}\n",
        "\n",
        "        # Compute correlations for each criterion\n",
        "        criteria = [\"novelty_score\", \"feasibility_score\", \"scientific_rigor_score\",\n",
        "                   \"impact_potential_score\", \"testability_score\"]\n",
        "        correlations = {}\n",
        "\n",
        "        for criterion in criteria:\n",
        "            criterion_scores = []\n",
        "            rank_scores = []\n",
        "\n",
        "            for review in reviews:\n",
        "                hyp_id = getattr(review, 'hypothesis_id', '')\n",
        "                if hyp_id in rank_map:\n",
        "                    criteria_obj = getattr(review, 'criteria', None)\n",
        "                    if criteria_obj:\n",
        "                        score = getattr(criteria_obj, criterion, None)\n",
        "                        if score is not None:\n",
        "                            try:\n",
        "                                criterion_scores.append(float(score))\n",
        "                                # Convert rank to score (lower rank = higher score)\n",
        "                                rank_scores.append(-rank_map[hyp_id])\n",
        "                            except (ValueError, TypeError):\n",
        "                                continue\n",
        "\n",
        "            # FIXED: Check for sufficient variance before correlation calculation\n",
        "            if len(criterion_scores) >= 2:\n",
        "                try:\n",
        "                    # Check if there's variance in the data\n",
        "                    criterion_var = np.var(criterion_scores)\n",
        "                    rank_var = np.var(rank_scores)\n",
        "\n",
        "                    if criterion_var == 0 or rank_var == 0:\n",
        "                        # No variance - set correlation to 0\n",
        "                        correlations[criterion] = 0.0\n",
        "                        logger.debug(f\"No variance in {criterion}, correlation set to 0\")\n",
        "                    else:\n",
        "                        # Calculate correlation\n",
        "                        corr_matrix = np.corrcoef(criterion_scores, rank_scores)\n",
        "                        correlation = corr_matrix[0, 1]\n",
        "\n",
        "                        if np.isnan(correlation) or np.isinf(correlation):\n",
        "                            correlations[criterion] = 0.0\n",
        "                        else:\n",
        "                            correlations[criterion] = float(correlation)\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.debug(f\"Correlation calculation failed for {criterion}: {e}\")\n",
        "                    correlations[criterion] = 0.0\n",
        "            else:\n",
        "                correlations[criterion] = 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error computing correlations: {e}\")\n",
        "        return {}\n",
        "\n",
        "    return correlations\n",
        "\n",
        "# ===========================================================================\n",
        "# 5️⃣ HELPER FUNCTIONS (Improved)\n",
        "# ===========================================================================\n",
        "\n",
        "def _safe_get_attribute(obj, attr_path: str, default=None):\n",
        "    \"\"\"Safely get nested attributes from objects.\"\"\"\n",
        "    try:\n",
        "        attrs = attr_path.split('.')\n",
        "        current = obj\n",
        "        for attr in attrs:\n",
        "            if hasattr(current, attr):\n",
        "                current = getattr(current, attr)\n",
        "            elif isinstance(current, dict):\n",
        "                current = current.get(attr)\n",
        "            else:\n",
        "                return default\n",
        "        return current\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "def _extract_critique_corpus_robust(reflection_state) -> List[str]:\n",
        "    \"\"\"Extract text corpus with better error handling.\"\"\"\n",
        "    corpus = []\n",
        "\n",
        "    try:\n",
        "        reviews = None\n",
        "        for attr_name in ['hypothesis_reviews', 'reviews', 'hypothesis_evaluations']:\n",
        "            reviews = _safe_get_attribute(reflection_state, attr_name, [])\n",
        "            if reviews:\n",
        "                break\n",
        "\n",
        "        if not reviews:\n",
        "            logger.warning(\"No hypothesis reviews found\")\n",
        "            return []\n",
        "\n",
        "        for review in reviews:\n",
        "            texts = []\n",
        "\n",
        "            # Extract criteria reasoning\n",
        "            criteria = _safe_get_attribute(review, 'criteria')\n",
        "            if criteria:\n",
        "                reasoning_fields = [\n",
        "                    'novelty_reasoning', 'feasibility_reasoning',\n",
        "                    'scientific_rigor_reasoning', 'impact_potential_reasoning',\n",
        "                    'testability_reasoning'\n",
        "                ]\n",
        "                for field in reasoning_fields:\n",
        "                    text = _safe_get_attribute(criteria, field, '')\n",
        "                    if text and str(text).strip() and str(text) != 'stub':\n",
        "                        texts.append(str(text))\n",
        "\n",
        "            # Extract other text fields\n",
        "            for attr in ['strengths', 'weaknesses', 'recommendations']:\n",
        "                items = _safe_get_attribute(review, attr, [])\n",
        "                if items:\n",
        "                    texts.extend([str(item) for item in items if str(item)])\n",
        "\n",
        "            # Extract overall assessment\n",
        "            overall = _safe_get_attribute(review, 'overall_assessment', '')\n",
        "            if overall and str(overall).strip():\n",
        "                texts.append(str(overall))\n",
        "\n",
        "            # Combine texts\n",
        "            combined_text = \" \".join(filter(None, texts))\n",
        "            if combined_text and len(combined_text.strip()) > 10:\n",
        "                corpus.append(combined_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting corpus: {e}\")\n",
        "\n",
        "    logger.info(f\"Extracted corpus with {len(corpus)} documents\")\n",
        "    return corpus\n",
        "\n",
        "def _summarize_clusters_robust(clusters: Dict[int, List[int]],\n",
        "                              reflection_state,\n",
        "                              corpus: List[str]) -> List[PatternInsight]:\n",
        "    \"\"\"Improved cluster summarization.\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    try:\n",
        "        for cluster_id, indices in clusters.items():\n",
        "            if not indices:\n",
        "                continue\n",
        "\n",
        "            # Get cluster texts\n",
        "            cluster_texts = []\n",
        "            for i in indices:\n",
        "                if i < len(corpus):\n",
        "                    cluster_texts.append(corpus[i])\n",
        "\n",
        "            if not cluster_texts:\n",
        "                continue\n",
        "\n",
        "            # Extract meaningful words\n",
        "            all_words = []\n",
        "            for text in cluster_texts:\n",
        "                # Better word extraction\n",
        "                words = [word.lower().strip() for word in str(text).split()\n",
        "                        if len(word) > 3 and word.isalpha() and word not in\n",
        "                        ['that', 'this', 'with', 'from', 'they', 'were', 'been', 'have']]\n",
        "                all_words.extend(words)\n",
        "\n",
        "            word_counts = Counter(all_words)\n",
        "            # Only keep words that appear more than once in the cluster\n",
        "            top_words = [word for word, count in word_counts.most_common(10)\n",
        "                        if count > 1 or len(cluster_texts) == 1]\n",
        "\n",
        "            # Create meaningful description\n",
        "            if top_words:\n",
        "                pattern_desc = f\"Cluster {cluster_id + 1}: Themes - {', '.join(top_words[:5])}\"\n",
        "            else:\n",
        "                pattern_desc = f\"Cluster {cluster_id + 1}: Unique patterns\"\n",
        "\n",
        "            # Get sample hypothesis IDs\n",
        "            sample_hypotheses = []\n",
        "            try:\n",
        "                reviews = _safe_get_attribute(reflection_state, 'hypothesis_reviews', [])\n",
        "                for i in indices[:3]:\n",
        "                    if i < len(reviews):\n",
        "                        hyp_id = _safe_get_attribute(reviews[i], 'hypothesis_id', f'hyp_{i}')\n",
        "                        sample_hypotheses.append(str(hyp_id))\n",
        "            except Exception:\n",
        "                sample_hypotheses = [f\"sample_{i}\" for i in indices[:3]]\n",
        "\n",
        "            # Calculate confidence based on cluster coherence\n",
        "            confidence = min(0.9,\n",
        "                           len(indices) / max(1, len(clusters)) * 0.4 +  # Size factor\n",
        "                           len(top_words) / 10 * 0.4 +  # Word overlap factor\n",
        "                           (1.0 if len(cluster_texts) > 1 else 0.8) * 0.2)  # Multiple texts factor\n",
        "\n",
        "            insights.append(PatternInsight(\n",
        "                description=pattern_desc[:200],\n",
        "                frequency=len(indices),\n",
        "                sample_hypotheses=sample_hypotheses,\n",
        "                confidence=confidence\n",
        "            ))\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in cluster summarization: {e}\")\n",
        "\n",
        "    # Sort by frequency and confidence\n",
        "    insights.sort(key=lambda x: (x.frequency, x.confidence), reverse=True)\n",
        "    return insights\n",
        "\n",
        "def compute_advanced_metrics_robust(reflection_state, pattern_insights, correlations) -> AdvancedMetrics:\n",
        "    \"\"\"Compute advanced metrics with better error handling.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Diversity score\n",
        "        cluster_sizes = [insight.frequency for insight in pattern_insights]\n",
        "        if cluster_sizes and sum(cluster_sizes) > 0:\n",
        "            diversity_score = 1 - (max(cluster_sizes) / sum(cluster_sizes))\n",
        "        else:\n",
        "            diversity_score = 0.0\n",
        "\n",
        "        # Consensus score (use absolute values and handle NaN)\n",
        "        if correlations:\n",
        "            valid_correlations = [abs(corr) for corr in correlations.values()\n",
        "                                if not (np.isnan(corr) or np.isinf(corr))]\n",
        "            consensus_score = np.mean(valid_correlations) if valid_correlations else 0.0\n",
        "        else:\n",
        "            consensus_score = 0.0\n",
        "\n",
        "        # Confidence intervals with proper error handling\n",
        "        confidence_intervals = {}\n",
        "        for criterion, corr in correlations.items():\n",
        "            if not (np.isnan(corr) or np.isinf(corr)):\n",
        "                se = 0.15  # Conservative standard error\n",
        "                ci_lower = max(-1, corr - 1.96 * se)\n",
        "                ci_upper = min(1, corr + 1.96 * se)\n",
        "                confidence_intervals[criterion] = (ci_lower, ci_upper)\n",
        "            else:\n",
        "                confidence_intervals[criterion] = (-0.15, 0.15)  # Default uncertainty\n",
        "\n",
        "        return AdvancedMetrics(\n",
        "            diversity_score=diversity_score,\n",
        "            consensus_score=consensus_score,\n",
        "            # FIX: Removed invalid escape character\n",
        "            trend_analysis={\"improving\": 0.0, \"declining\": 0.0, \"stable\": 1.0},\n",
        "            outlier_detection=[],\n",
        "            confidence_intervals=confidence_intervals\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error computing advanced metrics: {e}\")\n",
        "        return AdvancedMetrics(\n",
        "            diversity_score=0.0,\n",
        "            consensus_score=0.0,\n",
        "            # FIX: Removed invalid escape character\n",
        "            trend_analysis={\"improving\": 0.0, \"declining\": 0.0, \"stable\": 1.0},\n",
        "            outlier_detection=[],\n",
        "            confidence_intervals={}\n",
        "        )\n",
        "\n",
        "# ===========================================================================\n",
        "# 6️⃣ MAIN ENHANCED FUNCTION\n",
        "# ===========================================================================\n",
        "\n",
        "def run_enhanced_meta_review(reflection_state,\n",
        "                           ranking_state,\n",
        "                           config: MetaReviewConfig = None) -> MetaReviewState:\n",
        "    \"\"\"Enhanced meta-review with all fixes applied.\"\"\"\n",
        "\n",
        "    config = config or MetaReviewConfig()\n",
        "\n",
        "    logger.info(\"Starting enhanced meta-review analysis (v2)\")\n",
        "\n",
        "    try:\n",
        "        # Extract and process corpus\n",
        "        corpus = _extract_critique_corpus_robust(reflection_state)\n",
        "\n",
        "        # Perform improved clustering\n",
        "        clusters, metrics = _improved_text_clustering(corpus, config.max_clusters)\n",
        "\n",
        "        # Summarize patterns\n",
        "        insights = _summarize_clusters_robust(clusters, reflection_state, corpus)\n",
        "\n",
        "        # Compute robust correlations\n",
        "        correlations = _compute_correlations_robust(reflection_state, ranking_state)\n",
        "\n",
        "        # Generate feedback\n",
        "        feedback_gen = f\"Analysis complete: {len(insights)} patterns identified from {len(corpus)} documents\"\n",
        "        feedback_ref = f\"Correlation analysis: {len(correlations)} criteria evaluated, mean correlation strength: {np.mean([abs(c) for c in correlations.values()]) if correlations else 0:.3f}\"\n",
        "\n",
        "        # Compute advanced metrics\n",
        "        advanced_metrics = None\n",
        "        if config.enable_advanced_metrics:\n",
        "            advanced_metrics = compute_advanced_metrics_robust(reflection_state, insights, correlations)\n",
        "\n",
        "        # Assess quality\n",
        "        quality = \"high\"\n",
        "        if metrics.clustering_method.endswith(\"fallback\"):\n",
        "            quality = \"medium\"\n",
        "        elif metrics.n_samples < 3:\n",
        "            quality = \"low\"\n",
        "        elif not insights and not correlations:\n",
        "            quality = \"low\"\n",
        "\n",
        "        result = MetaReviewState(\n",
        "            pattern_insights=insights,\n",
        "            criterion_correlations=correlations,\n",
        "            actionable_for_generation=feedback_gen,\n",
        "            actionable_for_reflection=feedback_ref,\n",
        "            clustering_metrics=metrics,\n",
        "            # FIX: Corrected datetime call\n",
        "            created_at=datetime.datetime.now().isoformat(),\n",
        "            analysis_quality=quality,\n",
        "            advanced_metrics=advanced_metrics\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Enhanced meta-review completed successfully: {quality} quality\")\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Enhanced meta-review failed: {e}\")\n",
        "\n",
        "        return MetaReviewState(\n",
        "            pattern_insights=[],\n",
        "            criterion_correlations={},\n",
        "            actionable_for_generation=\"Meta-review analysis failed. Manual review recommended.\",\n",
        "            actionable_for_reflection=\"Meta-review analysis failed. Continue with current rubric.\",\n",
        "            clustering_metrics=ClusteringMetrics(0, 0, 0, \"failed\"),\n",
        "            # FIX: Corrected datetime call\n",
        "            created_at=datetime.datetime.now().isoformat(),\n",
        "            analysis_quality=\"failed\"\n",
        "        )\n",
        "\n",
        "# ===========================================================================\n",
        "# 7️⃣ VISUALIZATION AND TESTING\n",
        "# ===========================================================================\n",
        "\n",
        "def generate_analysis_report(meta_review_state: MetaReviewState) -> dict:\n",
        "    \"\"\"Generate comprehensive analysis report.\"\"\"\n",
        "\n",
        "    def safe_format(value, default=\"N/A\"):\n",
        "        try:\n",
        "            if isinstance(value, (int, float)):\n",
        "                return f\"{value:.3f}\" if isinstance(value, float) else str(value)\n",
        "            return str(value) if value is not None else default\n",
        "        except:\n",
        "            return default\n",
        "\n",
        "    return {\n",
        "        \"summary\": {\n",
        "            \"analysis_quality\": meta_review_state.analysis_quality,\n",
        "            \"patterns_found\": len(meta_review_state.pattern_insights),\n",
        "            \"clustering_method\": meta_review_state.clustering_metrics.clustering_method,\n",
        "            \"sample_size\": meta_review_state.clustering_metrics.n_samples\n",
        "        },\n",
        "        \"patterns\": [\n",
        "            {\n",
        "                \"description\": insight.description,\n",
        "                \"frequency\": insight.frequency,\n",
        "                \"confidence\": safe_format(insight.confidence),\n",
        "                \"sample_hypotheses\": insight.sample_hypotheses[:3]\n",
        "            }\n",
        "            for insight in meta_review_state.pattern_insights\n",
        "        ],\n",
        "        \"correlations\": {\n",
        "            criterion: safe_format(corr)\n",
        "            for criterion, corr in meta_review_state.criterion_correlations.items()\n",
        "        },\n",
        "        \"advanced_metrics\": {\n",
        "            \"diversity_score\": safe_format(meta_review_state.advanced_metrics.diversity_score) if meta_review_state.advanced_metrics else \"N/A\",\n",
        "            \"consensus_score\": safe_format(meta_review_state.advanced_metrics.consensus_score) if meta_review_state.advanced_metrics else \"N/A\"\n",
        "        } if meta_review_state.advanced_metrics else {},\n",
        "        \"recommendations\": {\n",
        "            \"for_generation\": meta_review_state.actionable_for_generation,\n",
        "            \"for_reflection\": meta_review_state.actionable_for_reflection\n",
        "        }\n",
        "    }\n",
        "\n",
        "def test_enhanced_meta_review_v2():\n",
        "    \"\"\"Test the fixed enhanced meta-review agent.\"\"\"\n",
        "    print(\"🧠 TESTING ENHANCED META-REVIEW AGENT (v2 - FIXED)\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    # Create more diverse mock data to test clustering\n",
        "    class MockCriteria:\n",
        "        def __init__(self, variant=0):\n",
        "            base_scores = [8.0, 7.0, 8.5, 9.0, 7.5]\n",
        "            variations = [0, 0.5, -0.5, 1.0, -1.0]\n",
        "\n",
        "            self.novelty_reasoning = [\"Novel approach\", \"Innovative method\", \"Creative solution\", \"Unique perspective\", \"Original idea\"][variant % 5]\n",
        "            self.feasibility_reasoning = [\"Technically feasible\", \"Practically achievable\", \"Resource efficient\", \"Implementation ready\", \"Scalable approach\"][variant % 5]\n",
        "            self.scientific_rigor_reasoning = [\"Well-grounded\", \"Methodologically sound\", \"Evidence-based\", \"Statistically robust\", \"Peer-reviewed\"][variant % 5]\n",
        "            self.impact_potential_reasoning = [\"High impact\", \"Transformative potential\", \"Significant benefit\", \"Wide applicability\", \"Game-changing\"][variant % 5]\n",
        "            self.testability_reasoning = [\"Easily testable\", \"Measurable outcomes\", \"Clear metrics\", \"Validation ready\", \"Hypothesis driven\"][variant % 5]\n",
        "\n",
        "            # Add some variation to scores\n",
        "            self.novelty_score = max(1, min(10, base_scores[0] + variations[variant % 5]))\n",
        "            self.feasibility_score = max(1, min(10, base_scores[1] + variations[variant % 5]))\n",
        "            self.scientific_rigor_score = max(1, min(10, base_scores[2] + variations[variant % 5]))\n",
        "            self.impact_potential_score = max(1, min(10, base_scores[3] + variations[variant % 5]))\n",
        "            self.testability_score = max(1, min(10, base_scores[4] + variations[variant % 5]))\n",
        "\n",
        "    class MockReview:\n",
        "        def __init__(self, i):\n",
        "            self.hypothesis_id = f'hyp_{i}'\n",
        "            self.hypothesis_text = f'Test hypothesis {i} with specific focus'\n",
        "            self.criteria = MockCriteria(i)\n",
        "\n",
        "            # Create more diverse content for better clustering\n",
        "            themes = [\n",
        "                ['Novel approach', 'High impact', 'Machine learning'],\n",
        "                ['Innovative method', 'Clinical application', 'Drug discovery'],\n",
        "                ['Creative solution', 'Biomarker development', 'Personalized medicine'],\n",
        "                ['Unique perspective', 'Therapeutic target', 'Precision oncology'],\n",
        "                ['Original idea', 'Diagnostic tool', 'Computational biology']\n",
        "            ]\n",
        "\n",
        "            theme = themes[i % len(themes)]\n",
        "            self.strengths = theme[:2]\n",
        "            self.weaknesses = ['Needs validation', 'Resource intensive'][i % 2:i % 2 + 1]\n",
        "            self.overall_assessment = f'Good hypothesis with {theme[2]} focus'\n",
        "\n",
        "    class MockReflectionState:\n",
        "        def __init__(self):\n",
        "            self.hypothesis_reviews = [MockReview(i) for i in range(7)]  # More samples\n",
        "\n",
        "    class MockRankingState:\n",
        "        def __init__(self):\n",
        "            # Create more realistic ranking with some ties\n",
        "            ranks = [1, 2, 3, 3, 4, 5, 6]  # Note: rank 3 appears twice\n",
        "            self.final_rankings = [\n",
        "                {'hypothesis_id': f'hyp_{i}', 'rank': ranks[i]} for i in range(7)\n",
        "            ]\n",
        "\n",
        "    # Run test\n",
        "    reflection_state = MockReflectionState()\n",
        "    ranking_state = MockRankingState()\n",
        "\n",
        "    config = MetaReviewConfig(\n",
        "        max_clusters=4,\n",
        "        cache_enabled=False,\n",
        "        async_processing=False,\n",
        "        enable_advanced_metrics=True\n",
        "    )\n",
        "\n",
        "    result = run_enhanced_meta_review(reflection_state, ranking_state, config)\n",
        "\n",
        "    # Generate report\n",
        "    report = generate_analysis_report(result)\n",
        "\n",
        "    print(f\"✅ Analysis Quality: {result.analysis_quality}\")\n",
        "    print(f\"📊 Patterns Found: {len(result.pattern_insights)}\")\n",
        "    print(f\"🔗 Correlations: {len(result.criterion_correlations)}\")\n",
        "    print(f\"🏷️ Clustering Method: {result.clustering_metrics.clustering_method}\")\n",
        "\n",
        "    if result.criterion_correlations:\n",
        "        print(f\"📈 Sample Correlations:\")\n",
        "        for criterion, corr in list(result.criterion_correlations.items())[:3]:\n",
        "            print(f\"   • {criterion}: {corr:.3f}\")\n",
        "\n",
        "    if result.advanced_metrics:\n",
        "        print(f\"📊 Diversity Score: {result.advanced_metrics.diversity_score:.3f}\")\n",
        "        print(f\"🤝 Consensus Score: {result.advanced_metrics.consensus_score:.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 55)\n",
        "    print(\"🎉 ENHANCED META-REVIEW AGENT (v2) TEST COMPLETED!\")\n",
        "    print(\"🛠️  Issues fixed: TF-IDF parameters, correlation calculation, clustering robustness\")\n",
        "\n",
        "    return result, report\n",
        "\n",
        "# Run the improved test\n",
        "if __name__ == \"__main__\":\n",
        "    test_result_v2, test_report_v2 = test_enhanced_meta_review_v2()\n",
        "\n",
        "    if test_result_v2.pattern_insights:\n",
        "        print(f\"\\n📋 Sample Pattern: {test_result_v2.pattern_insights[0].description}\")\n",
        "\n",
        "    print(f\"\\n📊 Report Summary:\")\n",
        "    print(f\"   Clustering: {test_report_v2['summary']['clustering_method']}\")\n",
        "    print(f\"   Quality: {test_report_v2['summary']['analysis_quality']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-fRTCXrs-XR",
        "outputId": "7b7c7c3b-7fcf-4b2e-e25c-3cfecc3b66a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌐 TESTING PROXIMITY AGENT\n",
            "==================================================\n",
            "📋 Initial state: 6 hypotheses.\n",
            "  - [hyp_a1] Targeting protein kinase C alpha will re...\n",
            "  - [hyp_a2] Inhibiting the PKC-alpha enzyme can alle...\n",
            "  - [hyp_b1] Using gut microbiome modulation is a via...\n",
            "  - [hyp_c1] A novel nanoparticle delivery system can...\n",
            "  - [hyp_a3] PKC-alpha inhibition is a key mechanism ...\n",
            "  - [hyp_b2] Altering gut bacteria composition could ...\n",
            "\n",
            "🚀 Running integrated proximity agent...\n",
            "\n",
            "📊 Proximity Analysis Results:\n",
            "  - Clusters found: 3\n",
            "  - Hypotheses removed: 3\n",
            "  - Unique hypotheses remaining: 3\n",
            "\n",
            "🔍 Cluster Breakdown:\n",
            "\n",
            "  Cluster with representative [hyp_a3]:\n",
            "    - Member IDs: hyp_a1, hyp_a2, hyp_a3\n",
            "    - Avg. Similarity: 0.86\n",
            "\n",
            "  Cluster with representative [hyp_b2]:\n",
            "    - Member IDs: hyp_b1, hyp_b2\n",
            "    - Avg. Similarity: 0.93\n",
            "\n",
            "  Cluster with representative [hyp_c1]:\n",
            "    - Member IDs: hyp_c1\n",
            "    - Avg. Similarity: 1.00\n",
            "\n",
            "✅ Final Deduplicated Hypotheses:\n",
            "  - [hyp_a3] PKC-alpha inhibition is a key mechanism for controlling neuroinflammatory responses in Alzheimer's.\n",
            "  - [hyp_b2] Altering gut bacteria composition could be a therapeutic method for slowing down Parkinson's.\n",
            "  - [hyp_c1] A novel nanoparticle delivery system can cross the blood-brain barrier to deliver drugs.\n",
            "\n",
            "==================================================\n",
            "🎉 ALL PROXIMITY AGENT TESTS COMPLETED SUCCESSFULLY!\n",
            "🌐 The Proximity Agent is ready for hypothesis deduplication.\n"
          ]
        }
      ],
      "source": [
        "#@title 🌐 Proximity Agent - Hypothesis Clustering & Deduplication\n",
        "# ===========================================================================\n",
        "# PROXIMITY AGENT IMPLEMENTATION\n",
        "# Hypothesis Clustering and Deduplication\n",
        "# ===========================================================================\n",
        "\n",
        "# Install required packages for sentence embeddings\n",
        "!pip install -q sentence-transformers scikit-learn\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass, field\n",
        "import datetime # FIX: Added the missing datetime import\n",
        "import logging\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ===========================================================================\n",
        "# DATA STRUCTURES\n",
        "# ===========================================================================\n",
        "\n",
        "@dataclass\n",
        "class HypothesisCluster:\n",
        "    \"\"\"Represents a cluster of similar hypotheses.\"\"\"\n",
        "    cluster_id: int\n",
        "    hypothesis_ids: List[str]\n",
        "    representative_id: str\n",
        "    representative_text: str\n",
        "    similarity_score: float  # Average intra-cluster similarity\n",
        "\n",
        "@dataclass\n",
        "class ProximityState:\n",
        "    \"\"\"Holds the results of the proximity analysis.\"\"\"\n",
        "    clusters: List[HypothesisCluster] = field(default_factory=list)\n",
        "    unique_hypotheses: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    removed_count: int = 0\n",
        "    analysis_timestamp: str = \"\"\n",
        "\n",
        "# ===========================================================================\n",
        "# PROXIMITY AGENT\n",
        "# ===========================================================================\n",
        "\n",
        "class ProximityAgent:\n",
        "    \"\"\"\n",
        "    Clusters similar hypotheses and removes duplicates to maintain idea diversity.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = 'all-MiniLM-L6-v2', distance_threshold: float = 0.2):\n",
        "        \"\"\"\n",
        "        Initializes the agent.\n",
        "\n",
        "        Args:\n",
        "            model_name: The sentence-transformer model to use for embeddings.\n",
        "            distance_threshold: The clustering distance. Lower values mean more clusters\n",
        "                                and stricter similarity for grouping. (0.2 is a good start)\n",
        "        \"\"\"\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        # The threshold for AgglomerativeClustering is based on distance (1 - similarity)\n",
        "        self.distance_threshold = distance_threshold\n",
        "        logger.info(f\"ProximityAgent initialized with model '{model_name}' and distance threshold {distance_threshold}\")\n",
        "\n",
        "    def run(self, proposals: List[Dict[str, Any]]) -> ProximityState:\n",
        "        \"\"\"\n",
        "        Runs the full clustering and deduplication process.\n",
        "\n",
        "        Args:\n",
        "            proposals: A list of hypothesis dictionaries from the GenerationAgent.\n",
        "                       Each dict must have 'id' and 'content' keys.\n",
        "\n",
        "        Returns:\n",
        "            A ProximityState object containing the results.\n",
        "        \"\"\"\n",
        "        if not proposals or len(proposals) < 2:\n",
        "            logger.warning(\"Not enough hypotheses to run proximity analysis. Skipping.\")\n",
        "            state = ProximityState(unique_hypotheses=proposals)\n",
        "            # FIX: Corrected datetime call\n",
        "            state.analysis_timestamp = datetime.datetime.now().isoformat()\n",
        "            return state\n",
        "\n",
        "        logger.info(f\"Running proximity analysis on {len(proposals)} hypotheses.\")\n",
        "\n",
        "        # 1. Embed all hypotheses\n",
        "        embeddings = self._embed_hypotheses(proposals)\n",
        "\n",
        "        # 2. Cluster the embeddings\n",
        "        cluster_labels = self._cluster_hypotheses(embeddings)\n",
        "\n",
        "        # 3. Organize hypotheses into clusters and select representatives\n",
        "        proximity_state = self._select_representatives(proposals, cluster_labels, embeddings)\n",
        "\n",
        "        # FIX: Corrected datetime call\n",
        "        proximity_state.analysis_timestamp = datetime.datetime.now().isoformat()\n",
        "        logger.info(f\"Proximity analysis complete. Found {len(proximity_state.clusters)} clusters. \"\n",
        "                    f\"Kept {len(proximity_state.unique_hypotheses)} unique hypotheses, removed {proximity_state.removed_count}.\")\n",
        "\n",
        "        return proximity_state\n",
        "\n",
        "    def _embed_hypotheses(self, proposals: List[Dict[str, Any]]) -> np.ndarray:\n",
        "        \"\"\"Converts hypothesis text into numerical embeddings.\"\"\"\n",
        "        texts = [p.get('content', '') for p in proposals]\n",
        "        return self.model.encode(texts, show_progress_bar=False)\n",
        "\n",
        "    def _cluster_hypotheses(self, embeddings: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Groups similar hypothesis embeddings together.\"\"\"\n",
        "        # We use distance_threshold, so n_clusters is None.\n",
        "        # Affinity is cosine distance (1-cosine_similarity)\n",
        "        clustering_model = AgglomerativeClustering(\n",
        "            n_clusters=None,\n",
        "            distance_threshold=self.distance_threshold,\n",
        "            metric='cosine',\n",
        "            linkage='average'\n",
        "        )\n",
        "        return clustering_model.fit_predict(embeddings)\n",
        "\n",
        "    def _select_representatives(self, proposals: List[Dict[str, Any]], labels: np.ndarray, embeddings: np.ndarray) -> ProximityState:\n",
        "        \"\"\"\n",
        "        For each cluster, selects the best representative and archives the others.\n",
        "        \"\"\"\n",
        "        state = ProximityState()\n",
        "\n",
        "        unique_cluster_labels = set(labels)\n",
        "\n",
        "        for label in unique_cluster_labels:\n",
        "            indices = [i for i, l in enumerate(labels) if l == label]\n",
        "\n",
        "            cluster_proposals = [proposals[i] for i in indices]\n",
        "            cluster_embeddings = embeddings[indices]\n",
        "\n",
        "            # Heuristic for selecting representative: the one closest to the cluster's mean embedding (centroid)\n",
        "            if len(cluster_proposals) > 1:\n",
        "                centroid = np.mean(cluster_embeddings, axis=0)\n",
        "                # Cosine similarity requires reshaping the centroid\n",
        "                similarities = cosine_similarity(cluster_embeddings, centroid.reshape(1, -1))\n",
        "                representative_index = np.argmax(similarities)\n",
        "                avg_similarity = np.mean(cosine_similarity(cluster_embeddings))\n",
        "            else:\n",
        "                representative_index = 0\n",
        "                avg_similarity = 1.0\n",
        "\n",
        "            representative_proposal = cluster_proposals[representative_index]\n",
        "            state.unique_hypotheses.append(representative_proposal)\n",
        "\n",
        "            cluster_info = HypothesisCluster(\n",
        "                cluster_id=int(label),\n",
        "                hypothesis_ids=[p['id'] for p in cluster_proposals],\n",
        "                representative_id=representative_proposal['id'],\n",
        "                representative_text=representative_proposal['content'],\n",
        "                similarity_score=float(avg_similarity)\n",
        "            )\n",
        "            state.clusters.append(cluster_info)\n",
        "\n",
        "        state.removed_count = len(proposals) - len(state.unique_hypotheses)\n",
        "        return state\n",
        "\n",
        "# ===========================================================================\n",
        "# INTEGRATION FUNCTION\n",
        "# ===========================================================================\n",
        "\n",
        "def run_proximity_agent(generation_state) -> Any:\n",
        "    \"\"\"\n",
        "    Integrates the Proximity Agent into the main workflow.\n",
        "\n",
        "    Args:\n",
        "        generation_state: The state object, which must have a `generated_proposals` attribute.\n",
        "\n",
        "    Returns:\n",
        "        The modified generation_state with deduplicated proposals.\n",
        "    \"\"\"\n",
        "    logger.info(\"🚀 Engaging Proximity Agent...\")\n",
        "\n",
        "    # Check for proposals in generation_state\n",
        "    # Using getattr for compatibility with both dataclasses and SimpleNamespace from stubs\n",
        "    proposals = getattr(generation_state, 'generated_proposals', [])\n",
        "    if not proposals:\n",
        "        logger.warning(\"No proposals found in generation_state for proximity analysis.\")\n",
        "        return generation_state\n",
        "\n",
        "    # Initialize and run the agent\n",
        "    proximity_agent = ProximityAgent(distance_threshold=0.25)\n",
        "    proximity_results = proximity_agent.run(proposals)\n",
        "\n",
        "    # Update the generation_state with the unique hypotheses\n",
        "    generation_state.generated_proposals = proximity_results.unique_hypotheses\n",
        "\n",
        "    # Store proximity results for logging/meta-review if needed\n",
        "    if not hasattr(generation_state, 'proximity_results'):\n",
        "        setattr(generation_state, 'proximity_results', [])\n",
        "    generation_state.proximity_results.append(proximity_results)\n",
        "\n",
        "    logger.info(f\"✅ Proximity Agent finished. {proximity_results.removed_count} redundant hypotheses removed.\")\n",
        "    return generation_state\n",
        "\n",
        "# ===========================================================================\n",
        "# TESTING CODE\n",
        "# ===========================================================================\n",
        "\n",
        "def test_proximity_agent():\n",
        "    \"\"\"Comprehensive test suite for the Proximity Agent.\"\"\"\n",
        "    print(\"🌐 TESTING PROXIMITY AGENT\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # 1. Create mock hypotheses with clear clusters and duplicates\n",
        "    mock_hypotheses = [\n",
        "        {\"id\": \"hyp_a1\", \"content\": \"Targeting protein kinase C alpha will reduce neuroinflammation in Alzheimer's disease.\"},\n",
        "        {\"id\": \"hyp_a2\", \"content\": \"Inhibiting the PKC-alpha enzyme can alleviate brain inflammation associated with Alzheimer's.\"},\n",
        "        {\"id\": \"hyp_b1\", \"content\": \"Using gut microbiome modulation is a viable strategy to slow Parkinson's disease progression.\"},\n",
        "        {\"id\": \"hyp_c1\", \"content\": \"A novel nanoparticle delivery system can cross the blood-brain barrier to deliver drugs.\"},\n",
        "        {\"id\": \"hyp_a3\", \"content\": \"PKC-alpha inhibition is a key mechanism for controlling neuroinflammatory responses in Alzheimer's.\"},\n",
        "        {\"id\": \"hyp_b2\", \"content\": \"Altering gut bacteria composition could be a therapeutic method for slowing down Parkinson's.\"}\n",
        "    ]\n",
        "\n",
        "    # Mock the main generation state object\n",
        "    class MockGenerationState:\n",
        "        def __init__(self, proposals):\n",
        "            self.generated_proposals = proposals\n",
        "            self.research_goal = \"Test Goal\"\n",
        "\n",
        "    mock_state = MockGenerationState(mock_hypotheses)\n",
        "\n",
        "    print(f\"📋 Initial state: {len(mock_state.generated_proposals)} hypotheses.\")\n",
        "    for h in mock_state.generated_proposals:\n",
        "        print(f\"  - [{h['id']}] {h['content'][:40]}...\")\n",
        "\n",
        "    # 2. Run the integrated agent function\n",
        "    print(\"\\n🚀 Running integrated proximity agent...\")\n",
        "    final_state = run_proximity_agent(mock_state)\n",
        "\n",
        "    # 3. Analyze and print the results\n",
        "    print(\"\\n📊 Proximity Analysis Results:\")\n",
        "\n",
        "    proximity_results = getattr(final_state, 'proximity_results', [ProximityState()])[0]\n",
        "\n",
        "    print(f\"  - Clusters found: {len(proximity_results.clusters)}\")\n",
        "    print(f\"  - Hypotheses removed: {proximity_results.removed_count}\")\n",
        "    print(f\"  - Unique hypotheses remaining: {len(proximity_results.unique_hypotheses)}\")\n",
        "\n",
        "    print(\"\\n🔍 Cluster Breakdown:\")\n",
        "    # Sort clusters for consistent output\n",
        "    sorted_clusters = sorted(proximity_results.clusters, key=lambda c: c.representative_id)\n",
        "    for cluster in sorted_clusters:\n",
        "        print(f\"\\n  Cluster with representative [{cluster.representative_id}]:\")\n",
        "        print(f\"    - Member IDs: {', '.join(cluster.hypothesis_ids)}\")\n",
        "        print(f\"    - Avg. Similarity: {cluster.similarity_score:.2f}\")\n",
        "\n",
        "    print(\"\\n✅ Final Deduplicated Hypotheses:\")\n",
        "    final_hypotheses = sorted(proximity_results.unique_hypotheses, key=lambda h: h['id'])\n",
        "    for h in final_hypotheses:\n",
        "        print(f\"  - [{h['id']}] {h['content']}\")\n",
        "\n",
        "    # Assertions\n",
        "    assert len(final_hypotheses) == 3\n",
        "    assert proximity_results.removed_count == 3\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"🎉 ALL PROXIMITY AGENT TESTS COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"🌐 The Proximity Agent is ready for hypothesis deduplication.\")\n",
        "\n",
        "    return final_state\n",
        "\n",
        "# Run the test\n",
        "if __name__ == \"__main__\":\n",
        "    proximity_test_results = test_proximity_agent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cA5nounnEvJ",
        "outputId": "da2a7d04-38ff-464f-f439-0beb923fcf1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__:No proposals found in generation_state for proximity analysis.\n",
            "WARNING:__main__:No proposals found in generation_state for proximity analysis.\n",
            "WARNING:__main__:No proposals found in generation_state for proximity analysis.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ℹ️  Creating stub for missing agent: run_generation_agent\n",
            "ℹ️  Creating stub for missing agent: run_reflection_agent\n",
            "ℹ️  Creating stub for missing agent: run_ranking_agent\n",
            "ℹ️  Creating stub for missing agent: run_evolution_agent\n",
            "ℹ️  Creating stub for missing agent: run_meta_review_agent\n",
            "\n",
            "🔄 Cycle 1 / 3\n",
            "\n",
            "🔄 Cycle 2 / 3\n",
            "\n",
            "🔄 Cycle 3 / 3\n",
            "\n",
            "✅ Supervisor finished: No improvement\n",
            "\n",
            "📊 Cycle summary:\n",
            "  • Cycle 1: n_hyp=5  best=H0\n",
            "  • Cycle 2: n_hyp=5  best=H0\n",
            "  • Cycle 3: n_hyp=5  best=H0\n"
          ]
        }
      ],
      "source": [
        "#@title 🕴️ Supervisor Agent (v5 — final demo with accurate n_hyp counts)\n",
        "# ================================================================\n",
        "# This cell orchestrates a “generate → reflect → rank → evolve”\n",
        "# research loop.  It includes *self-contained* stub agents so it\n",
        "# will run even before you wire in the real ones.\n",
        "#\n",
        "# Key improvements over v4:\n",
        "# • `run_proximity_agent` / `run_evolution_agent` stubs are wrapped\n",
        "#   in `_preserve_proposals()` so the `.proposals` list survives,\n",
        "#   giving correct `n_hyp` numbers in the report.\n",
        "# • Minor refactor for clarity; behaviour unchanged once real\n",
        "#   agent functions replace the stubs.\n",
        "# ================================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Any\n",
        "from types import SimpleNamespace\n",
        "import datetime, numpy as np\n",
        "\n",
        "# ---------- 1️⃣  Dataclasses --------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class SupervisorConfig:\n",
        "    research_goal: str\n",
        "    max_cycles: int = 5\n",
        "    generation_per_cycle: int = 1\n",
        "    evolution_every: int = 2\n",
        "    proximity_every: int = 1\n",
        "    meta_every: int = 2\n",
        "    no_improve_patience: int = 2\n",
        "\n",
        "@dataclass\n",
        "class CycleStats:\n",
        "    cycle_id: int\n",
        "    n_hypotheses: int\n",
        "    best_rank_text: str\n",
        "    timestamp: str\n",
        "\n",
        "@dataclass\n",
        "class SupervisorState:\n",
        "    config: SupervisorConfig\n",
        "    cycles: List[CycleStats] = field(default_factory=list)\n",
        "    generation_state: Any = None\n",
        "    reflection_state: Any = None\n",
        "    ranking_state: Any = None\n",
        "    meta_state: Any = None\n",
        "    finished: bool = False\n",
        "    finish_reason: str = \"\"\n",
        "\n",
        "# ---------- 2️⃣  Helper: guarantee proposals survive -------------------------\n",
        "\n",
        "def _preserve_proposals(obj):\n",
        "    \"\"\"Return an object (SimpleNamespace) that always has a .proposals list.\"\"\"\n",
        "    if hasattr(obj, \"proposals\"):\n",
        "        return obj\n",
        "    if isinstance(obj, dict) and \"proposals\" in obj:\n",
        "        return SimpleNamespace(**obj)\n",
        "    # fabricate an empty list so len()==0 is intentional\n",
        "    return SimpleNamespace(proposals=[])\n",
        "\n",
        "# ---------- 3️⃣  Stub builder (only if real agents absent) -------------------\n",
        "\n",
        "def _ensure_stub(name: str):\n",
        "    if name in globals():\n",
        "        return\n",
        "    print(f\"ℹ️  Creating stub for missing agent: {name}\")\n",
        "\n",
        "    if name == \"run_generation_agent\":\n",
        "        def fn(goal, constraints=None):\n",
        "            return SimpleNamespace(\n",
        "                proposals=[f\"hyp_{i}\" for i in range(5)],\n",
        "                ts=datetime.datetime.utcnow().isoformat()\n",
        "            )\n",
        "\n",
        "    elif name == \"run_reflection_agent\":\n",
        "        def fn(gen_state):\n",
        "            reviews = []\n",
        "            for i, hyp in enumerate(gen_state.proposals):\n",
        "                criteria = SimpleNamespace(\n",
        "                    novelty_reasoning=\"stub\", feasibility_reasoning=\"stub\",\n",
        "                    scientific_rigor_reasoning=\"stub\", impact_potential_reasoning=\"stub\",\n",
        "                    testability_reasoning=\"stub\",\n",
        "                    novelty_score=float(np.random.rand()*5),\n",
        "                    feasibility_score=float(np.random.rand()*5),\n",
        "                    scientific_rigor_score=float(np.random.rand()*5),\n",
        "                    impact_potential_score=float(np.random.rand()*5),\n",
        "                    testability_score=float(np.random.rand()*5)\n",
        "                )\n",
        "                reviews.append(SimpleNamespace(\n",
        "                    hypothesis_id=f\"H{i}\",\n",
        "                    hypothesis_text=hyp,\n",
        "                    criteria=criteria,\n",
        "                    weaknesses=[\"needs data\"], strengths=[\"novel\"]\n",
        "                ))\n",
        "            return SimpleNamespace(hypothesis_reviews=reviews)\n",
        "\n",
        "    elif name == \"run_ranking_agent\":\n",
        "        def fn(ref_state):\n",
        "            leaders = [SimpleNamespace(hypothesis_id=rev.hypothesis_id)\n",
        "                       for rev in ref_state.hypothesis_reviews]\n",
        "            return SimpleNamespace(leaderboard=leaders)\n",
        "\n",
        "    elif name == \"run_proximity_agent\":\n",
        "        def fn(gen_state):\n",
        "            return _preserve_proposals(gen_state)  # keeps list alive\n",
        "\n",
        "    elif name == \"run_evolution_agent\":\n",
        "        def fn(gen_state, rank_state):\n",
        "            return _preserve_proposals(gen_state)  # keeps list alive\n",
        "\n",
        "    elif name == \"run_meta_review_agent\":\n",
        "        def fn(ref_state, rank_state):\n",
        "            return SimpleNamespace(\n",
        "                actionable_for_generation=\"(stub) improve specificity\",\n",
        "                actionable_for_reflection=\"(stub) add novelty rubric\",\n",
        "                pattern_insights=[],\n",
        "                criterion_correlations={}\n",
        "            )\n",
        "\n",
        "    globals()[name] = fn\n",
        "\n",
        "for fn_name in [\n",
        "    \"run_generation_agent\", \"run_reflection_agent\", \"run_ranking_agent\",\n",
        "    \"run_proximity_agent\", \"run_evolution_agent\", \"run_meta_review_agent\"\n",
        "]:\n",
        "    _ensure_stub(fn_name)\n",
        "\n",
        "# ---------- 4️⃣  Utility getters ---------------------------------------------\n",
        "\n",
        "def _safe_get(obj, attr, default=None):\n",
        "    if isinstance(obj, dict):\n",
        "        return obj.get(attr, default)\n",
        "    return getattr(obj, attr, default)\n",
        "\n",
        "def _count_props(obj) -> int:\n",
        "    return len(_safe_get(obj, \"proposals\", []))\n",
        "\n",
        "# ---------- 5️⃣  Supervisor loop ---------------------------------------------\n",
        "\n",
        "def run_supervisor(config: SupervisorConfig) -> SupervisorState:\n",
        "    state = SupervisorState(config=config)\n",
        "    best_leader, stagnant = None, 0\n",
        "\n",
        "    for cycle in range(1, config.max_cycles + 1):\n",
        "        print(f\"\\n🔄 Cycle {cycle} / {config.max_cycles}\")\n",
        "\n",
        "        # Generation\n",
        "        for _ in range(config.generation_per_cycle):\n",
        "            state.generation_state = run_generation_agent(config.research_goal)\n",
        "\n",
        "        # Proximity\n",
        "        if cycle % config.proximity_every == 0:\n",
        "            state.generation_state = run_proximity_agent(state.generation_state)\n",
        "\n",
        "        # Reflection & Ranking\n",
        "        state.reflection_state = run_reflection_agent(state.generation_state)\n",
        "        state.ranking_state    = run_ranking_agent(state.reflection_state)\n",
        "\n",
        "        # Evolution\n",
        "        if cycle % config.evolution_every == 0:\n",
        "            state.generation_state = run_evolution_agent(state.generation_state,\n",
        "                                                         state.ranking_state)\n",
        "\n",
        "        # Meta-review\n",
        "        if cycle % config.meta_every == 0 and hasattr(state.reflection_state, \"hypothesis_reviews\"):\n",
        "            state.meta_state = run_meta_review_agent(state.reflection_state,\n",
        "                                                     state.ranking_state)\n",
        "\n",
        "        # Stats & early-stop\n",
        "        leaderboard = _safe_get(state.ranking_state, \"leaderboard\", [])\n",
        "        top_id = _safe_get(leaderboard[0], \"hypothesis_id\", None) if leaderboard else None\n",
        "        n_hyp  = _count_props(state.generation_state)\n",
        "\n",
        "        state.cycles.append(CycleStats(\n",
        "            cycle_id=cycle, n_hypotheses=n_hyp, best_rank_text=str(top_id),\n",
        "            timestamp=datetime.datetime.utcnow().isoformat()\n",
        "        ))\n",
        "\n",
        "        stagnant = stagnant + 1 if top_id == best_leader else 0\n",
        "        best_leader = best_leader or top_id\n",
        "        if top_id and top_id != best_leader:\n",
        "            best_leader, stagnant = top_id, 0\n",
        "\n",
        "        if stagnant >= config.no_improve_patience:\n",
        "            state.finished, state.finish_reason = True, \"No improvement\"\n",
        "            break\n",
        "\n",
        "    if not state.finished:\n",
        "        state.finished, state.finish_reason = True, \"Reached max_cycles\"\n",
        "\n",
        "    print(f\"\\n✅ Supervisor finished: {state.finish_reason}\")\n",
        "    return state\n",
        "\n",
        "# ---------- 6️⃣  Smoke-test ---------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\" or True:  # auto-run in notebook\n",
        "    cfg = SupervisorConfig(research_goal=\"Stubbed demo\", max_cycles=3)\n",
        "    sup_state = run_supervisor(cfg)\n",
        "\n",
        "    print(\"\\n📊 Cycle summary:\")\n",
        "    for c in sup_state.cycles:\n",
        "        print(f\"  • Cycle {c.cycle_id}: n_hyp={c.n_hypotheses}  best={c.best_rank_text}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpGDbkmqv2Of",
        "outputId": "6da28635-1024-4c62-c5f3-c8569f6c282e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "🔬 TESTING THE FULLY INTEGRATED AI CO-SCIENTIST SYSTEM (v3)\n",
            "============================================================\n",
            "🚀 Starting AI Co-Scientist Run: 'Discover novel epigenetic-based therapeutic strategies for liver fibrosis'\n",
            "============================================================\n",
            "\n",
            "🔄 CYCLE 1 / 2\n",
            "  -> Running Generation Agent...\n",
            "🔍 Generating multi-source search queries...\n",
            "✅ Generated 4 optimized queries\n",
            "   1. AI personalized medicine neurodegenerative diseases\n",
            "   2. machine learning precision medicine Alzheimer\n",
            "   3. computational biomarkers neurodegeneration\n",
            "   4. artificial intelligence clinical decision support\n",
            "📚 Searching multiple literature sources...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Web academic search failed for 'AI personalized medicine neurodegenerative diseases': https://lite.duckduckgo.com/lite/ RuntimeError: error sending request for url (https://lite.duckduckgo.com/lite/): operation timed out\n",
            "\n",
            "Caused by:\n",
            "    operation timed out\n",
            "ERROR:__main__:Web academic search failed for 'machine learning precision medicine Alzheimer': https://lite.duckduckgo.com/lite/ RuntimeError: error sending request for url (https://lite.duckduckgo.com/lite/): operation timed out\n",
            "\n",
            "Caused by:\n",
            "    operation timed out\n",
            "ERROR:__main__:Web academic search failed for 'computational biomarkers neurodegeneration': https://html.duckduckgo.com/html RuntimeError: error sending request for url (https://html.duckduckgo.com/html): operation timed out\n",
            "\n",
            "Caused by:\n",
            "    operation timed out\n",
            "ERROR:__main__:Web academic search failed for 'artificial intelligence clinical decision support': https://html.duckduckgo.com/html RuntimeError: error sending request for url (https://html.duckduckgo.com/html): operation timed out\n",
            "\n",
            "Caused by:\n",
            "    operation timed out\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Found 12 papers from 1 sources:\n",
            "   • PubMed: 12 papers\n",
            "🧠 Synthesizing knowledge from multiple sources...\n",
            "✅ Multi-source knowledge synthesis complete\n",
            "💡 Generating hypotheses from multi-source research...\n",
            "✅ Generated 1 distinct hypotheses.\n",
            "     + Added 1 new hypotheses to the pool.\n",
            "  -> Running Proximity Agent...\n",
            "     - Skipping, not enough hypotheses to compare.\n",
            "  -> Running Reflection Agent...\n",
            "     - Reviewed 1 hypotheses.\n",
            "  -> Running Ranking Agent...\n",
            "     - Skipping, not enough hypotheses to rank.\n",
            "  -> Running Evolution Agent...\n",
            "     + Evolved top hypotheses into 2 new proposals.\n",
            "  -> Post-Evolution Re-evaluation:\n",
            "  -> Running Reflection Agent...\n",
            "     - Reviewed 2 hypotheses.\n",
            "  -> Running Ranking Agent...\n",
            "     - Ranked 2 hypotheses.\n",
            "  -> Cycle 1 complete. Best hypothesis: evolved_simplification_013933\n",
            "\n",
            "🔄 CYCLE 2 / 2\n",
            "  -> Running Generation Agent...\n",
            "🔍 Generating multi-source search queries...\n",
            "✅ Generated 4 optimized queries\n",
            "   1. AI personalized medicine neurodegenerative diseases\n",
            "   2. machine learning precision medicine Alzheimer\n",
            "   3. computational biomarkers neurodegeneration\n",
            "   4. artificial intelligence clinical decision support\n",
            "📚 Searching multiple literature sources...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Web academic search failed for 'AI personalized medicine neurodegenerative diseases': https://html.duckduckgo.com/html RuntimeError: error sending request for url (https://html.duckduckgo.com/html): operation timed out\n",
            "\n",
            "Caused by:\n",
            "    operation timed out\n",
            "ERROR:__main__:CrossRef search failed for 'machine learning precision medicine Alzheimer': HTTPSConnectionPool(host='api.crossref.org', port=443): Read timed out. (read timeout=10)\n",
            "ERROR:__main__:Web academic search failed for 'machine learning precision medicine Alzheimer': https://html.duckduckgo.com/html RuntimeError: error sending request for url (https://html.duckduckgo.com/html): operation timed out\n",
            "\n",
            "Caused by:\n",
            "    operation timed out\n",
            "ERROR:__main__:Web academic search failed for 'computational biomarkers neurodegeneration': https://html.duckduckgo.com/html RuntimeError: error sending request for url (https://html.duckduckgo.com/html): operation timed out\n",
            "\n",
            "Caused by:\n",
            "    operation timed out\n",
            "ERROR:__main__:Web academic search failed for 'artificial intelligence clinical decision support': https://lite.duckduckgo.com/lite/ RuntimeError: error sending request for url (https://lite.duckduckgo.com/lite/): operation timed out\n",
            "\n",
            "Caused by:\n",
            "    operation timed out\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Found 12 papers from 1 sources:\n",
            "   • PubMed: 12 papers\n",
            "🧠 Synthesizing knowledge from multiple sources...\n",
            "✅ Multi-source knowledge synthesis complete\n",
            "💡 Generating hypotheses from multi-source research...\n",
            "✅ Generated 1 distinct hypotheses.\n",
            "     + Added 1 new hypotheses to the pool.\n",
            "  -> Running Proximity Agent...\n",
            "     - Deduplicated proposals. Removed: 0, Kept: 3.\n",
            "  -> Running Reflection Agent...\n",
            "     - Reviewed 3 hypotheses.\n",
            "  -> Running Ranking Agent...\n",
            "     - Ranked 3 hypotheses.\n",
            "  -> Running Evolution Agent...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 52\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Analogical reasoning failed for gen_hyp_0_1_1750038142: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 50\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 50\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Constraint relaxation failed for gen_hyp_0_1_1750038142: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 48\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 48\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Combination failed for ['evolved_simplification_013933', 'gen_hyp_0_1_1750038142']: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 46\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 46\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     + Evolved top hypotheses into 5 new proposals.\n",
            "  -> Post-Evolution Re-evaluation:\n",
            "  -> Running Reflection Agent...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:__main__:Quick review failed for evolved_simplification_014307: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 43\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 43\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Quick review failed for evolved_radical_variation_014307: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 41\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 41\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Quick review failed for evolved_analogical_reasoning_014311: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 39\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 39\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Quick review failed for evolved_constraint_relaxation_014311: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 37\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 37\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     - Reviewed 5 hypotheses.\n",
            "  -> Running Ranking Agent...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 48\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Debate failed for evolved_simplification_014307 vs evolved_radical_variation_014307: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 46\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 46\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Debate failed for evolved_simplification_014307 vs evolved_analogical_reasoning_014311: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 44\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 44\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Debate failed for evolved_simplification_014307 vs evolved_constraint_relaxation_014311: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 42\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 42\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Debate failed for evolved_simplification_014307 vs evolved_combination_014313: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 40\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 40\n",
            "}\n",
            "].\n",
            "ERROR:__main__:Debate failed for evolved_radical_variation_014307 vs evolved_analogical_reasoning_014311: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 38\n",
            "}\n",
            "]\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.0 Flash Preview (Image Generation) (models/gemini-2.0-flash-preview-image-generation) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/generate_requests_per_model\"\n",
            "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel\"\n",
            "  quota_dimensions {\n",
            "    key: \"model\"\n",
            "    value: \"gemini-2.0-flash-exp\"\n",
            "  }\n",
            "  quota_dimensions {\n",
            "    key: \"location\"\n",
            "    value: \"global\"\n",
            "  }\n",
            "  quota_value: 10\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            ", retry_delay {\n",
            "  seconds: 37\n",
            "}\n",
            "].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     - Ranked 5 hypotheses.\n",
            "  -> Running Meta-review Agent...\n",
            "     - Generated system-wide feedback.\n",
            "  -> Cycle 2 complete. Best hypothesis: evolved_simplification_014307\n",
            "\n",
            "============================================================\n",
            "✅ AI Co-Scientist Run Finished. Reason: Reached max cycles.\n",
            "\n",
            "\n",
            "============================================================\n",
            "📈 FINAL REPORT\n",
            "============================================================\n",
            "Research Goal: Discover novel epigenetic-based therapeutic strategies for liver fibrosis\n",
            "Total Cycles: 2\n",
            "Final Proposal Count: 5\n",
            "\n",
            "🏆 Top-Ranked Hypothesis: [evolved_simplification_014307]\n",
            "   Content: **\n",
            "\n",
            "A federated learning model trained on longitudinal plasma neurofilament light chain (NfL) data can predict conversion to mild cognitive impairment (MCI) in healthy controls with greater accuracy than baseline NfL levels alone.\n",
            "\n",
            "**...\n",
            "\n",
            "🧠 Final Meta-Review Feedback for Generation Agent:\n",
            "   - Analysis complete: 2 patterns identified from 5 documents\n",
            "\n",
            "📋 Cycle History:\n",
            "  - Cycle 1: 2 proposals, Best ID: evolved_simplification_013933, Duration: 172.3s\n",
            "  - Cycle 2: 5 proposals, Best ID: evolved_simplification_014307, Duration: 298.3s\n",
            "\n",
            "============================================================\n",
            "🎉 FULL SYSTEM TEST COMPLETE\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 🧬 Full System Integration: The AI Co-Scientist Loop (v3 - Resilient)\n",
        "# ===========================================================================\n",
        "# This cell integrates all previously defined agents into a single,\n",
        "# automated, multi-cycle research loop.\n",
        "#\n",
        "# Key Fixes in this version:\n",
        "# 1. Reorders the Supervisor loop to a more logical flow (Generate -> ... -> Evolve).\n",
        "# 2. Makes the Ranking Agent's skip-logic more robust by creating a\n",
        "#    properly populated RankingState object even when there's only one hypothesis.\n",
        "# 3. Further improves the Generation Agent's prompt for better multi-output.\n",
        "# 4. Corrected a critical runtime error in the Generation Agent.\n",
        "# ===========================================================================\n",
        "\n",
        "import time\n",
        "import re\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict, Any, Optional\n",
        "from types import SimpleNamespace # Used for mock objects\n",
        "import datetime # FIX: Added the missing datetime import\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 1. UNIFIED STATE & CONFIGURATION (Unchanged)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class SupervisorConfig:\n",
        "    \"\"\"Configuration for the supervisor's run.\"\"\"\n",
        "    research_goal: str\n",
        "    constraints: List[str] = field(default_factory=list)\n",
        "    max_cycles: int = 3\n",
        "    evolution_every: int = 2\n",
        "    meta_every: int = 2\n",
        "    proximity_every: int = 1\n",
        "    no_improve_patience: int = 2\n",
        "\n",
        "@dataclass\n",
        "class FullSystemState:\n",
        "    \"\"\"A unified state object to hold all data for the integrated run.\"\"\"\n",
        "    config: SupervisorConfig\n",
        "    proposals: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    reviews: List[Any] = field(default_factory=list)\n",
        "    rankings: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    generation_agent_state: Optional[Any] = None\n",
        "    reflection_agent_state: Optional[Any] = None\n",
        "    ranking_agent_state: Optional[Any] = None\n",
        "    evolution_agent_state: Optional[Any] = None\n",
        "    meta_review_agent_state: Optional[Any] = None\n",
        "    cycle_history: List[Dict[str, Any]] = field(default_factory=list)\n",
        "    is_finished: bool = False\n",
        "    finish_reason: str = \"\"\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 2. CORRECTED GENERATION AGENT (Improved Prompt & Fixed Timestamp)\n",
        "# ---------------------------------------------------------------------------\n",
        "class CorrectedGenerationAgent(EnhancedGenerationAgent):\n",
        "    \"\"\"Inherits from the original and fixes the hypothesis parsing.\"\"\"\n",
        "    def generate_hypotheses(self, state: GenerationState) -> GenerationState:\n",
        "        \"\"\"FIXED: Generate hypotheses and correctly parse them into a list.\"\"\"\n",
        "        print(\"💡 Generating hypotheses from multi-source research...\")\n",
        "        if not state.synthesized_knowledge:\n",
        "            state.generated_proposals = []\n",
        "            return state\n",
        "        try:\n",
        "            # Improved prompt to strongly encourage multiple, distinct outputs.\n",
        "            prompt = f\"\"\"Based on this multi-source research synthesis, generate 2-3 novel and distinct hypotheses for: {state.research_goal}\n",
        "Constraints: {', '.join(state.constraints)}\n",
        "\n",
        "Multi-Source Knowledge Base:\n",
        "{state.synthesized_knowledge}\n",
        "\n",
        "Structure your response with each distinct hypothesis separated by '---'. Each hypothesis must be a clear, testable statement.\n",
        "Example:\n",
        "Hypothesis 1: [Statement for hypothesis 1]\n",
        "---\n",
        "Hypothesis 2: [Statement for hypothesis 2]\n",
        "\"\"\"\n",
        "            response = self.llm.invoke([(\"human\", prompt)])\n",
        "            content = response.content\n",
        "            hypotheses_text = [h.strip() for h in re.split(r'\\\\n---\\\\n', content) if h.strip()]\n",
        "            proposals = []\n",
        "            for i, text in enumerate(hypotheses_text):\n",
        "                clean_text = re.sub(r'^Hypothesis \\\\d+:\\\\s*', '', text, flags=re.IGNORECASE)\n",
        "                if clean_text:\n",
        "                    proposals.append({\n",
        "                        \"id\": f\"gen_hyp_{state.iteration}_{i+1}_{int(time.time())}\",\n",
        "                        \"content\": clean_text,\n",
        "                        # FIX: Changed datetime.now() to datetime.datetime.now()\n",
        "                        \"timestamp\": datetime.datetime.now().isoformat(),\n",
        "                        \"based_on_sources\": len(state.literature_findings),\n",
        "                    })\n",
        "            state.generated_proposals = proposals\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Hypothesis generation failed: {str(e)}\")\n",
        "            state.generated_proposals = []\n",
        "        state.status = \"hypotheses_generated\"\n",
        "        state.iteration += 1\n",
        "        print(f\"✅ Generated {len(state.generated_proposals)} distinct hypotheses.\")\n",
        "        return state\n",
        "\n",
        "def _run_generation_step(state: FullSystemState) -> FullSystemState:\n",
        "    \"\"\"Runs the Generation Agent and integrates its output.\"\"\"\n",
        "    print(\"  -> Running Generation Agent...\")\n",
        "    agent = CorrectedGenerationAgent()\n",
        "    workflow = StateGraph(GenerationState)\n",
        "    workflow.add_node(\"generate_queries\", agent.generate_search_queries)\n",
        "    workflow.add_node(\"explore_literature\", agent.literature_exploration)\n",
        "    workflow.add_node(\"synthesize_knowledge\", agent.synthesize_knowledge)\n",
        "    workflow.add_node(\"generate_hypotheses\", agent.generate_hypotheses)\n",
        "    workflow.add_edge(START, \"generate_queries\")\n",
        "    workflow.add_edge(\"generate_queries\", \"explore_literature\")\n",
        "    workflow.add_edge(\"explore_literature\", \"synthesize_knowledge\")\n",
        "    workflow.add_edge(\"synthesize_knowledge\", \"generate_hypotheses\")\n",
        "    workflow.add_edge(\"generate_hypotheses\", END)\n",
        "    app = workflow.compile()\n",
        "    initial_state = GenerationState(\n",
        "        research_goal=state.config.research_goal,\n",
        "        constraints=state.config.constraints\n",
        "    )\n",
        "    generation_state = app.invoke(initial_state)\n",
        "    if generation_state:\n",
        "        new_proposals = generation_state.get('generated_proposals', [])\n",
        "        state.proposals.extend(new_proposals)\n",
        "        state.generation_agent_state = generation_state\n",
        "        print(f\"     + Added {len(new_proposals)} new hypotheses to the pool.\")\n",
        "    else:\n",
        "        print(\"     ! Generation Agent failed to produce output.\")\n",
        "    return state\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 3. ROBUST AGENT ADAPTERS (Ranking Step Fixed)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def _run_proximity_step(state: FullSystemState) -> FullSystemState:\n",
        "    print(\"  -> Running Proximity Agent...\")\n",
        "    initial_count = len(state.proposals)\n",
        "    if initial_count < 2:\n",
        "        print(\"     - Skipping, not enough hypotheses to compare.\")\n",
        "        return state\n",
        "    temp_gen_state = SimpleNamespace(generated_proposals=state.proposals)\n",
        "    final_gen_state = run_proximity_agent(temp_gen_state)\n",
        "    state.proposals = final_gen_state.generated_proposals\n",
        "    removed_count = initial_count - len(state.proposals)\n",
        "    print(f\"     - Deduplicated proposals. Removed: {removed_count}, Kept: {len(state.proposals)}.\")\n",
        "    return state\n",
        "\n",
        "def _run_reflection_step(state: FullSystemState) -> FullSystemState:\n",
        "    print(\"  -> Running Reflection Agent...\")\n",
        "    if not state.proposals:\n",
        "        print(\"     ! No proposals to reflect on.\")\n",
        "        return state\n",
        "    reflection_agent = RobustReflectionAgent(llm)\n",
        "    reflection_state = reflection_agent.adaptive_batch_review(\n",
        "        hypotheses=state.proposals, research_goal=state.config.research_goal\n",
        "    )\n",
        "    state.reviews = reflection_state.hypothesis_reviews\n",
        "    state.reflection_agent_state = reflection_state\n",
        "    print(f\"     - Reviewed {len(state.reviews)} hypotheses.\")\n",
        "    return state\n",
        "\n",
        "def _run_ranking_step(state: FullSystemState) -> FullSystemState:\n",
        "    \"\"\"Runs the Ranking Agent to create a leaderboard.\"\"\"\n",
        "    print(\"  -> Running Ranking Agent...\")\n",
        "    if len(state.reviews) < 2:\n",
        "        print(\"     - Skipping, not enough hypotheses to rank.\")\n",
        "        # FIX: When skipping, create a valid RankingState object with the single\n",
        "        # hypothesis as the final ranking. This prevents downstream NoneType errors.\n",
        "        if state.reviews:\n",
        "            ranking_list = [{'rank': 1, 'hypothesis_id': state.reviews[0].hypothesis_id}]\n",
        "            state.rankings = ranking_list\n",
        "            state.ranking_agent_state = RankingState(final_rankings=ranking_list)\n",
        "        else:\n",
        "            state.rankings = []\n",
        "            state.ranking_agent_state = RankingState() # Empty state\n",
        "        return state\n",
        "\n",
        "    ranking_state = run_ranking_tournament(state.reflection_agent_state, tournament_rounds=2)\n",
        "    state.rankings = ranking_state.final_rankings\n",
        "    state.ranking_agent_state = ranking_state\n",
        "    print(f\"     - Ranked {len(state.rankings)} hypotheses.\")\n",
        "    return state\n",
        "\n",
        "def _run_evolution_step(state: FullSystemState) -> FullSystemState:\n",
        "    \"\"\"Runs the Evolution Agent on the top-ranked proposals.\"\"\"\n",
        "    print(\"  -> Running Evolution Agent...\")\n",
        "    if not state.rankings or not state.ranking_agent_state:\n",
        "        print(\"     ! No ranked hypotheses to evolve. Skipping.\")\n",
        "        return state\n",
        "    evolution_state = run_evolution_process(\n",
        "        ranking_state=state.ranking_agent_state,\n",
        "        reflection_state=state.reflection_agent_state,\n",
        "        original_constraints=state.config.constraints,\n",
        "        top_n=2\n",
        "    )\n",
        "    evolved_proposals = [\n",
        "        {\"id\": hyp.hypothesis_id, \"content\": hyp.evolved_content}\n",
        "        for hyp in evolution_state.evolved_hypotheses\n",
        "    ]\n",
        "    state.proposals = evolved_proposals\n",
        "    state.evolution_agent_state = evolution_state\n",
        "    print(f\"     + Evolved top hypotheses into {len(evolved_proposals)} new proposals.\")\n",
        "    return state\n",
        "\n",
        "def _run_meta_review_step(state: FullSystemState) -> FullSystemState:\n",
        "    \"\"\"Runs the Meta-review Agent to provide system-level feedback.\"\"\"\n",
        "    print(\"  -> Running Meta-review Agent...\")\n",
        "    if not state.reflection_agent_state or not state.ranking_agent_state or not state.rankings:\n",
        "        print(\"     ! Insufficient data for meta-review.\")\n",
        "        return state\n",
        "    temp_rank_state = SimpleNamespace(\n",
        "        leaderboard=[SimpleNamespace(**r) for r in state.rankings]\n",
        "    )\n",
        "    # Corrected attribute access for meta-review agent\n",
        "    if hasattr(temp_rank_state, 'leaderboard') and isinstance(temp_rank_state.leaderboard, list):\n",
        "         # Add final_rankings attribute if missing, which the robust correlation function expects\n",
        "        if not hasattr(temp_rank_state, 'final_rankings'):\n",
        "            setattr(temp_rank_state, 'final_rankings', [{'hypothesis_id': item.hypothesis_id} for item in temp_rank_state.leaderboard])\n",
        "\n",
        "    meta_review_state = run_enhanced_meta_review(\n",
        "        reflection_state=state.reflection_agent_state,\n",
        "        ranking_state=temp_rank_state\n",
        "    )\n",
        "    state.meta_review_agent_state = meta_review_state\n",
        "    print(\"     - Generated system-wide feedback.\")\n",
        "    return state\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 4. THE INTEGRATED SUPERVISOR (Reordered Loop)\n",
        "# ---------------------------------------------------------------------------\n",
        "class IntegratedSupervisor:\n",
        "    \"\"\"Orchestrates the full Generate -> Reflect -> Rank -> Evolve loop.\"\"\"\n",
        "    def __init__(self, config: SupervisorConfig):\n",
        "        self.config = config\n",
        "\n",
        "    def run(self) -> FullSystemState:\n",
        "        \"\"\"Executes the full, multi-cycle research process.\"\"\"\n",
        "        print(f\"🚀 Starting AI Co-Scientist Run: '{self.config.research_goal}'\")\n",
        "        print(\"=\"*60)\n",
        "        state = FullSystemState(config=self.config)\n",
        "        best_leader_id = None\n",
        "        stagnation_counter = 0\n",
        "\n",
        "        for cycle in range(1, self.config.max_cycles + 1):\n",
        "            print(f\"\\n🔄 CYCLE {cycle} / {self.config.max_cycles}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # 1. Generation (add new ideas to the pool)\n",
        "            state = _run_generation_step(state)\n",
        "\n",
        "            # 2. Proximity (Deduplicate)\n",
        "            if cycle % self.config.proximity_every == 0:\n",
        "                state = _run_proximity_step(state)\n",
        "\n",
        "            # 3. Reflection & Ranking\n",
        "            state = _run_reflection_step(state)\n",
        "            state = _run_ranking_step(state)\n",
        "\n",
        "            # 4. Evolution (if applicable, evolve this cycle's best)\n",
        "            if cycle % self.config.evolution_every == 0:\n",
        "                state = _run_evolution_step(state)\n",
        "                # After evolution, we have a new generation of proposals.\n",
        "                # We reflect and rank them to find the new leader for this cycle.\n",
        "                print(\"  -> Post-Evolution Re-evaluation:\")\n",
        "                state = _run_reflection_step(state)\n",
        "                state = _run_ranking_step(state)\n",
        "\n",
        "            # 5. Meta-Review\n",
        "            if cycle % self.config.meta_every == 0:\n",
        "                state = _run_meta_review_step(state)\n",
        "\n",
        "            # 6. Logging and Early Stopping Check\n",
        "            current_leader_id = state.rankings[0]['hypothesis_id'] if state.rankings else None\n",
        "            state.cycle_history.append({\n",
        "                'cycle': cycle,\n",
        "                'num_proposals': len(state.proposals),\n",
        "                'best_hypothesis_id': current_leader_id,\n",
        "                'duration_sec': time.time() - start_time\n",
        "            })\n",
        "            print(f\"  -> Cycle {cycle} complete. Best hypothesis: {current_leader_id}\")\n",
        "\n",
        "            if current_leader_id and current_leader_id == best_leader_id:\n",
        "                stagnation_counter += 1\n",
        "            else:\n",
        "                best_leader_id = current_leader_id\n",
        "                stagnation_counter = 0\n",
        "\n",
        "            if stagnation_counter >= self.config.no_improve_patience:\n",
        "                state.is_finished = True\n",
        "                state.finish_reason = f\"No improvement in top hypothesis for {stagnation_counter} cycles.\"\n",
        "                break\n",
        "\n",
        "        if not state.is_finished:\n",
        "            state.is_finished = True\n",
        "            state.finish_reason = \"Reached max cycles.\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"✅ AI Co-Scientist Run Finished. Reason: {state.finish_reason}\")\n",
        "        return state\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# 5. TESTING THE FULLY INTEGRATED SYSTEM\n",
        "# ---------------------------------------------------------------------------\n",
        "def test_full_system():\n",
        "    \"\"\"Defines a research goal and runs the entire integrated system.\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"🔬 TESTING THE FULLY INTEGRATED AI CO-SCIENTIST SYSTEM (v3)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    config = SupervisorConfig(\n",
        "        research_goal=\"Discover novel epigenetic-based therapeutic strategies for liver fibrosis\",\n",
        "        constraints=[\n",
        "            \"Must be translatable to clinical practice within 5-7 years.\",\n",
        "            \"Focus on reversing fibrosis, not just halting progression.\",\n",
        "            \"Consider novel delivery mechanisms for targeted therapy.\"\n",
        "        ],\n",
        "        max_cycles=2,\n",
        "        evolution_every=1, # Run every cycle for a thorough test\n",
        "        meta_every=2\n",
        "    )\n",
        "    supervisor = IntegratedSupervisor(config)\n",
        "    final_state = supervisor.run()\n",
        "\n",
        "    print(\"\\n\\n\" + \"=\"*60)\n",
        "    print(\"📈 FINAL REPORT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Research Goal: {final_state.config.research_goal}\")\n",
        "    print(f\"Total Cycles: {len(final_state.cycle_history)}\")\n",
        "    print(f\"Final Proposal Count: {len(final_state.proposals)}\")\n",
        "\n",
        "    if final_state.rankings:\n",
        "        top_hypothesis_id = final_state.rankings[0]['hypothesis_id']\n",
        "        top_hypothesis_content = \"Content not found in final proposals.\"\n",
        "        for prop in final_state.proposals:\n",
        "             if prop.get('id') == top_hypothesis_id:\n",
        "                top_hypothesis_content = prop.get('content')\n",
        "                break\n",
        "        print(f\"\\n🏆 Top-Ranked Hypothesis: [{top_hypothesis_id}]\")\n",
        "        print(f\"   Content: {top_hypothesis_content[:250]}...\")\n",
        "    else:\n",
        "        print(\"\\n🏆 No final hypotheses were ranked.\")\n",
        "\n",
        "    if final_state.meta_review_agent_state and hasattr(final_state.meta_review_agent_state, 'actionable_for_generation'):\n",
        "        print(\"\\n🧠 Final Meta-Review Feedback for Generation Agent:\")\n",
        "        print(f\"   - {final_state.meta_review_agent_state.actionable_for_generation}\")\n",
        "    else:\n",
        "        print(\"\\n🧠 No meta-review was generated in this short run.\")\n",
        "\n",
        "    print(\"\\n📋 Cycle History:\")\n",
        "    for entry in final_state.cycle_history:\n",
        "        print(f\"  - Cycle {entry['cycle']}: {entry['num_proposals']} proposals, Best ID: {entry['best_hypothesis_id']}, Duration: {entry['duration_sec']:.1f}s\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎉 FULL SYSTEM TEST COMPLETE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return final_state\n",
        "\n",
        "# --- Run the test ---\n",
        "if __name__ == \"__main__\":\n",
        "    final_system_state = test_full_system()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}